{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f74ada-05b3-43da-b6de-c7e12745c66a",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"https://escience.sdu.dk/index.php/ucloud/\">\n",
    "    <img src=\"https://escience.sdu.dk/wp-content/uploads/2020/03/logo_esc.svg\" width=\"400\" height=\"186\" />\n",
    "  </a>\n",
    "</center>\n",
    "<br>\n",
    "<p style=\"font-size: 1.2em;\">\n",
    "  This notebook was tested using <strong>NeMo Framework v24.07</strong> and machine type <code>u3-gpu4</code> on UCloud.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3323204-1463-4df3-8c75-5e95b6d66ba1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Building a Llama-3.3 LoRA Adapter with the NeMo Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3d632-44a0-4e6c-9229-b70bbcff1e99",
   "metadata": {},
   "source": [
    "This notebook showcases performing LoRA PEFT [**Llama 3.1 8B**](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/tree/main) on [PubMedQA](https://pubmedqa.github.io/) using NeMo Framework. PubMedQA is a Question-Answering dataset for biomedical texts.\n",
    "\n",
    "In this notebook, we demonstrate how to apply Low-Rank Adaptation (LoRA) Parameter-Efficient Fine-Tuning (PEFT) techniques to the Llama 3.3 70B model using the NeMo Framework. We use [PubMedQA](https://pubmedqa.github.io/), a specialized question-answering dataset derived from biomedical literature, to illustrate how LoRA adapters can efficiently enhance model performance within a domain-specific context.\n",
    "\n",
    "**Disclaimer**: This notebook is adapted from the [NVIDIA NeMo tutorial on biomedical QA with Llama-3](https://github.com/NVIDIA/NeMo/blob/main/tutorials/llm/llama-3/biomedical-qa/llama3-lora-nemofw.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee38a79-b107-494e-b8e7-3d1f6d26b412",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Estimating GPU Memory Requirements for Serving LLMs\n",
    "\n",
    "\n",
    "### **1. Model Size**\n",
    "Before you begin, it’s essential to understand how much GPU memory you’ll need to serve a large language model (LLM). A commonly used formula is:\n",
    "\n",
    "$$\n",
    "M_{\\text{model}} = \\frac{(P \\times 4B)}{(32 / Q)}\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- **M**: The GPU memory required (in Gigabytes)  \n",
    "- **P**: The number of parameters in the model (e.g., 7 billion parameters for a 7B model)  \n",
    "- **4B**: 4 bytes, representing the size of each parameter at full precision (32 bits)  \n",
    "- **32**: The number of bits in 4 bytes (32 bits)  \n",
    "- **Q**: The model precision in bits used during serving (e.g., 16 bits, 8 bits, or 4 bits)  \n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Start with $P \\times 4B$ to get the base memory needed for all parameters at full precision (FP32).\n",
    "- Divide by $(32/Q)$, which scales the memory requirement according to the lower-precision format you’re using. For example, loading a model in 16-bit precision effectively halves the memory usage compared to 32-bit.\n",
    "\n",
    "#### **Example:**\n",
    "\n",
    "For a 70B parameter model loaded in 8-bit precision:\n",
    "\n",
    "- $P = 70 \\times 10^9$ ($70$ billion)\n",
    "- $Q = 8$\n",
    "\n",
    "Plugging these in:\n",
    "\n",
    "$$\n",
    "M_{\\text{model}} = \\frac{(70 \\times 10^9 \\times 4B)}{(32 / 8)} \n",
    "= \\frac{(280 \\times 10^9 B)}{2} \n",
    "= 70 \\times 10^9 B\n",
    "$$\n",
    "\n",
    "Convert bytes to gigabytes (1 GB = $10^9$ bytes):\n",
    "\n",
    "$$\n",
    "M = 70 \\text{ GB}\n",
    "$$\n",
    "\n",
    "This rough calculation helps estimate the GPU memory needed for serving large models, ensuring you have the right hardware configuration before starting fine-tuning or inference steps.\n",
    "\n",
    "### **2. Context Window**\n",
    "\n",
    "The **context window** refers to the maximum number of tokens (words or subwords) the model can process in a single inference pass. During inference, the model needs to store activations for each token in the input sequence. This storage requirement scales linearly with the length of the context window.\n",
    "\n",
    "#### **Memory Calculation for Context Window**\n",
    "\n",
    "$$\n",
    "M_{\\text{context}} = L \\times H \\times D \\times N\n",
    "$$\n",
    "\n",
    "- **$M_{\\text{context}}$**: Memory required for the context window (in Gigabytes)\n",
    "- **$L$**: Length of the context window (number of tokens)\n",
    "- **$H$**: Hidden size (dimensionality of the model's hidden layers)\n",
    "- **$D$**: Data type size (bytes per element, e.g., 2 for FP16)\n",
    "- **$N$**: Number of transformer layers\n",
    "\n",
    "#### **Example:**\n",
    "\n",
    "Assume:\n",
    "- **$L = 1024$** tokens\n",
    "- **$H = 8192$** dimensions\n",
    "- **$D = 1$** bytes (for INT8 precision)\n",
    "- **$N = 80$** number of hidden layers\n",
    "\n",
    "$$\n",
    "M_{\\text{context}} = 1024 \\times 8192 \\times 1 \\times 80 = 671,088,640 \\text{ bytes} \\approx 671 \\text{ MB}\n",
    "$$\n",
    "\n",
    "### **3. Batch Size**\n",
    "\n",
    "**Batch size** determines how many input sequences the model processes simultaneously. Increasing the batch size can lead to higher GPU memory usage because the model needs to store activations for each sequence in the batch.\n",
    "\n",
    "#### **Memory Calculation for Batch Size**\n",
    "\n",
    "$$\n",
    "M_{\\text{batch}} = B \\times M_{\\text{context}}\n",
    "$$\n",
    "\n",
    "- **$M_{\\text{batch}}$**: Additional memory required for batching (in Gigabytes)\n",
    "- **$B$**: Batch size (number of sequences)\n",
    "- **$M_{\\text{context}}$**: Memory per sequence (from context window calculation)\n",
    "\n",
    "#### **Example:**\n",
    "\n",
    "Using the previous **$M_{\\text{context}} =  671 \\text{ MB}$** and a **batch size $B = 8$**:\n",
    "\n",
    "$$\n",
    "M_{\\text{batch}} = 8 \\times  671 \\text{ MB} = 5.4 \\text{ GB}\n",
    "$$\n",
    "\n",
    "### **4. Total Inference Memory Estimation**\n",
    "\n",
    "Combining all these factors gives a more comprehensive estimate of the GPU memory required for inference:\n",
    "\n",
    "$$\n",
    "M_{\\text{total}} = M_{\\text{model}} + M_{\\text{context}} \\times B + M_{\\text{overhead}}\n",
    "$$\n",
    "\n",
    "- **$M_{\\text{total}}$**: Total GPU memory required (in Gigabytes)\n",
    "- **$M_{\\text{model}}$**: Memory for the model\n",
    "- **$M_{\\text{context}}$**: Memory per token sequence\n",
    "- **$B$**: Batch size\n",
    "- **$M_{\\text{overhead}}$**: Additional overhead for operations like caching, temporary buffers, etc. (typically 10-20%)\n",
    "\n",
    "#### Example\n",
    "\n",
    "Using the previous results:\n",
    "\n",
    "$$\n",
    "M_{\\text{total}} \\approx 90 \\text{ GB}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f729b57-6b66-46ff-ae26-8f0e7f80428b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated GPU Memory Required for LLama-3 8B: 21.78 GB\n",
      "Estimated GPU Memory Required for LLama-3 70B: 180.88 GB\n"
     ]
    }
   ],
   "source": [
    "from utils import estimate_gpu_memory\n",
    "\n",
    "Q = 16  # 16-bit precision (bfloat16)\n",
    "L = 1024  # Context window\n",
    "B = 8  # Batch size\n",
    "\n",
    "# Example usage for LLama-3.1 8B\n",
    "P_8B = 8_000_000_000  # 8B parameters\n",
    "H_8B = 4096  # Hidden size\n",
    "N_8B = 32\n",
    "\n",
    "estimated_memory_8B = estimate_gpu_memory(P_8B, Q, L, H_8B, B, N_8B)\n",
    "print(f\"Estimated GPU Memory Required for LLama-3 8B: {estimated_memory_8B:.2f} GB\")\n",
    "\n",
    "# Example usage for LLama-3.1 70B\n",
    "P_70B = 70_000_000_000  # 70B parameters\n",
    "H_70B = 8192  # Hidden size\n",
    "N_70B = 80\n",
    "\n",
    "estimated_memory_70B = estimate_gpu_memory(P_70B, Q, L, H_70B, B, N_70B)\n",
    "print(f\"Estimated GPU Memory Required for LLama-3 70B: {estimated_memory_70B:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b285d5a-d838-423b-9d6c-65add61f48ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download the Model\n",
    "Before you begin, ensure you have a local copy of the Meta Llama3.3 70B Instruct model. If you haven’t already downloaded it, you can obtain it from the official [Hugging Face repository](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/tree/main). This step is crucial to ensure that all subsequent operations in the notebook run smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93614eb2-8f1f-49c0-a5bd-c455b4549f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481525106dcd4136bfda461b4c468a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Password(description='Hugging Face Token:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Password\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "pwd = Password(description=\"Hugging Face Token:\")\n",
    "display(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec7be368-f5db-4c34-87ec-00f574cd8ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d52a11336847c3a9ec349128d8b90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/work/ucloud-workshop-11-12-2024/models/llama-3.1/8B/hf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = pwd.value\n",
    "hf_model=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "hf_model_path=\"models/llama-3.1/8B/hf\"\n",
    "snapshot_download(\n",
    "    repo_id=hf_model,\n",
    "    local_dir=hf_model_path,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3057e525-7957-45c0-bedc-c347d4811081",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE\n",
      "README.md\n",
      "USE_POLICY.md\n",
      "config.json\n",
      "config.json.bak\n",
      "generation_config.json\n",
      "model-00001-of-00004.safetensors\n",
      "model-00002-of-00004.safetensors\n",
      "model-00003-of-00004.safetensors\n",
      "model-00004-of-00004.safetensors\n",
      "model.safetensors.index.json\n",
      "original\n",
      "special_tokens_map.json\n",
      "tokenizer.json\n",
      "tokenizer_config.json\n",
      "30G\tmodels/llama-3.1/8B/hf\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$hf_model_path\"\n",
    "\n",
    "ls $1\n",
    "du -sh $1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba9d09-2412-404e-9bd9-45e67724a46c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert the Model in NeMo Format\n",
    "\n",
    "To fully leverage the NeMo toolkit and its ecosystem of training, inference, and deployment tools, it’s often necessary to convert your model into NeMo’s native `.nemo` format. For detailed, step-by-step instructions on performing such conversions, refer to the [NeMo user guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/checkpoints/user_guide.html) on checkpoint conversion.\n",
    "\n",
    "This conversion will help ensure compatibility and streamline the process of fine-tuning, evaluating, and deploying your NeMo-based LLM workflows.\n",
    "\n",
    "In this case, we will use the `convert_llama_hf_to_nemo.py` script provided by NeMo:\n",
    "\n",
    "```\n",
    "$ python /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py --help\n",
    "```\n",
    "\n",
    "```text\n",
    "    usage: convert_llama_hf_to_nemo.py [-h] --input_name_or_path INPUT_NAME_OR_PATH --output_path OUTPUT_PATH [--hparams_file HPARAMS_FILE] [--precision PRECISION]\n",
    "\n",
    "    options:\n",
    "      -h, --help            show this help message and exit\n",
    "      --input_name_or_path INPUT_NAME_OR_PATH\n",
    "                            Path to Huggingface LLaMA checkpoints\n",
    "      --output_path OUTPUT_PATH\n",
    "                            Path to output .nemo file.\n",
    "      --hparams_file HPARAMS_FILE\n",
    "                            Path config for restoring (hparams.yaml).\n",
    "      --precision PRECISION\n",
    "                            Model precision\n",
    "```\n",
    "\n",
    "Below is a summary of different model precision choices, along with their key trade-offs:\n",
    "- **FP32 (32-bit Float):** Maximum precision, but slower and uses more memory.\n",
    "- **FP16 (16-bit Float):** Reduces memory usage and speeds up training, but can be numerically unstable if used alone.\n",
    "- **BF16 (BFloat16):** Offers similar speed and memory benefits to FP16, but with greater numerical stability due to a larger exponent range, making it more robust than pure FP16.\n",
    "- **FP16 Mixed Precision:** Employs FP16 for most operations and FP32 for critical ones, striking a balance between performance and stability.\n",
    "- **BF16 Mixed Precision:** Similar to FP16 mixed, but even more stable, leveraging BF16 for most operations and FP32 where necessary for optimal stability, performance, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e25ab64-a613-4d9d-b2f4-0cc9a31f7e16",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:40:41 convert_llama_hf_to_nemo:111] loading checkpoint models/llama-3.1/8B/hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.55s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_config: {'vocab_size': 128256, 'max_position_embeddings': 131072, 'hidden_size': 4096, 'intermediate_size': 14336, 'num_hidden_layers': 32, 'num_attention_heads': 32, 'num_key_value_heads': 8, 'hidden_act': 'silu', 'initializer_range': 0.02, 'rms_norm_eps': 1e-05, 'pretraining_tp': 1, 'use_cache': True, 'rope_theta': 500000, 'rope_scaling': {'factor': 8.000000001, 'type': 'linear'}, 'attention_bias': False, 'attention_dropout': 0, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': torch.bfloat16, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': False, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['LlamaForCausalLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 128000, 'pad_token_id': None, 'eos_token_id': [128001, 128008, 128009], 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'models/llama-3.1/8B/hf', '_commit_hash': None, '_attn_implementation_internal': 'sdpa', 'transformers_version': '4.42.3', 'mlp_bias': False, 'model_type': 'llama'}\n",
      "named parameters:\n",
      "- model.embed_tokens.weight\n",
      "- model.layers.0.self_attn.q_proj.weight\n",
      "- model.layers.0.self_attn.k_proj.weight\n",
      "- model.layers.0.self_attn.v_proj.weight\n",
      "- model.layers.0.self_attn.o_proj.weight\n",
      "- model.layers.0.mlp.gate_proj.weight\n",
      "- model.layers.0.mlp.up_proj.weight\n",
      "- model.layers.0.mlp.down_proj.weight\n",
      "- model.layers.0.input_layernorm.weight\n",
      "- model.layers.0.post_attention_layernorm.weight\n",
      "- model.layers.1.self_attn.q_proj.weight\n",
      "- model.layers.1.self_attn.k_proj.weight\n",
      "- model.layers.1.self_attn.v_proj.weight\n",
      "- model.layers.1.self_attn.o_proj.weight\n",
      "- model.layers.1.mlp.gate_proj.weight\n",
      "- model.layers.1.mlp.up_proj.weight\n",
      "- model.layers.1.mlp.down_proj.weight\n",
      "- model.layers.1.input_layernorm.weight\n",
      "- model.layers.1.post_attention_layernorm.weight\n",
      "- model.layers.2.self_attn.q_proj.weight\n",
      "- model.layers.2.self_attn.k_proj.weight\n",
      "- model.layers.2.self_attn.v_proj.weight\n",
      "- model.layers.2.self_attn.o_proj.weight\n",
      "- model.layers.2.mlp.gate_proj.weight\n",
      "- model.layers.2.mlp.up_proj.weight\n",
      "- model.layers.2.mlp.down_proj.weight\n",
      "- model.layers.2.input_layernorm.weight\n",
      "- model.layers.2.post_attention_layernorm.weight\n",
      "- model.layers.3.self_attn.q_proj.weight\n",
      "- model.layers.3.self_attn.k_proj.weight\n",
      "- model.layers.3.self_attn.v_proj.weight\n",
      "- model.layers.3.self_attn.o_proj.weight\n",
      "- model.layers.3.mlp.gate_proj.weight\n",
      "- model.layers.3.mlp.up_proj.weight\n",
      "- model.layers.3.mlp.down_proj.weight\n",
      "- model.layers.3.input_layernorm.weight\n",
      "- model.layers.3.post_attention_layernorm.weight\n",
      "- model.layers.4.self_attn.q_proj.weight\n",
      "- model.layers.4.self_attn.k_proj.weight\n",
      "- model.layers.4.self_attn.v_proj.weight\n",
      "- model.layers.4.self_attn.o_proj.weight\n",
      "- model.layers.4.mlp.gate_proj.weight\n",
      "- model.layers.4.mlp.up_proj.weight\n",
      "- model.layers.4.mlp.down_proj.weight\n",
      "- model.layers.4.input_layernorm.weight\n",
      "- model.layers.4.post_attention_layernorm.weight\n",
      "- model.layers.5.self_attn.q_proj.weight\n",
      "- model.layers.5.self_attn.k_proj.weight\n",
      "- model.layers.5.self_attn.v_proj.weight\n",
      "- model.layers.5.self_attn.o_proj.weight\n",
      "- model.layers.5.mlp.gate_proj.weight\n",
      "- model.layers.5.mlp.up_proj.weight\n",
      "- model.layers.5.mlp.down_proj.weight\n",
      "- model.layers.5.input_layernorm.weight\n",
      "- model.layers.5.post_attention_layernorm.weight\n",
      "- model.layers.6.self_attn.q_proj.weight\n",
      "- model.layers.6.self_attn.k_proj.weight\n",
      "- model.layers.6.self_attn.v_proj.weight\n",
      "- model.layers.6.self_attn.o_proj.weight\n",
      "- model.layers.6.mlp.gate_proj.weight\n",
      "- model.layers.6.mlp.up_proj.weight\n",
      "- model.layers.6.mlp.down_proj.weight\n",
      "- model.layers.6.input_layernorm.weight\n",
      "- model.layers.6.post_attention_layernorm.weight\n",
      "- model.layers.7.self_attn.q_proj.weight\n",
      "- model.layers.7.self_attn.k_proj.weight\n",
      "- model.layers.7.self_attn.v_proj.weight\n",
      "- model.layers.7.self_attn.o_proj.weight\n",
      "- model.layers.7.mlp.gate_proj.weight\n",
      "- model.layers.7.mlp.up_proj.weight\n",
      "- model.layers.7.mlp.down_proj.weight\n",
      "- model.layers.7.input_layernorm.weight\n",
      "- model.layers.7.post_attention_layernorm.weight\n",
      "- model.layers.8.self_attn.q_proj.weight\n",
      "- model.layers.8.self_attn.k_proj.weight\n",
      "- model.layers.8.self_attn.v_proj.weight\n",
      "- model.layers.8.self_attn.o_proj.weight\n",
      "- model.layers.8.mlp.gate_proj.weight\n",
      "- model.layers.8.mlp.up_proj.weight\n",
      "- model.layers.8.mlp.down_proj.weight\n",
      "- model.layers.8.input_layernorm.weight\n",
      "- model.layers.8.post_attention_layernorm.weight\n",
      "- model.layers.9.self_attn.q_proj.weight\n",
      "- model.layers.9.self_attn.k_proj.weight\n",
      "- model.layers.9.self_attn.v_proj.weight\n",
      "- model.layers.9.self_attn.o_proj.weight\n",
      "- model.layers.9.mlp.gate_proj.weight\n",
      "- model.layers.9.mlp.up_proj.weight\n",
      "- model.layers.9.mlp.down_proj.weight\n",
      "- model.layers.9.input_layernorm.weight\n",
      "- model.layers.9.post_attention_layernorm.weight\n",
      "- model.layers.10.self_attn.q_proj.weight\n",
      "- model.layers.10.self_attn.k_proj.weight\n",
      "- model.layers.10.self_attn.v_proj.weight\n",
      "- model.layers.10.self_attn.o_proj.weight\n",
      "- model.layers.10.mlp.gate_proj.weight\n",
      "- model.layers.10.mlp.up_proj.weight\n",
      "- model.layers.10.mlp.down_proj.weight\n",
      "- model.layers.10.input_layernorm.weight\n",
      "- model.layers.10.post_attention_layernorm.weight\n",
      "- model.layers.11.self_attn.q_proj.weight\n",
      "- model.layers.11.self_attn.k_proj.weight\n",
      "- model.layers.11.self_attn.v_proj.weight\n",
      "- model.layers.11.self_attn.o_proj.weight\n",
      "- model.layers.11.mlp.gate_proj.weight\n",
      "- model.layers.11.mlp.up_proj.weight\n",
      "- model.layers.11.mlp.down_proj.weight\n",
      "- model.layers.11.input_layernorm.weight\n",
      "- model.layers.11.post_attention_layernorm.weight\n",
      "- model.layers.12.self_attn.q_proj.weight\n",
      "- model.layers.12.self_attn.k_proj.weight\n",
      "- model.layers.12.self_attn.v_proj.weight\n",
      "- model.layers.12.self_attn.o_proj.weight\n",
      "- model.layers.12.mlp.gate_proj.weight\n",
      "- model.layers.12.mlp.up_proj.weight\n",
      "- model.layers.12.mlp.down_proj.weight\n",
      "- model.layers.12.input_layernorm.weight\n",
      "- model.layers.12.post_attention_layernorm.weight\n",
      "- model.layers.13.self_attn.q_proj.weight\n",
      "- model.layers.13.self_attn.k_proj.weight\n",
      "- model.layers.13.self_attn.v_proj.weight\n",
      "- model.layers.13.self_attn.o_proj.weight\n",
      "- model.layers.13.mlp.gate_proj.weight\n",
      "- model.layers.13.mlp.up_proj.weight\n",
      "- model.layers.13.mlp.down_proj.weight\n",
      "- model.layers.13.input_layernorm.weight\n",
      "- model.layers.13.post_attention_layernorm.weight\n",
      "- model.layers.14.self_attn.q_proj.weight\n",
      "- model.layers.14.self_attn.k_proj.weight\n",
      "- model.layers.14.self_attn.v_proj.weight\n",
      "- model.layers.14.self_attn.o_proj.weight\n",
      "- model.layers.14.mlp.gate_proj.weight\n",
      "- model.layers.14.mlp.up_proj.weight\n",
      "- model.layers.14.mlp.down_proj.weight\n",
      "- model.layers.14.input_layernorm.weight\n",
      "- model.layers.14.post_attention_layernorm.weight\n",
      "- model.layers.15.self_attn.q_proj.weight\n",
      "- model.layers.15.self_attn.k_proj.weight\n",
      "- model.layers.15.self_attn.v_proj.weight\n",
      "- model.layers.15.self_attn.o_proj.weight\n",
      "- model.layers.15.mlp.gate_proj.weight\n",
      "- model.layers.15.mlp.up_proj.weight\n",
      "- model.layers.15.mlp.down_proj.weight\n",
      "- model.layers.15.input_layernorm.weight\n",
      "- model.layers.15.post_attention_layernorm.weight\n",
      "- model.layers.16.self_attn.q_proj.weight\n",
      "- model.layers.16.self_attn.k_proj.weight\n",
      "- model.layers.16.self_attn.v_proj.weight\n",
      "- model.layers.16.self_attn.o_proj.weight\n",
      "- model.layers.16.mlp.gate_proj.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 15:41:07 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model.layers.16.mlp.up_proj.weight\n",
      "- model.layers.16.mlp.down_proj.weight\n",
      "- model.layers.16.input_layernorm.weight\n",
      "- model.layers.16.post_attention_layernorm.weight\n",
      "- model.layers.17.self_attn.q_proj.weight\n",
      "- model.layers.17.self_attn.k_proj.weight\n",
      "- model.layers.17.self_attn.v_proj.weight\n",
      "- model.layers.17.self_attn.o_proj.weight\n",
      "- model.layers.17.mlp.gate_proj.weight\n",
      "- model.layers.17.mlp.up_proj.weight\n",
      "- model.layers.17.mlp.down_proj.weight\n",
      "- model.layers.17.input_layernorm.weight\n",
      "- model.layers.17.post_attention_layernorm.weight\n",
      "- model.layers.18.self_attn.q_proj.weight\n",
      "- model.layers.18.self_attn.k_proj.weight\n",
      "- model.layers.18.self_attn.v_proj.weight\n",
      "- model.layers.18.self_attn.o_proj.weight\n",
      "- model.layers.18.mlp.gate_proj.weight\n",
      "- model.layers.18.mlp.up_proj.weight\n",
      "- model.layers.18.mlp.down_proj.weight\n",
      "- model.layers.18.input_layernorm.weight\n",
      "- model.layers.18.post_attention_layernorm.weight\n",
      "- model.layers.19.self_attn.q_proj.weight\n",
      "- model.layers.19.self_attn.k_proj.weight\n",
      "- model.layers.19.self_attn.v_proj.weight\n",
      "- model.layers.19.self_attn.o_proj.weight\n",
      "- model.layers.19.mlp.gate_proj.weight\n",
      "- model.layers.19.mlp.up_proj.weight\n",
      "- model.layers.19.mlp.down_proj.weight\n",
      "- model.layers.19.input_layernorm.weight\n",
      "- model.layers.19.post_attention_layernorm.weight\n",
      "- model.layers.20.self_attn.q_proj.weight\n",
      "- model.layers.20.self_attn.k_proj.weight\n",
      "- model.layers.20.self_attn.v_proj.weight\n",
      "- model.layers.20.self_attn.o_proj.weight\n",
      "- model.layers.20.mlp.gate_proj.weight\n",
      "- model.layers.20.mlp.up_proj.weight\n",
      "- model.layers.20.mlp.down_proj.weight\n",
      "- model.layers.20.input_layernorm.weight\n",
      "- model.layers.20.post_attention_layernorm.weight\n",
      "- model.layers.21.self_attn.q_proj.weight\n",
      "- model.layers.21.self_attn.k_proj.weight\n",
      "- model.layers.21.self_attn.v_proj.weight\n",
      "- model.layers.21.self_attn.o_proj.weight\n",
      "- model.layers.21.mlp.gate_proj.weight\n",
      "- model.layers.21.mlp.up_proj.weight\n",
      "- model.layers.21.mlp.down_proj.weight\n",
      "- model.layers.21.input_layernorm.weight\n",
      "- model.layers.21.post_attention_layernorm.weight\n",
      "- model.layers.22.self_attn.q_proj.weight\n",
      "- model.layers.22.self_attn.k_proj.weight\n",
      "- model.layers.22.self_attn.v_proj.weight\n",
      "- model.layers.22.self_attn.o_proj.weight\n",
      "- model.layers.22.mlp.gate_proj.weight\n",
      "- model.layers.22.mlp.up_proj.weight\n",
      "- model.layers.22.mlp.down_proj.weight\n",
      "- model.layers.22.input_layernorm.weight\n",
      "- model.layers.22.post_attention_layernorm.weight\n",
      "- model.layers.23.self_attn.q_proj.weight\n",
      "- model.layers.23.self_attn.k_proj.weight\n",
      "- model.layers.23.self_attn.v_proj.weight\n",
      "- model.layers.23.self_attn.o_proj.weight\n",
      "- model.layers.23.mlp.gate_proj.weight\n",
      "- model.layers.23.mlp.up_proj.weight\n",
      "- model.layers.23.mlp.down_proj.weight\n",
      "- model.layers.23.input_layernorm.weight\n",
      "- model.layers.23.post_attention_layernorm.weight\n",
      "- model.layers.24.self_attn.q_proj.weight\n",
      "- model.layers.24.self_attn.k_proj.weight\n",
      "- model.layers.24.self_attn.v_proj.weight\n",
      "- model.layers.24.self_attn.o_proj.weight\n",
      "- model.layers.24.mlp.gate_proj.weight\n",
      "- model.layers.24.mlp.up_proj.weight\n",
      "- model.layers.24.mlp.down_proj.weight\n",
      "- model.layers.24.input_layernorm.weight\n",
      "- model.layers.24.post_attention_layernorm.weight\n",
      "- model.layers.25.self_attn.q_proj.weight\n",
      "- model.layers.25.self_attn.k_proj.weight\n",
      "- model.layers.25.self_attn.v_proj.weight\n",
      "- model.layers.25.self_attn.o_proj.weight\n",
      "- model.layers.25.mlp.gate_proj.weight\n",
      "- model.layers.25.mlp.up_proj.weight\n",
      "- model.layers.25.mlp.down_proj.weight\n",
      "- model.layers.25.input_layernorm.weight\n",
      "- model.layers.25.post_attention_layernorm.weight\n",
      "- model.layers.26.self_attn.q_proj.weight\n",
      "- model.layers.26.self_attn.k_proj.weight\n",
      "- model.layers.26.self_attn.v_proj.weight\n",
      "- model.layers.26.self_attn.o_proj.weight\n",
      "- model.layers.26.mlp.gate_proj.weight\n",
      "- model.layers.26.mlp.up_proj.weight\n",
      "- model.layers.26.mlp.down_proj.weight\n",
      "- model.layers.26.input_layernorm.weight\n",
      "- model.layers.26.post_attention_layernorm.weight\n",
      "- model.layers.27.self_attn.q_proj.weight\n",
      "- model.layers.27.self_attn.k_proj.weight\n",
      "- model.layers.27.self_attn.v_proj.weight\n",
      "- model.layers.27.self_attn.o_proj.weight\n",
      "- model.layers.27.mlp.gate_proj.weight\n",
      "- model.layers.27.mlp.up_proj.weight\n",
      "- model.layers.27.mlp.down_proj.weight\n",
      "- model.layers.27.input_layernorm.weight\n",
      "- model.layers.27.post_attention_layernorm.weight\n",
      "- model.layers.28.self_attn.q_proj.weight\n",
      "- model.layers.28.self_attn.k_proj.weight\n",
      "- model.layers.28.self_attn.v_proj.weight\n",
      "- model.layers.28.self_attn.o_proj.weight\n",
      "- model.layers.28.mlp.gate_proj.weight\n",
      "- model.layers.28.mlp.up_proj.weight\n",
      "- model.layers.28.mlp.down_proj.weight\n",
      "- model.layers.28.input_layernorm.weight\n",
      "- model.layers.28.post_attention_layernorm.weight\n",
      "- model.layers.29.self_attn.q_proj.weight\n",
      "- model.layers.29.self_attn.k_proj.weight\n",
      "- model.layers.29.self_attn.v_proj.weight\n",
      "- model.layers.29.self_attn.o_proj.weight\n",
      "- model.layers.29.mlp.gate_proj.weight\n",
      "- model.layers.29.mlp.up_proj.weight\n",
      "- model.layers.29.mlp.down_proj.weight\n",
      "- model.layers.29.input_layernorm.weight\n",
      "- model.layers.29.post_attention_layernorm.weight\n",
      "- model.layers.30.self_attn.q_proj.weight\n",
      "- model.layers.30.self_attn.k_proj.weight\n",
      "- model.layers.30.self_attn.v_proj.weight\n",
      "- model.layers.30.self_attn.o_proj.weight\n",
      "- model.layers.30.mlp.gate_proj.weight\n",
      "- model.layers.30.mlp.up_proj.weight\n",
      "- model.layers.30.mlp.down_proj.weight\n",
      "- model.layers.30.input_layernorm.weight\n",
      "- model.layers.30.post_attention_layernorm.weight\n",
      "- model.layers.31.self_attn.q_proj.weight\n",
      "- model.layers.31.self_attn.k_proj.weight\n",
      "- model.layers.31.self_attn.v_proj.weight\n",
      "- model.layers.31.self_attn.o_proj.weight\n",
      "- model.layers.31.mlp.gate_proj.weight\n",
      "- model.layers.31.mlp.up_proj.weight\n",
      "- model.layers.31.mlp.down_proj.weight\n",
      "- model.layers.31.input_layernorm.weight\n",
      "- model.layers.31.post_attention_layernorm.weight\n",
      "- model.norm.weight\n",
      "- lm_head.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-12-10 15:41:08 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "    \n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nemo_config: {'mcore_gpt': True, 'micro_batch_size': 4, 'global_batch_size': 8, 'tensor_model_parallel_size': 1, 'pipeline_model_parallel_size': 1, 'virtual_pipeline_model_parallel_size': None, 'encoder_seq_length': 131072, 'max_position_embeddings': 131072, 'num_layers': 32, 'hidden_size': 4096, 'ffn_hidden_size': 14336, 'num_attention_heads': 32, 'init_method_std': 0.02, 'use_scaled_init_method': True, 'hidden_dropout': 0.0, 'attention_dropout': 0.0, 'ffn_dropout': 0.0, 'kv_channels': None, 'apply_query_key_layer_scaling': True, 'normalization': 'rmsnorm', 'layernorm_epsilon': 1e-05, 'do_layer_norm_weight_decay': False, 'make_vocab_size_divisible_by': 128, 'pre_process': True, 'post_process': True, 'persist_layer_norm': True, 'bias': False, 'activation': 'fast-swiglu', 'headscale': False, 'transformer_block_type': 'pre_ln', 'openai_gelu': False, 'normalize_attention_scores': True, 'position_embedding_type': 'rope', 'rotary_percentage': 1.0, 'attention_type': 'multihead', 'share_embeddings_and_output_weights': False, 'overlap_p2p_comm': False, 'batch_p2p_comm': True, 'num_query_groups': 8, 'tokenizer': {'library': 'huggingface', 'type': 'models/llama-3.1/8B/hf', 'use_fast': True}, 'native_amp_init_scale': 4294967296, 'native_amp_growth_interval': 1000, 'hysteresis': 2, 'fp32_residual_connection': False, 'fp16_lm_cross_entropy': False, 'megatron_amp_O2': False, 'grad_allreduce_chunk_size_mb': 125, 'grad_div_ar_fusion': True, 'gradient_accumulation_fusion': False, 'bias_activation_fusion': False, 'bias_dropout_add_fusion': False, 'masked_softmax_fusion': True, 'get_attention_mask_from_fusion': True, 'apply_rope_fusion': False, 'seed': 1234, 'resume_from_checkpoint': None, 'use_cpu_initialization': True, 'onnx_safe': False, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'sync_batch_comm': False, 'activations_checkpoint_granularity': None, 'activations_checkpoint_method': None, 'activations_checkpoint_num_layers': None, 'num_micro_batches_with_partial_activation_checkpoints': None, 'activations_checkpoint_layers_per_pipeline': None, 'sequence_parallel': False, 'transformer_engine': True, 'fp8': False, 'fp8_e4m3': False, 'fp8_hybrid': True, 'fp8_margin': 0, 'fp8_interval': 1, 'fp8_amax_history_len': 1024, 'fp8_amax_compute_algo': 'max', 'reduce_amax': True, 'use_emha': False, 'data': {'index_mapping_dir': None, 'data_impl': 'mmap', 'splits_string': '900,50,50', 'seq_length': '${model.encoder_seq_length}', 'skip_warmup': True, 'num_workers': 2, 'dataloader_type': 'single', 'reset_position_ids': False, 'reset_attention_mask': False, 'eod_mask_loss': False, 'validation_drop_last': True, 'no_seqlen_plus_one_input_tokens': False, 'pad_samples_to_global_batch_size': False, 'shuffle_documents': True}, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'optim': {'name': 'fused_adam', 'lr': 0.0002, 'weight_decay': 0.01, 'betas': [0.9, 0.98], 'sched': {'name': 'CosineAnnealing', 'warmup_steps': 500, 'constant_steps': 50000, 'min_lr': 2e-05}}, 'rotary_base': 500000, 'seq_len_interpolation_factor': 8.000000001, 'precision': 'bf16'}\n",
      "converting layer 0\n",
      "done layer 0\n",
      "converting layer 1\n",
      "done layer 1\n",
      "converting layer 2\n",
      "done layer 2\n",
      "converting layer 3\n",
      "done layer 3\n",
      "converting layer 4\n",
      "done layer 4\n",
      "converting layer 5\n",
      "done layer 5\n",
      "converting layer 6\n",
      "done layer 6\n",
      "converting layer 7\n",
      "done layer 7\n",
      "converting layer 8\n",
      "done layer 8\n",
      "converting layer 9\n",
      "done layer 9\n",
      "converting layer 10\n",
      "done layer 10\n",
      "converting layer 11\n",
      "done layer 11\n",
      "converting layer 12\n",
      "done layer 12\n",
      "converting layer 13\n",
      "done layer 13\n",
      "converting layer 14\n",
      "done layer 14\n",
      "converting layer 15\n",
      "done layer 15\n",
      "converting layer 16\n",
      "done layer 16\n",
      "converting layer 17\n",
      "done layer 17\n",
      "converting layer 18\n",
      "done layer 18\n",
      "converting layer 19\n",
      "done layer 19\n",
      "converting layer 20\n",
      "done layer 20\n",
      "converting layer 21\n",
      "done layer 21\n",
      "converting layer 22\n",
      "done layer 22\n",
      "converting layer 23\n",
      "done layer 23\n",
      "converting layer 24\n",
      "done layer 24\n",
      "converting layer 25\n",
      "done layer 25\n",
      "converting layer 26\n",
      "done layer 26\n",
      "converting layer 27\n",
      "done layer 27\n",
      "converting layer 28\n",
      "done layer 28\n",
      "converting layer 29\n",
      "done layer 29\n",
      "converting layer 30\n",
      "done layer 30\n",
      "converting layer 31\n",
      "done layer 31\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:269] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:280] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:283] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:291] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:294] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:295] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:302] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:303] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:312] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:316] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:317] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:337] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:349] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:355] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:356] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:357] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-12-10 15:41:11 megatron_init:358] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:41:11 tokenizer_utils:183] Getting HuggingFace AutoTokenizer with pretrained_model_name: models/llama-3.1/8B/hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:41:11 megatron_base_model:595] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:510] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:41:11 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:41:44 dist_ckpt_io:421] Using TorchDistSaveShardedStrategy(torch_dist, 1) dist-ckpt save strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:42:35 convert_llama_hf_to_nemo:307] NeMo model saved to: models/llama-3.1/8B/nemo/bf16/Llama-3_1-8B-Instruct.nemo\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HF_MODEL=\"models/llama-3.1/8B/hf\"\n",
    "PRECISION=bf16\n",
    "NeMo_MODEL=\"models/llama-3.1/8B/nemo/$PRECISION/Llama-3_1-8B-Instruct.nemo\"\n",
    "\n",
    "# Modify rope_scaling properties\n",
    "[ ! -f \"$HF_MODEL/config.json.bak\" ] && cp \"$HF_MODEL/config.json\" \"$HF_MODEL/config.json.bak\"\n",
    "jq '.rope_scaling = {\"factor\": 8.000000001, \"type\": \"linear\"}' \"$HF_MODEL/config.json\" > /tmp/config.tmp && mv /tmp/config.tmp \"$HF_MODEL/config.json\"\n",
    "\n",
    "export TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "# Convert model to .nemo \n",
    "python3 /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py \\\n",
    "        --input_name_or_path \"$HF_MODEL\" \\\n",
    "        --output_path \"$NeMo_MODEL\" \\\n",
    "        --precision \"$PRECISION\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd2e10e9-5c2f-4f97-b145-40b7d0c524e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/llama-3.1/8B/nemo/bf16/Llama-3_1-8B-Instruct.nemo: POSIX tar archive\n",
      "22G\tmodels/llama-3.1/8B/nemo/bf16/Llama-3_1-8B-Instruct.nemo\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "PRECISION=bf16\n",
    "NeMo_MODEL=\"models/llama-3.1/8B/nemo/$PRECISION/Llama-3_1-8B-Instruct.nemo\"\n",
    "\n",
    "file \"$NeMo_MODEL\"\n",
    "du -sh \"$NeMo_MODEL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6a910-a05e-4ae1-aac4-56e5092be2b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Step-by-Step Instructions\n",
    "\n",
    "This notebook is organized into four main steps:\n",
    "\n",
    "1. **Prepare the Dataset:**\n",
    "   Load and preprocess the PubMedQA dataset, ensuring that it’s correctly formatted and ready for fine-tuning.\n",
    "\n",
    "2. **Run the PEFT Fine-Tuning Script:**\n",
    "   Apply Low-Rank Adaptation (LoRA) Parameter-Efficient Fine-Tuning methods to tailor the Llama 3.3 70B model to the PubMedQA domain.\n",
    "\n",
    "3. **Perform Inference with the NeMo Framework:**\n",
    "   Use the trained model to generate answers to biomedical questions and observe how it performs on real queries.\n",
    "\n",
    "4. **Evaluate Model Accuracy:**\n",
    "   Assess the quality and correctness of the model’s responses to measure improvements gained through the fine-tuning process.\n",
    "   \n",
    "5. **Export Model to TensorRT-LLM Format for Inference:**\n",
    "   use the APIs in the export module to export a NeMo checkpoint to TensorRT-LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5bd31",
   "metadata": {},
   "source": [
    "### Step 1: Prepare the dataset\n",
    "\n",
    "Download the PubMedQA dataset and run the pre-processing script in the cloned directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "944b43c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pubmedqa' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Download the dataset and prep. scripts\n",
    "git clone https://github.com/pubmedqa/pubmedqa.git\n",
    "\n",
    "# split it into train/val/test datasets\n",
    "cd pubmedqa/preprocess\n",
    "python split_dataset.py pqal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025b2d4",
   "metadata": {},
   "source": [
    "The following example shows what a single row looks inside of the PubMedQA train, validation and test splits.\n",
    "\n",
    "```json\n",
    "\"18251357\": {\n",
    "    \"QUESTION\": \"Does histologic chorioamnionitis correspond to clinical chorioamnionitis?\",\n",
    "    \"CONTEXTS\": [\n",
    "        \"To evaluate the degree to which histologic chorioamnionitis, a frequent finding in placentas submitted for histopathologic evaluation, correlates with clinical indicators of infection in the mother.\",\n",
    "        \"A retrospective review was performed on 52 cases with a histologic diagnosis of acute chorioamnionitis from 2,051 deliveries at University Hospital, Newark, from January 2003 to July 2003. Third-trimester placentas without histologic chorioamnionitis (n = 52) served as controls. Cases and controls were selected sequentially. Maternal medical records were reviewed for indicators of maternal infection.\",\n",
    "        \"Histologic chorioamnionitis was significantly associated with the usage of antibiotics (p = 0.0095) and a higher mean white blood cell count (p = 0.018). The presence of 1 or more clinical indicators was significantly associated with the presence of histologic chorioamnionitis (p = 0.019).\"\n",
    "    ],\n",
    "    \"reasoning_required_pred\": \"yes\",\n",
    "    \"reasoning_free_pred\": \"yes\",\n",
    "    \"final_decision\": \"yes\",\n",
    "    \"LONG_ANSWER\": \"Histologic chorioamnionitis is a reliable indicator of infection whether or not it is clinically apparent.\"\n",
    "},\n",
    "```\n",
    "\n",
    "Use the following code to convert the train, validation, and test PubMedQA data into the `JSONL` format that NeMo needs for PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f69729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl(fname):\n",
    "    obj = []\n",
    "    with open(fname, 'rt') as f:\n",
    "        st = f.readline()\n",
    "        while st:\n",
    "            obj.append(json.loads(st))\n",
    "            st = f.readline()\n",
    "    return obj\n",
    "\n",
    "def write_jsonl(fname, json_objs):\n",
    "    with open(fname, 'wt') as f:\n",
    "        for o in json_objs:\n",
    "            f.write(json.dumps(o)+\"\\n\")\n",
    "            \n",
    "def form_question(obj):\n",
    "    st = \"\"    \n",
    "    for i, label in enumerate(obj['LABELS']):\n",
    "        st += f\"{label}: {obj['CONTEXTS'][i]}\\n\"\n",
    "    st += f\"QUESTION: {obj['QUESTION']}\\n\"\n",
    "    st += f\" ### ANSWER (yes|no|maybe): \"\n",
    "    return st\n",
    "\n",
    "def convert_to_jsonl(data_path, output_path):\n",
    "    data = json.load(open(data_path, 'rt'))\n",
    "    json_objs = []\n",
    "    for k in data.keys():\n",
    "        obj = data[k]\n",
    "        prompt = form_question(obj)\n",
    "        completion = obj['final_decision']\n",
    "        json_objs.append({\"input\": prompt, \"output\": f\"<<< {completion} >>>\"})\n",
    "    write_jsonl(output_path, json_objs)\n",
    "    return json_objs\n",
    "\n",
    "\n",
    "test_json_objs = convert_to_jsonl(\"pubmedqa/data/test_set.json\", \"pubmedqa/data/pubmedqa_test.jsonl\")\n",
    "train_json_objs = convert_to_jsonl(\"pubmedqa/data/pqal_fold0/train_set.json\", \"pubmedqa/data/pubmedqa_train.jsonl\")\n",
    "dev_json_objs = convert_to_jsonl(\"pubmedqa/data/pqal_fold0/dev_set.json\", \"pubmedqa/data/pubmedqa_val.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62777542",
   "metadata": {},
   "source": [
    "> `Note:` In the output, we enforce the inclusion of “<<<” and “>>>“ markers which would allow verification of the LoRA tuned model during inference. This is  because the base model can produce “yes” / “no” responses based on zero-shot templates as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddd0f2a",
   "metadata": {},
   "source": [
    "After running the above script, you will see  `pubmedqa_train.jsonl`, `pubmedqa_val.jsonl`, and `pubmedqa_test.jsonl` files appear in the data directory.\n",
    "\n",
    "This is what an example will be formatted like after the script has converted the PubMedQA data into `JSONL` -\n",
    "\n",
    "```json\n",
    "{\"input\": \"QUESTION: Failed IUD insertions in community practice: an under-recognized problem?\\nCONTEXT: The data analysis was conducted to describe the rate of unsuccessful copper T380A intrauterine device (IUD) insertions among women using the IUD for emergency contraception (EC) at community family planning clinics in Utah.\\n ...  ### ANSWER (yes|no|maybe): \",\n",
    "\"output\": \"<<< yes >>>\"}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef7c5d4e-fafb-40df-806d-7c366071d08d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 pubmedqa/data/pubmedqa_train.jsonl\n",
      "50 pubmedqa/data/pubmedqa_val.jsonl\n",
      "500 pubmedqa/data/pubmedqa_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# clear up cached mem-map file\n",
    "rm pubmedqa/data/*idx*\n",
    "\n",
    "wc -l pubmedqa/data/pubmedqa_train.jsonl\n",
    "wc -l pubmedqa/data/pubmedqa_val.jsonl\n",
    "wc -l pubmedqa/data/pubmedqa_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1d887",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Run PEFT finetuning script for LoRA\n",
    "\n",
    "NeMo framework includes a high level python script for fine-tuning  [megatron_gpt_finetuning.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py) that can abstract away some of the lower level API calls. Once you have your model downloaded and the dataset ready, LoRA fine-tuning with NeMo is essentially just running this script!\n",
    "\n",
    "For this demonstration, this training run is capped by `max_steps`, and validation is carried out every `val_check_interval` steps. If the validation loss does not improve after a few checks, training is halted to avoid overfitting.\n",
    "\n",
    "> `NOTE:` In the block of code below, pass the paths to your train, test and validation data files as well as path to the .nemo model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939482b9-40ce-4aac-876c-894a04fe0742",
   "metadata": {},
   "source": [
    "#### Understanding Global Batch Size (GBS) in Multi-GPU Training\n",
    "\n",
    "\n",
    "##### **1. Global Batch Size (GBS)**\n",
    "- **Definition:**\n",
    "  - The **total number of training samples** processed in **one training step** across **all GPUs** involved.\n",
    "\n",
    "##### **2. Data Parallelism (DP)**\n",
    "- **Definition:**\n",
    "  - The **number of GPUs** that each hold a **replica** of the entire model.\n",
    "  - **Function:** Distributes different data batches to each GPU simultaneously.\n",
    "  - **GAS (Gradient Accumulation Steps):** The number of mini-batches over which gradients are accumulated before performing a parameter update.\n",
    "  - **DP formula:**\n",
    "      $$\n",
    "      \\text{Data Parallelism (DP)} = \\frac{\\text{Total GPUs} \\times \\text{Gradient Accumulation Step (GAS)}}{\\text{Tensor Parallelism (TP)} \\times \\text{Pipeline Parallelism (PP)}}\n",
    "      $$\n",
    "\n",
    "\n",
    "##### **3. Micro Batch Size (MB)**\n",
    "- **Definition:**\n",
    "  - The **number of samples** processed **per GPU** in a single forward/backward pass.\n",
    "\n",
    "##### **4. GBS Formula**\n",
    "$$\n",
    "\\text{Global Batch Size (GBS)} = \\text{Data Parallelism (DP)} \\times \\text{Micro Batch Size (MB)}\n",
    "$$\n",
    "\n",
    "##### **5. How to Set GBS**\n",
    "1. **Determine Available GPUs:**\n",
    "   - Total GPUs (e.g., 4 GPUs).\n",
    "2. **Choose Data Parallelism (DP):**\n",
    "   - Decide how many GPUs to use for DP (e.g., DP = 4).\n",
    "3. **Set Micro Batch Size (MB):**\n",
    "   - Based on GPU memory capacity (e.g., MB = 8).\n",
    "4. **Calculate GBS:**\n",
    "   - Use the formula to find GBS (e.g., GBS = 4 × 8 = 32).\n",
    "\n",
    "##### **Best Practices**\n",
    "- **Align GBS with DP and MB:**\n",
    "  - Ensure $\\text{GBS} = \\text{DP} \\times \\text{MB}$.\n",
    "- **Monitor GPU Utilization:**\n",
    "  - Use tools like `nvidia-smi` to ensure all GPUs are effectively utilized.\n",
    "- **Adjust Batch Sizes as Needed:**\n",
    "  - Optimize **MB** based on memory constraints and **GBS** to balance load.\n",
    "- **Utilize Gradient Accumulation:**\n",
    "  - When larger **GBS** is desired but constrained by memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "899a230d-51b2-467e-9ea2-aa59a23f08e6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ucloud/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "[NeMo W 2024-12-10 15:46:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:46:42 megatron_gpt_finetuning:56] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-12-10 15:46:42 megatron_gpt_finetuning:57] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: bf16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 1000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 20\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: results/llama-3.1/8B/bf16\n",
      "      exp_dir: results/llama-3.1/8B/bf16\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: true\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 8\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: models/llama-3.1/8B/nemo/bf16/Llama-3_1-8B-Instruct.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      fsdp: false\n",
      "      fsdp_sharding_strategy: full\n",
      "      fsdp_grad_reduce_dtype: fp32\n",
      "      fsdp_sharded_checkpoint: false\n",
      "      fsdp_use_orig_params: false\n",
      "      peft:\n",
      "        peft_scheme: lora\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          alpha: ${model.peft.lora_tuning.adapter_dim}\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "        selective_tuning:\n",
      "          tunable_base_param_names:\n",
      "          - self_attention\n",
      "          - word_embeddings\n",
      "      data:\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - pubmedqa/data/pubmedqa_train.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 10\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{input} {output}'\n",
      "          truncation_method: right\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - pubmedqa/data/pubmedqa_val.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 10\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 0.0\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "      mcore_gpt: true\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 15:46:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:46:42 exp_manager:396] ExpManager schema\n",
      "[NeMo I 2024-12-10 15:46:42 exp_manager:397] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2024-12-10 15:46:42 exp_manager:830] exp_manager received explicit_log_dir: results/llama-3.1/8B/bf16 and at least one of exp_dir: results/llama-3.1/8B/bf16, or version: None. Please note that exp_dir, name, and version will be ignored.\n",
      "[NeMo W 2024-12-10 15:46:42 exp_manager:757] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :results/llama-3.1/8B/bf16/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:46:42 exp_manager:455] Experiments will be logged at results/llama-3.1/8B/bf16\n",
      "[NeMo I 2024-12-10 15:46:42 exp_manager:983] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 15:46:42 exp_manager:1111] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 1000. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:46:59 megatron_init:269] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:280] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:283] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:291] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:294] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:295] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:302] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:303] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:312] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:316] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:317] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:337] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:349] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:355] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:356] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:357] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-12-10 15:46:59 megatron_init:358] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2024-12-10 15:46:59 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:46:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:46:59 tokenizer_utils:183] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:47:01 megatron_base_model:595] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:510] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 15:47:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo W 2024-12-10 15:47:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/state_dict_loader.py:27: UserWarning: 'load_state_dict' is deprecated and will be removed in future versions. Please use 'load' instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-10 15:47:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/_shard/sharded_tensor/api.py:1132: UserWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "      warnings.warn(DEPRECATE_MSG)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:47:34 nlp_overrides:1346] Model MegatronGPTSFTModel was successfully restored from /work/ucloud-workshop-11-12-2024/models/llama-3.1/8B/nemo/bf16/Llama-3_1-8B-Instruct.nemo.\n",
      "[NeMo I 2024-12-10 15:47:34 megatron_gpt_finetuning:72] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2024-12-10 15:47:34 nlp_adapter_mixins:240] Before adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | train\n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,121.045Total estimated model params size (MB)\n",
      "[NeMo I 2024-12-10 15:47:35 nlp_adapter_mixins:245] After adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | train\n",
      "    -------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 15:47:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-12-10 15:47:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:47:35 megatron_gpt_sft_model:801] Building GPT SFT validation datasets.\n",
      "[NeMo I 2024-12-10 15:47:35 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-12-10 15:47:35 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:47:35 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.067935\n",
      "[NeMo I 2024-12-10 15:47:35 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:47:35 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.068146\n",
      "[NeMo I 2024-12-10 15:47:35 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-12-10 15:47:35 text_memmap_dataset:249] Loading pubmedqa/data/pubmedqa_val.jsonl\n",
      "[NeMo I 2024-12-10 15:47:35 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001661\n",
      "[NeMo I 2024-12-10 15:47:35 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-12-10 15:47:35 megatron_gpt_sft_model:805] Length of val dataset: 50\n",
      "[NeMo I 2024-12-10 15:47:35 megatron_gpt_sft_model:812] Building GPT SFT traing datasets.\n",
      "[NeMo I 2024-12-10 15:47:35 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-12-10 15:47:35 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:47:35 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.067487\n",
      "[NeMo I 2024-12-10 15:47:35 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:47:36 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.067611\n",
      "[NeMo I 2024-12-10 15:47:36 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-12-10 15:47:36 text_memmap_dataset:249] Loading pubmedqa/data/pubmedqa_train.jsonl\n",
      "[NeMo I 2024-12-10 15:47:36 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001484\n",
      "[NeMo I 2024-12-10 15:47:36 text_memmap_dataset:165] Computing global indices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 15:47:36 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1332: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:79.)\n",
      "      counts = torch.cuda.LongTensor([1])\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2024-12-10 15:47:38 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.06 (sec)\n",
      "[NeMo I 2024-12-10 15:47:38 megatron_gpt_sft_model:814] Length of train dataset: 8040\n",
      "[NeMo I 2024-12-10 15:47:38 megatron_gpt_sft_model:819] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-12-10 15:47:38 megatron_gpt_sft_model:819] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:47:38 nlp_overrides:268] Configuring DDP for model parallelism.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 15:47:38 megatron_base_model:1223] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 15:47:38 nlp_adapter_mixins:324] Optimizer groups set:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | train\n",
      "    -------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n",
      "[NeMo I 2024-12-10 15:47:38 modelPT:786] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-12-10 15:47:38 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fd1e26d0370>\" \n",
      "    will be used during training (effective maximum steps = 1000) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 1000\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type     | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | model | GPTModel | 8.0 B  | train\n",
      "-------------------------------------------\n",
      "10.5 M    Trainable params\n",
      "8.0 B     Non-trainable params\n",
      "8.0 B     Total params\n",
      "32,162.988Total estimated model params size (MB)\n",
      "[NeMo W 2024-12-10 15:47:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s][NeMo I 2024-12-10 15:49:06 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:03<00:00,  0.55it/s][NeMo I 2024-12-10 15:49:09 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 15:49:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-10 15:49:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-10 15:49:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-10 15:49:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :   2%|▏         | 20/1000 [00:20<16:27, reduced_train_loss=5.410, global_step=19.00, consumed_samples=160.0, train_step_timing in s=0.741]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 15:50:57 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:00<00:02,  2.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:00<00:02,  2.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:01<00:01,  2.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:01<00:01,  2.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:02<00:00,  2.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:02<00:00,  2.26it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:03<00:00,  2.30it/s]\u001b[A[NeMo I 2024-12-10 15:51:00 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 5.121\n",
      "Epoch 0, global step 20: 'validation_loss' reached 5.12138 (best 5.12138), saving model to '/work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=5.121-step=20-consumed_samples=160.0.ckpt' as top 1\n",
      "[NeMo W 2024-12-10 15:51:01 nlp_overrides:609] DistributedCheckpointIO configured but should not be used. Reverting back to TorchCheckpointIO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :   4%|▍         | 40/1000 [00:39<15:37, reduced_train_loss=0.371, global_step=39.00, consumed_samples=320.0, train_step_timing in s=0.754, val_loss=5.120]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 15:51:16 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:00<00:02,  2.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:00<00:02,  2.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:01<00:01,  2.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:01<00:01,  2.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:02<00:00,  2.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]\u001b[A[NeMo I 2024-12-10 15:51:19 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 4.808 >= min_delta = 0.001. New best score: 0.313\n",
      "Epoch 0, global step 40: 'validation_loss' reached 0.31326 (best 0.31326), saving model to '/work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.313-step=40-consumed_samples=320.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :   4%|▍         | 40/1000 [00:41<16:45, reduced_train_loss=0.371, global_step=39.00, consumed_samples=320.0, train_step_timing in s=0.754, val_loss=0.313][NeMo I 2024-12-10 15:51:19 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=5.121-step=20-consumed_samples=160.0.ckpt\n",
      "[NeMo I 2024-12-10 15:51:20 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=5.121-step=20-consumed_samples=160.0-last.ckpt\n",
      "Epoch 0: :   6%|▌         | 60/1000 [00:57<14:53, reduced_train_loss=0.243, global_step=59.00, consumed_samples=480.0, train_step_timing in s=0.752, val_loss=0.313]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 15:51:34 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:00<00:02,  2.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:00<00:02,  2.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:01<00:01,  2.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:01<00:01,  2.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]\u001b[A[NeMo I 2024-12-10 15:51:37 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.024 >= min_delta = 0.001. New best score: 0.289\n",
      "Epoch 0, global step 60: 'validation_loss' reached 0.28923 (best 0.28923), saving model to '/work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.289-step=60-consumed_samples=480.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :   6%|▌         | 60/1000 [00:59<15:39, reduced_train_loss=0.243, global_step=59.00, consumed_samples=480.0, train_step_timing in s=0.752, val_loss=0.289][NeMo I 2024-12-10 15:51:37 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.313-step=40-consumed_samples=320.0.ckpt\n",
      "[NeMo I 2024-12-10 15:51:38 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.313-step=40-consumed_samples=320.0-last.ckpt\n",
      "Epoch 0: :   8%|▊         | 80/1000 [01:15<14:27, reduced_train_loss=0.225, global_step=79.00, consumed_samples=640.0, train_step_timing in s=0.737, val_loss=0.289]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 15:51:53 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:00<00:02,  2.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:00<00:02,  2.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:01<00:01,  2.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:01<00:01,  2.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]\u001b[A[NeMo I 2024-12-10 15:51:56 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.027 >= min_delta = 0.001. New best score: 0.262\n",
      "Epoch 0, global step 80: 'validation_loss' reached 0.26176 (best 0.26176), saving model to '/work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.262-step=80-consumed_samples=640.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :   8%|▊         | 80/1000 [01:18<15:00, reduced_train_loss=0.225, global_step=79.00, consumed_samples=640.0, train_step_timing in s=0.737, val_loss=0.262][NeMo I 2024-12-10 15:51:56 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.289-step=60-consumed_samples=480.0.ckpt\n",
      "[NeMo I 2024-12-10 15:51:56 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.289-step=60-consumed_samples=480.0-last.ckpt\n",
      "Epoch 0: :  10%|█         | 100/1000 [01:33<14:01, reduced_train_loss=0.217, global_step=99.00, consumed_samples=800.0, train_step_timing in s=0.735, val_loss=0.262]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 15:52:11 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:00<00:02,  2.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:00<00:02,  2.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:01<00:01,  2.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:01<00:01,  2.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]\u001b[A[NeMo I 2024-12-10 15:52:14 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 100: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  10%|█         | 100/1000 [01:36<14:26, reduced_train_loss=0.217, global_step=99.00, consumed_samples=800.0, train_step_timing in s=0.735, val_loss=0.277][NeMo I 2024-12-10 15:52:14 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.262-step=80-consumed_samples=640.0-last.ckpt\n",
      "Epoch 0: :  12%|█▏        | 120/1000 [01:51<13:36, reduced_train_loss=0.454, global_step=119.0, consumed_samples=960.0, train_step_timing in s=0.756, val_loss=0.277]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 15:52:29 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:00<00:02,  2.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:00<00:02,  2.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:01<00:01,  2.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:01<00:01,  2.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]\u001b[A[NeMo I 2024-12-10 15:52:31 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 120: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  12%|█▏        | 120/1000 [01:54<13:57, reduced_train_loss=0.454, global_step=119.0, consumed_samples=960.0, train_step_timing in s=0.756, val_loss=0.395][NeMo I 2024-12-10 15:52:32 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.277-step=100-consumed_samples=800.0-last.ckpt\n",
      "Epoch 0: :  14%|█▍        | 140/1000 [02:09<13:13, reduced_train_loss=0.176, global_step=139.0, consumed_samples=1120.0, train_step_timing in s=0.737, val_loss=0.395]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 15:52:47 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:00<00:02,  2.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:00<00:02,  2.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:01<00:01,  2.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:01<00:01,  2.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]\u001b[A[NeMo I 2024-12-10 15:52:49 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 140: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  14%|█▍        | 140/1000 [02:12<13:31, reduced_train_loss=0.176, global_step=139.0, consumed_samples=1120.0, train_step_timing in s=0.737, val_loss=0.333][NeMo I 2024-12-10 15:52:49 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.395-step=120-consumed_samples=960.0-last.ckpt\n",
      "Epoch 0: :  16%|█▌        | 160/1000 [02:27<12:53, reduced_train_loss=0.226, global_step=159.0, consumed_samples=1280.0, train_step_timing in s=0.741, val_loss=0.333] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 15:53:05 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:00<00:02,  2.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:00<00:02,  2.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:01<00:01,  2.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:01<00:01,  2.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]\u001b[A[NeMo I 2024-12-10 15:53:08 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 160: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  16%|█▌        | 160/1000 [02:30<13:08, reduced_train_loss=0.226, global_step=159.0, consumed_samples=1280.0, train_step_timing in s=0.741, val_loss=0.294][NeMo I 2024-12-10 15:53:08 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.333-step=140-consumed_samples=1120.0-last.ckpt\n",
      "Epoch 0: :  18%|█▊        | 180/1000 [02:45<12:33, reduced_train_loss=0.229, global_step=179.0, consumed_samples=1440.0, train_step_timing in s=0.740, val_loss=0.294]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 15:53:23 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:00<00:02,  2.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:00<00:02,  2.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:01<00:01,  2.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:01<00:01,  2.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]\u001b[A[NeMo I 2024-12-10 15:53:25 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 180: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  18%|█▊        | 180/1000 [02:48<12:45, reduced_train_loss=0.229, global_step=179.0, consumed_samples=1440.0, train_step_timing in s=0.740, val_loss=0.328][NeMo I 2024-12-10 15:53:26 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.294-step=160-consumed_samples=1280.0-last.ckpt\n",
      "Epoch 0: :  20%|██        | 200/1000 [03:03<12:13, reduced_train_loss=0.240, global_step=199.0, consumed_samples=1600.0, train_step_timing in s=0.738, val_loss=0.328] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 15:53:41 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:00<00:02,  2.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:00<00:02,  2.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:01<00:01,  2.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:01<00:01,  2.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:02<00:00,  2.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:02<00:00,  2.49it/s]\u001b[A[NeMo I 2024-12-10 15:53:44 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 200: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  20%|██        | 200/1000 [03:06<12:25, reduced_train_loss=0.240, global_step=199.0, consumed_samples=1600.0, train_step_timing in s=0.738, val_loss=0.290][NeMo I 2024-12-10 15:53:44 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.328-step=180-consumed_samples=1440.0-last.ckpt\n",
      "Epoch 0: :  22%|██▏       | 220/1000 [03:21<11:53, reduced_train_loss=0.170, global_step=219.0, consumed_samples=1760.0, train_step_timing in s=0.735, val_loss=0.290]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 15:53:59 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:00<00:02,  2.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:00<00:02,  2.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:01<00:01,  2.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:01<00:01,  2.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]\u001b[A[NeMo I 2024-12-10 15:54:01 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 220: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  22%|██▏       | 220/1000 [03:24<12:03, reduced_train_loss=0.170, global_step=219.0, consumed_samples=1760.0, train_step_timing in s=0.735, val_loss=0.280][NeMo I 2024-12-10 15:54:02 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.290-step=200-consumed_samples=1600.0-last.ckpt\n",
      "Epoch 0: :  24%|██▍       | 240/1000 [03:39<11:34, reduced_train_loss=0.0792, global_step=239.0, consumed_samples=1920.0, train_step_timing in s=0.738, val_loss=0.280]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 15:54:17 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:00<00:02,  2.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:00<00:02,  2.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:01<00:01,  2.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:01<00:01,  2.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]\u001b[A[NeMo I 2024-12-10 15:54:19 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 240: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  24%|██▍       | 240/1000 [03:42<11:43, reduced_train_loss=0.0792, global_step=239.0, consumed_samples=1920.0, train_step_timing in s=0.738, val_loss=0.284][NeMo I 2024-12-10 15:54:20 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.280-step=220-consumed_samples=1760.0-last.ckpt\n",
      "Epoch 0: :  26%|██▌       | 260/1000 [03:57<11:14, reduced_train_loss=0.0281, global_step=259.0, consumed_samples=2080.0, train_step_timing in s=0.742, val_loss=0.284]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 15:54:34 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:00<00:02,  2.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:00<00:02,  2.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:01<00:01,  2.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:01<00:01,  2.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]\u001b[A[NeMo I 2024-12-10 15:54:37 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 260: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  26%|██▌       | 260/1000 [03:59<11:22, reduced_train_loss=0.0281, global_step=259.0, consumed_samples=2080.0, train_step_timing in s=0.742, val_loss=0.494][NeMo I 2024-12-10 15:54:37 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.284-step=240-consumed_samples=1920.0-last.ckpt\n",
      "Epoch 0: :  28%|██▊       | 280/1000 [04:15<10:56, reduced_train_loss=0.0584, global_step=279.0, consumed_samples=2240.0, train_step_timing in s=0.743, val_loss=0.494]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 15:54:53 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:00<00:02,  2.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:00<00:02,  2.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:01<00:01,  2.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:01<00:01,  2.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]\u001b[A[NeMo I 2024-12-10 15:54:55 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 0.262. Signaling Trainer to stop.\n",
      "Epoch 0, global step 280: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  28%|██▊       | 280/1000 [04:18<11:03, reduced_train_loss=0.0584, global_step=279.0, consumed_samples=2240.0, train_step_timing in s=0.743, val_loss=0.394][NeMo I 2024-12-10 15:54:56 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.494-step=260-consumed_samples=2080.0-last.ckpt\n",
      "Epoch 0: :  28%|██▊       | 280/1000 [04:18<11:04, reduced_train_loss=0.0584, global_step=279.0, consumed_samples=2240.0, train_step_timing in s=0.743, val_loss=0.394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.262-step=80-consumed_samples=640.0.ckpt\n",
      "Restored all states from the checkpoint at /work/ucloud-workshop-11-12-2024/results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.262-step=80-consumed_samples=640.0.ckpt\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "# Set paths to the model, train, validation and test sets.\n",
    "PRECISION=bf16\n",
    "MODEL=\"models/llama-3.1/8B/nemo/$PRECISION/Llama-3_1-8B-Instruct.nemo\"\n",
    "OUTPUT_DIR=\"results/llama-3.1/8B/$PRECISION\"\n",
    "rm -rf \"$OUTPUT_DIR\"\n",
    "\n",
    "TRAIN_DS=\"[pubmedqa/data/pubmedqa_train.jsonl]\"\n",
    "VALID_DS=\"[pubmedqa/data/pubmedqa_val.jsonl]\"\n",
    "\n",
    "SCHEME=\"lora\"\n",
    "GPUS=1       # set equal to 4 for 70B model\n",
    "TP_SIZE=1    # set equal to 4 for 70B model\n",
    "PP_SIZE=1\n",
    "\n",
    "torchrun --nproc_per_node=${GPUS} \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    exp_manager.exp_dir=${OUTPUT_DIR} \\\n",
    "    exp_manager.explicit_log_dir=${OUTPUT_DIR} \\\n",
    "    trainer.devices=${GPUS} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=${PRECISION} \\\n",
    "    trainer.val_check_interval=20 \\\n",
    "    trainer.max_steps=1000 \\\n",
    "    model.megatron_amp_O2=False \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    model.global_batch_size=8 \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.data.train_ds.num_workers=10 \\\n",
    "    model.data.validation_ds.num_workers=10 \\\n",
    "    model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "    model.peft.peft_scheme=${SCHEME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4331fd-da30-4e29-8477-3085118e4a7b",
   "metadata": {},
   "source": [
    "This will create a LoRA adapter - a file named `megatron_gpt_peft_lora_tuning.nemo` in `./results/.../checkpoints/`. We'll use this later.\n",
    "\n",
    "To further configure the run above -\n",
    "\n",
    "* **A different PEFT technique**: The `peft.peft_scheme` parameter determines the technique being used. In this case, we did LoRA, but NeMo Framework supports other techniques as well - such as P-tuning, Adapters, and IA3. For more information, refer to the [PEFT support matrix](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/nlp/nemo_megatron/peft/landing_page.html). For example, for P-tuning, simply set \n",
    "\n",
    "```bash\n",
    "model.peft.peft_scheme=\"ptuning\" # instead of \"lora\"\n",
    "```\n",
    "\n",
    "* **Tuning Llama-3.1 70B**: You will need 4xH100 GPUs. Provide the path to it's .nemo checkpoint (similar to the download and conversion steps earlier), and change the model parallelization settings for Llama-3 70B PEFT to distribute across the GPUs. It is also recommended to run the fine-tuning script from a terminal directly instead of Jupyter when using more than 1 GPU.\n",
    "```bash\n",
    "model.tensor_model_parallel_size=4\n",
    "model.pipeline_model_parallel_size=1\n",
    "```\n",
    "\n",
    "You can override many such configurations while running the script. A full set of possible configurations is located in [NeMo Framework Github](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/conf/megatron_gpt_finetuning_config.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53979a4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Inference with NeMo Framework\n",
    "\n",
    "Running text generation within the framework is also possible with running a Python script. Note that is more for testing and validation, not a full-fledged  deployment solution like NVIDIA NIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00d1e3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 286968\n",
      "-rw-r--r--. 1 ucloud ucloud 125934382 Dec 10 15:51 megatron_gpt_peft_lora_tuning--validation_loss=0.262-step=80-consumed_samples=640.0.ckpt\n",
      "-rw-r--r--. 1 ucloud ucloud 125934382 Dec 10 15:54 megatron_gpt_peft_lora_tuning--validation_loss=0.394-step=280-consumed_samples=2240.0-last.ckpt\n",
      "-rw-r--r--. 1 ucloud ucloud  41984000 Dec 10 15:54 megatron_gpt_peft_lora_tuning.nemo\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Check that the LORA model file exists\n",
    "\n",
    "python -c \"import torch; torch.cuda.empty_cache()\"\n",
    "\n",
    "PRECISION=bf16\n",
    "OUTPUT_DIR=\"results/llama-3.1/8B/$PRECISION\"\n",
    "ls -l $OUTPUT_DIR/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3430a0b0-05a0-4179-8750-151d492bb9ae",
   "metadata": {},
   "source": [
    "In the code snippet below, the following configurations are worth noting: \n",
    "\n",
    "1. `model.restore_from_path` to the path for the Meta-Llama-3-8B-Instruct.nemo file.\n",
    "2. `model.peft.restore_from_path` to the path for the PEFT checkpoint that was created in the fine-tuning run in the last step.\n",
    "3. `model.test_ds.file_names` to the path of the pubmedqa_test.jsonl file\n",
    "\n",
    "If you have made any changes in model or experiment paths, please ensure they are configured correctly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93108124-32a5-4c8f-ab25-52dbe9b26ac6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ucloud/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "[NeMo W 2024-12-10 16:10:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 16:10:06 megatron_gpt_generate:125] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-12-10 16:10:06 megatron_gpt_generate:126] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 20000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 200\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.test_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: max\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: true\n",
      "        save_best_model: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 1\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: models/llama-3.1/8B/nemo/bf16/Llama-3_1-8B-Instruct.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: true\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: adapter\n",
      "        restore_from_path: results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning.nemo\n",
      "        restore_from_ckpt:\n",
      "          checkpoint_dir: null\n",
      "          checkpoint_name: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        test_ds:\n",
      "          file_names:\n",
      "          - pubmedqa/data/pubmedqa_test.jsonl\n",
      "          names:\n",
      "          - pubmedqa\n",
      "          global_batch_size: 1\n",
      "          micro_batch_size: 1\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          context_key: input\n",
      "          label_key: ${data.train_ds.label_key}\n",
      "          add_eos: ${data.train_ds.add_eos}\n",
      "          add_sep: ${data.train_ds.add_sep}\n",
      "          add_bos: ${data.train_ds.add_bos}\n",
      "          write_predictions_to_file: true\n",
      "          output_file_path_prefix: pubmedQA_result_\n",
      "          truncation_field: ${data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 3\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "    inference:\n",
      "      greedy: true\n",
      "      top_k: 0\n",
      "      top_p: 0.9\n",
      "      temperature: 1.0\n",
      "      all_probs: false\n",
      "      repetition_penalty: 1.0\n",
      "      min_tokens_to_generate: 0\n",
      "      compute_logprob: false\n",
      "      outfile_path: output.txt\n",
      "      compute_attention_mask: true\n",
      "    server: false\n",
      "    port: 5555\n",
      "    web_server: false\n",
      "    share: true\n",
      "    username: test\n",
      "    password: test2\n",
      "    web_port: 9889\n",
      "    chat: false\n",
      "    chatbot_config:\n",
      "      value: false\n",
      "      attributes:\n",
      "      - name: Quality\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: quality\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Toxicity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: toxcity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Humor\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: humor\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Creativity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: creativity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Violence\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: violence\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Helpfulness\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: helpfulness\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Not_Appropriate\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: not_appropriate\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Language\n",
      "        choices:\n",
      "        - ar\n",
      "        - bg\n",
      "        - bn\n",
      "        - ca\n",
      "        - cs\n",
      "        - da\n",
      "        - de\n",
      "        - el\n",
      "        - en\n",
      "        - eo\n",
      "        - es\n",
      "        - eu\n",
      "        - fa\n",
      "        - fi\n",
      "        - fr\n",
      "        - gl\n",
      "        - he\n",
      "        - hu\n",
      "        - id\n",
      "        - it\n",
      "        - ja\n",
      "        - ko\n",
      "        - nb\n",
      "        - nl\n",
      "        - pl\n",
      "        - pt\n",
      "        - ro\n",
      "        - ru\n",
      "        - sk\n",
      "        - sv\n",
      "        - th\n",
      "        - tr\n",
      "        - uk\n",
      "        - vi\n",
      "        - zh\n",
      "        key: lang\n",
      "        type: list\n",
      "        default: en\n",
      "      user: User\n",
      "      assistant: Assistant\n",
      "      system: 'A chat between a curious human and an artificial intelligence assistant.\n",
      "        The assistant gives helpful, detailed, and polite answers to the human''s questions.\n",
      "    \n",
      "    \n",
      "        '\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 16:10:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 16:10:24 megatron_init:269] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:280] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:283] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:291] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:294] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:295] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:302] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:303] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:312] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:316] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:317] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:337] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:349] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:355] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:356] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:357] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-12-10 16:10:24 megatron_init:358] Rank 0 has embedding rank: 0\n",
      "setting number of micro-batches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 16:10:24 tokenizer_utils:183] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 16:10:25 megatron_base_model:595] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 16:10:25 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo W 2024-12-10 16:10:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/state_dict_loader.py:27: UserWarning: 'load_state_dict' is deprecated and will be removed in future versions. Please use 'load' instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-10 16:10:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/_shard/sharded_tensor/api.py:1132: UserWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "      warnings.warn(DEPRECATE_MSG)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 16:10:57 nlp_overrides:1346] Model MegatronGPTSFTModel was successfully restored from /work/ucloud-workshop-11-12-2024/models/llama-3.1/8B/nemo/bf16/Llama-3_1-8B-Instruct.nemo.\n",
      "[NeMo I 2024-12-10 16:10:57 nlp_adapter_mixins:240] Before adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | train\n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,121.045Total estimated model params size (MB)\n",
      "[NeMo I 2024-12-10 16:10:59 nlp_adapter_mixins:245] After adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | train\n",
      "    -------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n",
      "[NeMo I 2024-12-10 16:10:59 megatron_gpt_generate:156] Freezing parameters for PEFT eval:\n",
      "      | Name  | Type     | Params | Mode\n",
      "    ------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | eval\n",
      "    ------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 16:10:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-12-10 16:10:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 16:10:59 megatron_gpt_sft_model:793] Building GPT SFT test datasets.\n",
      "[NeMo I 2024-12-10 16:10:59 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-12-10 16:10:59 text_memmap_dataset:525] Processing 1 data files using 96 workers\n",
      "[NeMo I 2024-12-10 16:11:01 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:01.866361\n",
      "[NeMo I 2024-12-10 16:11:01 text_memmap_dataset:525] Processing 1 data files using 96 workers\n",
      "[NeMo I 2024-12-10 16:11:02 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:01.881394\n",
      "[NeMo I 2024-12-10 16:11:02 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-12-10 16:11:02 text_memmap_dataset:249] Loading pubmedqa/data/pubmedqa_test.jsonl\n",
      "[NeMo I 2024-12-10 16:11:02 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001769\n",
      "[NeMo I 2024-12-10 16:11:02 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-12-10 16:11:02 megatron_gpt_sft_model:796] Length of test dataset: 500\n",
      "[NeMo I 2024-12-10 16:11:02 megatron_gpt_sft_model:819] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "[NeMo W 2024-12-10 16:11:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=191` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-12-10 16:11:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `test_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: |          | 0/? [00:00<?, ?it/s]setting number of micro-batches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 16:11:06 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:484: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:79.)\n",
      "      input_info_tensor = torch.cuda.FloatTensor(input_info)\n",
      "    \n",
      "[NeMo W 2024-12-10 16:11:07 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:492: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "      string_tensor = torch.as_tensor(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/500 [00:00<?, ?it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   0%|          | 1/500 [00:07<1:00:32,  0.14it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   0%|          | 2/500 [00:07<31:14,  0.27it/s]  setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   1%|          | 3/500 [00:07<21:27,  0.39it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   1%|          | 4/500 [00:08<16:40,  0.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   1%|          | 5/500 [00:08<13:43,  0.60it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   1%|          | 6/500 [00:08<11:49,  0.70it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   1%|▏         | 7/500 [00:09<10:43,  0.77it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   2%|▏         | 8/500 [00:09<09:37,  0.85it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   2%|▏         | 9/500 [00:09<08:45,  0.93it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   2%|▏         | 10/500 [00:09<08:03,  1.01it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   2%|▏         | 11/500 [00:10<07:39,  1.06it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   2%|▏         | 12/500 [00:10<07:11,  1.13it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   3%|▎         | 13/500 [00:10<06:46,  1.20it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   3%|▎         | 14/500 [00:11<06:25,  1.26it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   3%|▎         | 15/500 [00:11<06:08,  1.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   3%|▎         | 16/500 [00:11<05:53,  1.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   3%|▎         | 17/500 [00:11<05:40,  1.42it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   4%|▎         | 18/500 [00:12<05:28,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   4%|▍         | 19/500 [00:12<05:18,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   4%|▍         | 20/500 [00:12<05:08,  1.56it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   4%|▍         | 21/500 [00:13<04:59,  1.60it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   4%|▍         | 22/500 [00:13<04:50,  1.64it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   5%|▍         | 23/500 [00:13<04:42,  1.69it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   5%|▍         | 24/500 [00:13<04:36,  1.72it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   5%|▌         | 25/500 [00:14<04:30,  1.76it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   5%|▌         | 26/500 [00:14<04:28,  1.77it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   5%|▌         | 27/500 [00:14<04:21,  1.81it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   6%|▌         | 28/500 [00:15<04:16,  1.84it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   6%|▌         | 29/500 [00:15<04:14,  1.85it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   6%|▌         | 30/500 [00:15<04:09,  1.89it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   6%|▌         | 31/500 [00:16<04:04,  1.92it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   6%|▋         | 32/500 [00:16<04:00,  1.95it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   7%|▋         | 33/500 [00:16<03:56,  1.98it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   7%|▋         | 34/500 [00:16<03:52,  2.00it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   7%|▋         | 35/500 [00:17<03:49,  2.03it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   7%|▋         | 36/500 [00:17<03:45,  2.06it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   7%|▋         | 37/500 [00:17<03:41,  2.09it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   8%|▊         | 38/500 [00:17<03:38,  2.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   8%|▊         | 39/500 [00:18<03:35,  2.14it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   8%|▊         | 40/500 [00:18<03:32,  2.17it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   8%|▊         | 41/500 [00:18<03:29,  2.19it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   8%|▊         | 42/500 [00:18<03:26,  2.22it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   9%|▊         | 43/500 [00:19<03:24,  2.24it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   9%|▉         | 44/500 [00:19<03:21,  2.26it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   9%|▉         | 45/500 [00:19<03:19,  2.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   9%|▉         | 46/500 [00:20<03:19,  2.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   9%|▉         | 47/500 [00:20<03:17,  2.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  10%|▉         | 48/500 [00:20<03:15,  2.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  10%|▉         | 49/500 [00:20<03:13,  2.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  10%|█         | 50/500 [00:21<03:11,  2.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  10%|█         | 51/500 [00:21<03:09,  2.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  10%|█         | 52/500 [00:21<03:07,  2.39it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  11%|█         | 53/500 [00:21<03:05,  2.41it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  11%|█         | 54/500 [00:22<03:03,  2.42it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  11%|█         | 55/500 [00:22<03:02,  2.44it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  11%|█         | 56/500 [00:22<03:00,  2.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  11%|█▏        | 57/500 [00:23<02:59,  2.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  12%|█▏        | 58/500 [00:23<02:57,  2.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  12%|█▏        | 59/500 [00:23<02:56,  2.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  12%|█▏        | 60/500 [00:23<02:54,  2.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  12%|█▏        | 61/500 [00:24<02:53,  2.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  12%|█▏        | 62/500 [00:24<02:52,  2.54it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  13%|█▎        | 63/500 [00:24<02:51,  2.55it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  13%|█▎        | 64/500 [00:25<02:50,  2.56it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  13%|█▎        | 65/500 [00:25<02:49,  2.57it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  13%|█▎        | 66/500 [00:25<02:48,  2.58it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  13%|█▎        | 67/500 [00:25<02:47,  2.59it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  14%|█▎        | 68/500 [00:26<02:46,  2.59it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  14%|█▍        | 69/500 [00:26<02:45,  2.61it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  14%|█▍        | 70/500 [00:26<02:44,  2.62it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  14%|█▍        | 71/500 [00:26<02:42,  2.63it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  14%|█▍        | 72/500 [00:27<02:41,  2.65it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  15%|█▍        | 73/500 [00:27<02:40,  2.65it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  15%|█▍        | 74/500 [00:27<02:40,  2.66it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  15%|█▌        | 75/500 [00:28<02:38,  2.67it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  15%|█▌        | 76/500 [00:28<02:38,  2.68it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  15%|█▌        | 77/500 [00:28<02:37,  2.69it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  16%|█▌        | 78/500 [00:28<02:36,  2.70it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  16%|█▌        | 79/500 [00:29<02:35,  2.70it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  16%|█▌        | 80/500 [00:29<02:35,  2.71it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  16%|█▌        | 81/500 [00:29<02:34,  2.72it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  16%|█▋        | 82/500 [00:30<02:33,  2.72it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  17%|█▋        | 83/500 [00:30<02:32,  2.73it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  17%|█▋        | 84/500 [00:30<02:31,  2.74it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  17%|█▋        | 85/500 [00:30<02:31,  2.75it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  17%|█▋        | 86/500 [00:31<02:30,  2.76it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  17%|█▋        | 87/500 [00:31<02:29,  2.77it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  18%|█▊        | 88/500 [00:31<02:28,  2.77it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  18%|█▊        | 89/500 [00:31<02:27,  2.78it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  18%|█▊        | 90/500 [00:32<02:26,  2.79it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  18%|█▊        | 91/500 [00:32<02:26,  2.80it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  18%|█▊        | 92/500 [00:32<02:25,  2.81it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  19%|█▊        | 93/500 [00:33<02:24,  2.81it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  19%|█▉        | 94/500 [00:33<02:24,  2.82it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  19%|█▉        | 95/500 [00:33<02:23,  2.83it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  19%|█▉        | 96/500 [00:33<02:22,  2.83it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  19%|█▉        | 97/500 [00:34<02:21,  2.84it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  20%|█▉        | 98/500 [00:34<02:21,  2.85it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  20%|█▉        | 99/500 [00:34<02:20,  2.86it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  20%|██        | 100/500 [00:34<02:19,  2.87it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  20%|██        | 101/500 [00:35<02:18,  2.87it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  20%|██        | 102/500 [00:35<02:18,  2.88it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  21%|██        | 103/500 [00:35<02:17,  2.88it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  21%|██        | 104/500 [00:36<02:17,  2.89it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  21%|██        | 105/500 [00:36<02:16,  2.89it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  21%|██        | 106/500 [00:36<02:15,  2.90it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  21%|██▏       | 107/500 [00:36<02:15,  2.91it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  22%|██▏       | 108/500 [00:37<02:14,  2.91it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  22%|██▏       | 109/500 [00:37<02:13,  2.92it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  22%|██▏       | 110/500 [00:37<02:14,  2.91it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  22%|██▏       | 111/500 [00:38<02:13,  2.91it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  22%|██▏       | 112/500 [00:38<02:13,  2.91it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  23%|██▎       | 113/500 [00:38<02:12,  2.92it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  23%|██▎       | 114/500 [00:38<02:11,  2.93it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  23%|██▎       | 115/500 [00:39<02:11,  2.93it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  23%|██▎       | 116/500 [00:39<02:10,  2.94it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  23%|██▎       | 117/500 [00:39<02:10,  2.95it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  24%|██▎       | 118/500 [00:39<02:09,  2.95it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  24%|██▍       | 119/500 [00:40<02:08,  2.96it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  24%|██▍       | 120/500 [00:40<02:08,  2.96it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  24%|██▍       | 121/500 [00:40<02:07,  2.97it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  24%|██▍       | 122/500 [00:41<02:07,  2.98it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  25%|██▍       | 123/500 [00:41<02:06,  2.98it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  25%|██▍       | 124/500 [00:41<02:06,  2.98it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  25%|██▌       | 125/500 [00:41<02:05,  2.99it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  25%|██▌       | 126/500 [00:42<02:05,  2.98it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  25%|██▌       | 127/500 [00:42<02:05,  2.98it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  26%|██▌       | 128/500 [00:42<02:04,  2.99it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  26%|██▌       | 129/500 [00:43<02:03,  2.99it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  26%|██▌       | 130/500 [00:43<02:03,  3.00it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  26%|██▌       | 131/500 [00:43<02:02,  3.00it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  26%|██▋       | 132/500 [00:43<02:02,  3.00it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  27%|██▋       | 133/500 [00:44<02:01,  3.01it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  27%|██▋       | 134/500 [00:44<02:01,  3.01it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  27%|██▋       | 135/500 [00:44<02:00,  3.02it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  27%|██▋       | 136/500 [00:44<02:00,  3.02it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  27%|██▋       | 137/500 [00:45<01:59,  3.03it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  28%|██▊       | 138/500 [00:45<01:59,  3.03it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  28%|██▊       | 139/500 [00:45<01:59,  3.02it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  28%|██▊       | 140/500 [00:46<01:59,  3.02it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  28%|██▊       | 141/500 [00:46<01:58,  3.03it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  28%|██▊       | 142/500 [00:46<01:58,  3.03it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  29%|██▊       | 143/500 [00:47<01:57,  3.03it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  29%|██▉       | 144/500 [00:47<01:57,  3.03it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  29%|██▉       | 145/500 [00:47<01:57,  3.03it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  29%|██▉       | 146/500 [00:48<01:56,  3.04it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  29%|██▉       | 147/500 [00:48<01:56,  3.04it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  30%|██▉       | 148/500 [00:48<01:55,  3.04it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  30%|██▉       | 149/500 [00:48<01:55,  3.04it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  30%|███       | 150/500 [00:49<01:55,  3.04it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  30%|███       | 151/500 [00:49<01:54,  3.04it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  30%|███       | 152/500 [00:49<01:54,  3.05it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  31%|███       | 153/500 [00:50<01:53,  3.05it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  31%|███       | 154/500 [00:50<01:53,  3.05it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  31%|███       | 155/500 [00:50<01:53,  3.05it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  31%|███       | 156/500 [00:51<01:52,  3.05it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  31%|███▏      | 157/500 [00:51<01:52,  3.05it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  32%|███▏      | 158/500 [00:51<01:51,  3.06it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  32%|███▏      | 159/500 [00:51<01:51,  3.06it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  32%|███▏      | 160/500 [00:52<01:51,  3.06it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  32%|███▏      | 161/500 [00:52<01:50,  3.06it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  32%|███▏      | 162/500 [00:52<01:50,  3.06it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  33%|███▎      | 163/500 [00:53<01:49,  3.06it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  33%|███▎      | 164/500 [00:53<01:49,  3.07it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  33%|███▎      | 165/500 [00:53<01:48,  3.07it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  33%|███▎      | 166/500 [00:53<01:48,  3.08it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  33%|███▎      | 167/500 [00:54<01:48,  3.08it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  34%|███▎      | 168/500 [00:54<01:47,  3.08it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  34%|███▍      | 169/500 [00:54<01:47,  3.08it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  34%|███▍      | 170/500 [00:55<01:46,  3.08it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  34%|███▍      | 171/500 [00:55<01:46,  3.09it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  34%|███▍      | 172/500 [00:55<01:46,  3.09it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  35%|███▍      | 173/500 [00:56<01:45,  3.09it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  35%|███▍      | 174/500 [00:56<01:45,  3.09it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  35%|███▌      | 175/500 [00:56<01:45,  3.09it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  35%|███▌      | 176/500 [00:56<01:44,  3.09it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  35%|███▌      | 177/500 [00:57<01:44,  3.09it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  36%|███▌      | 178/500 [00:57<01:44,  3.09it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  36%|███▌      | 179/500 [00:57<01:43,  3.10it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  36%|███▌      | 180/500 [00:58<01:43,  3.10it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  36%|███▌      | 181/500 [00:58<01:42,  3.10it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  36%|███▋      | 182/500 [00:58<01:42,  3.10it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  37%|███▋      | 183/500 [00:59<01:42,  3.10it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  37%|███▋      | 184/500 [00:59<01:41,  3.10it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  37%|███▋      | 185/500 [00:59<01:41,  3.10it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  37%|███▋      | 186/500 [00:59<01:41,  3.10it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  37%|███▋      | 187/500 [01:00<01:40,  3.10it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  38%|███▊      | 188/500 [01:00<01:40,  3.10it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  38%|███▊      | 189/500 [01:00<01:40,  3.10it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  38%|███▊      | 190/500 [01:01<01:39,  3.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  38%|███▊      | 191/500 [01:01<01:39,  3.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  38%|███▊      | 192/500 [01:01<01:39,  3.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  39%|███▊      | 193/500 [01:02<01:38,  3.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  39%|███▉      | 194/500 [01:02<01:38,  3.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  39%|███▉      | 195/500 [01:02<01:38,  3.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  39%|███▉      | 196/500 [01:03<01:37,  3.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  39%|███▉      | 197/500 [01:03<01:37,  3.10it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  40%|███▉      | 198/500 [01:03<01:37,  3.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  40%|███▉      | 199/500 [01:04<01:36,  3.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  40%|████      | 200/500 [01:04<01:36,  3.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  40%|████      | 201/500 [01:04<01:36,  3.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  40%|████      | 202/500 [01:04<01:35,  3.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  41%|████      | 203/500 [01:05<01:35,  3.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  41%|████      | 204/500 [01:05<01:35,  3.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  41%|████      | 205/500 [01:05<01:34,  3.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  41%|████      | 206/500 [01:06<01:34,  3.12it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  41%|████▏     | 207/500 [01:06<01:33,  3.12it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  42%|████▏     | 208/500 [01:06<01:33,  3.12it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  42%|████▏     | 209/500 [01:06<01:33,  3.13it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  42%|████▏     | 210/500 [01:07<01:32,  3.13it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  42%|████▏     | 211/500 [01:07<01:32,  3.13it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  42%|████▏     | 212/500 [01:07<01:31,  3.13it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  43%|████▎     | 213/500 [01:07<01:31,  3.14it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  43%|████▎     | 214/500 [01:08<01:31,  3.14it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  43%|████▎     | 215/500 [01:08<01:30,  3.14it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  43%|████▎     | 216/500 [01:08<01:30,  3.14it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  43%|████▎     | 217/500 [01:09<01:30,  3.14it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  44%|████▎     | 218/500 [01:09<01:29,  3.14it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  44%|████▍     | 219/500 [01:09<01:29,  3.14it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  44%|████▍     | 220/500 [01:09<01:28,  3.15it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  44%|████▍     | 221/500 [01:10<01:28,  3.15it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  44%|████▍     | 222/500 [01:10<01:28,  3.15it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  45%|████▍     | 223/500 [01:10<01:27,  3.15it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  45%|████▍     | 224/500 [01:11<01:27,  3.15it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  45%|████▌     | 225/500 [01:11<01:27,  3.15it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  45%|████▌     | 226/500 [01:11<01:26,  3.15it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  45%|████▌     | 227/500 [01:11<01:26,  3.16it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  46%|████▌     | 228/500 [01:12<01:26,  3.16it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  46%|████▌     | 229/500 [01:12<01:25,  3.16it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  46%|████▌     | 230/500 [01:12<01:25,  3.16it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  46%|████▌     | 231/500 [01:13<01:25,  3.16it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  46%|████▋     | 232/500 [01:13<01:24,  3.16it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  47%|████▋     | 233/500 [01:13<01:24,  3.16it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  47%|████▋     | 234/500 [01:13<01:24,  3.16it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  47%|████▋     | 235/500 [01:14<01:23,  3.17it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  47%|████▋     | 236/500 [01:14<01:23,  3.17it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  47%|████▋     | 237/500 [01:14<01:22,  3.17it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  48%|████▊     | 238/500 [01:15<01:22,  3.17it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  48%|████▊     | 239/500 [01:15<01:22,  3.17it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  48%|████▊     | 240/500 [01:15<01:21,  3.17it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  48%|████▊     | 241/500 [01:15<01:21,  3.18it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  48%|████▊     | 242/500 [01:16<01:21,  3.18it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  49%|████▊     | 243/500 [01:16<01:20,  3.18it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  49%|████▉     | 244/500 [01:16<01:20,  3.18it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  49%|████▉     | 245/500 [01:17<01:20,  3.18it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  49%|████▉     | 246/500 [01:17<01:19,  3.18it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  49%|████▉     | 247/500 [01:17<01:19,  3.18it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  50%|████▉     | 248/500 [01:17<01:19,  3.18it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  50%|████▉     | 249/500 [01:18<01:18,  3.18it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  50%|█████     | 250/500 [01:18<01:18,  3.19it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  50%|█████     | 251/500 [01:18<01:18,  3.19it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  50%|█████     | 252/500 [01:19<01:17,  3.19it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  51%|█████     | 253/500 [01:19<01:17,  3.19it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  51%|█████     | 254/500 [01:19<01:17,  3.19it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  51%|█████     | 255/500 [01:19<01:16,  3.19it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  51%|█████     | 256/500 [01:20<01:16,  3.20it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  51%|█████▏    | 257/500 [01:20<01:15,  3.20it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  52%|█████▏    | 258/500 [01:20<01:15,  3.20it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  52%|█████▏    | 259/500 [01:20<01:15,  3.21it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  52%|█████▏    | 260/500 [01:21<01:14,  3.21it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  52%|█████▏    | 261/500 [01:21<01:14,  3.21it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  52%|█████▏    | 262/500 [01:21<01:14,  3.21it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  53%|█████▎    | 263/500 [01:21<01:13,  3.21it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  53%|█████▎    | 264/500 [01:22<01:13,  3.21it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  53%|█████▎    | 265/500 [01:22<01:13,  3.22it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  53%|█████▎    | 266/500 [01:22<01:12,  3.22it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  53%|█████▎    | 267/500 [01:22<01:12,  3.22it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  54%|█████▎    | 268/500 [01:23<01:11,  3.22it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  54%|█████▍    | 269/500 [01:23<01:11,  3.22it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  54%|█████▍    | 270/500 [01:23<01:11,  3.23it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  54%|█████▍    | 271/500 [01:23<01:10,  3.23it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  54%|█████▍    | 272/500 [01:24<01:10,  3.23it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  55%|█████▍    | 273/500 [01:24<01:10,  3.23it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  55%|█████▍    | 274/500 [01:24<01:09,  3.23it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  55%|█████▌    | 275/500 [01:25<01:09,  3.24it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  55%|█████▌    | 276/500 [01:25<01:09,  3.24it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  55%|█████▌    | 277/500 [01:25<01:08,  3.24it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  56%|█████▌    | 278/500 [01:25<01:08,  3.24it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  56%|█████▌    | 279/500 [01:25<01:08,  3.24it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  56%|█████▌    | 280/500 [01:26<01:07,  3.25it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  56%|█████▌    | 281/500 [01:26<01:07,  3.25it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  56%|█████▋    | 282/500 [01:26<01:07,  3.25it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  57%|█████▋    | 283/500 [01:27<01:06,  3.25it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  57%|█████▋    | 284/500 [01:27<01:06,  3.25it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  57%|█████▋    | 285/500 [01:27<01:06,  3.26it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  57%|█████▋    | 286/500 [01:27<01:05,  3.26it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  57%|█████▋    | 287/500 [01:28<01:05,  3.26it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  58%|█████▊    | 288/500 [01:28<01:04,  3.26it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  58%|█████▊    | 289/500 [01:28<01:04,  3.26it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  58%|█████▊    | 290/500 [01:28<01:04,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  58%|█████▊    | 291/500 [01:29<01:03,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  58%|█████▊    | 292/500 [01:29<01:03,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  59%|█████▊    | 293/500 [01:29<01:03,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  59%|█████▉    | 294/500 [01:29<01:02,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  59%|█████▉    | 295/500 [01:30<01:02,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  59%|█████▉    | 296/500 [01:30<01:02,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  59%|█████▉    | 297/500 [01:30<01:02,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  60%|█████▉    | 298/500 [01:31<01:01,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  60%|█████▉    | 299/500 [01:31<01:01,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  60%|██████    | 300/500 [01:31<01:01,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  60%|██████    | 301/500 [01:32<01:00,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  60%|██████    | 302/500 [01:32<01:00,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  61%|██████    | 303/500 [01:32<01:00,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  61%|██████    | 304/500 [01:32<00:59,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  61%|██████    | 305/500 [01:33<00:59,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  61%|██████    | 306/500 [01:33<00:59,  3.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  61%|██████▏   | 307/500 [01:33<00:58,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  62%|██████▏   | 308/500 [01:33<00:58,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  62%|██████▏   | 309/500 [01:34<00:58,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  62%|██████▏   | 310/500 [01:34<00:57,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  62%|██████▏   | 311/500 [01:34<00:57,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  62%|██████▏   | 312/500 [01:35<00:57,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  63%|██████▎   | 313/500 [01:35<00:57,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  63%|██████▎   | 314/500 [01:35<00:56,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  63%|██████▎   | 315/500 [01:35<00:56,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  63%|██████▎   | 316/500 [01:36<00:56,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  63%|██████▎   | 317/500 [01:36<00:55,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  64%|██████▎   | 318/500 [01:36<00:55,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  64%|██████▍   | 319/500 [01:37<00:55,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  64%|██████▍   | 320/500 [01:37<00:54,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  64%|██████▍   | 321/500 [01:37<00:54,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  64%|██████▍   | 322/500 [01:38<00:54,  3.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  65%|██████▍   | 323/500 [01:38<00:53,  3.29it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  65%|██████▍   | 324/500 [01:38<00:53,  3.29it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  65%|██████▌   | 325/500 [01:38<00:53,  3.29it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  65%|██████▌   | 326/500 [01:39<00:52,  3.29it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  65%|██████▌   | 327/500 [01:39<00:52,  3.29it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  66%|██████▌   | 328/500 [01:39<00:52,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  66%|██████▌   | 329/500 [01:39<00:51,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  66%|██████▌   | 330/500 [01:40<00:51,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  66%|██████▌   | 331/500 [01:40<00:51,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  66%|██████▋   | 332/500 [01:40<00:50,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  67%|██████▋   | 333/500 [01:40<00:50,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  67%|██████▋   | 334/500 [01:41<00:50,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  67%|██████▋   | 335/500 [01:41<00:49,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  67%|██████▋   | 336/500 [01:41<00:49,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  67%|██████▋   | 337/500 [01:42<00:49,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  68%|██████▊   | 338/500 [01:42<00:49,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  68%|██████▊   | 339/500 [01:42<00:48,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  68%|██████▊   | 340/500 [01:42<00:48,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  68%|██████▊   | 341/500 [01:43<00:48,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  68%|██████▊   | 342/500 [01:43<00:47,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  69%|██████▊   | 343/500 [01:43<00:47,  3.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  69%|██████▉   | 344/500 [01:44<00:47,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  69%|██████▉   | 345/500 [01:44<00:46,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  69%|██████▉   | 346/500 [01:44<00:46,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  69%|██████▉   | 347/500 [01:44<00:46,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  70%|██████▉   | 348/500 [01:45<00:45,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  70%|██████▉   | 349/500 [01:45<00:45,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  70%|███████   | 350/500 [01:45<00:45,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  70%|███████   | 351/500 [01:46<00:45,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  70%|███████   | 352/500 [01:46<00:44,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  71%|███████   | 353/500 [01:46<00:44,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  71%|███████   | 354/500 [01:46<00:44,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  71%|███████   | 355/500 [01:47<00:43,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  71%|███████   | 356/500 [01:47<00:43,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  71%|███████▏  | 357/500 [01:47<00:43,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  72%|███████▏  | 358/500 [01:48<00:42,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  72%|███████▏  | 359/500 [01:48<00:42,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  72%|███████▏  | 360/500 [01:48<00:42,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  72%|███████▏  | 361/500 [01:48<00:41,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  72%|███████▏  | 362/500 [01:49<00:41,  3.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  73%|███████▎  | 363/500 [01:49<00:41,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  73%|███████▎  | 364/500 [01:49<00:41,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  73%|███████▎  | 365/500 [01:50<00:40,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  73%|███████▎  | 366/500 [01:50<00:40,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  73%|███████▎  | 367/500 [01:50<00:40,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  74%|███████▎  | 368/500 [01:50<00:39,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  74%|███████▍  | 369/500 [01:51<00:39,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  74%|███████▍  | 370/500 [01:51<00:39,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  74%|███████▍  | 371/500 [01:51<00:38,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  74%|███████▍  | 372/500 [01:52<00:38,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  75%|███████▍  | 373/500 [01:52<00:38,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  75%|███████▍  | 374/500 [01:52<00:37,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  75%|███████▌  | 375/500 [01:52<00:37,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  75%|███████▌  | 376/500 [01:53<00:37,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  75%|███████▌  | 377/500 [01:53<00:37,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  76%|███████▌  | 378/500 [01:53<00:36,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  76%|███████▌  | 379/500 [01:54<00:36,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  76%|███████▌  | 380/500 [01:54<00:36,  3.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  76%|███████▌  | 381/500 [01:54<00:35,  3.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  76%|███████▋  | 382/500 [01:54<00:35,  3.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  77%|███████▋  | 383/500 [01:55<00:35,  3.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  77%|███████▋  | 384/500 [01:55<00:34,  3.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  77%|███████▋  | 385/500 [01:55<00:34,  3.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  77%|███████▋  | 386/500 [01:56<00:34,  3.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  77%|███████▋  | 387/500 [01:56<00:33,  3.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  78%|███████▊  | 388/500 [01:56<00:33,  3.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  78%|███████▊  | 389/500 [01:56<00:33,  3.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  78%|███████▊  | 390/500 [01:57<00:33,  3.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  78%|███████▊  | 391/500 [01:57<00:32,  3.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  78%|███████▊  | 392/500 [01:57<00:32,  3.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  79%|███████▊  | 393/500 [01:57<00:32,  3.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  79%|███████▉  | 394/500 [01:58<00:31,  3.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  79%|███████▉  | 395/500 [01:58<00:31,  3.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  79%|███████▉  | 396/500 [01:58<00:31,  3.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  79%|███████▉  | 397/500 [01:58<00:30,  3.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  80%|███████▉  | 398/500 [01:59<00:30,  3.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  80%|███████▉  | 399/500 [01:59<00:30,  3.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  80%|████████  | 400/500 [01:59<00:29,  3.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  80%|████████  | 401/500 [01:59<00:29,  3.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  80%|████████  | 402/500 [02:00<00:29,  3.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  81%|████████  | 403/500 [02:00<00:28,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  81%|████████  | 404/500 [02:00<00:28,  3.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  81%|████████  | 405/500 [02:01<00:28,  3.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  81%|████████  | 406/500 [02:01<00:28,  3.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  81%|████████▏ | 407/500 [02:01<00:27,  3.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  82%|████████▏ | 408/500 [02:02<00:27,  3.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  82%|████████▏ | 409/500 [02:02<00:27,  3.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  82%|████████▏ | 410/500 [02:02<00:26,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  82%|████████▏ | 411/500 [02:02<00:26,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  82%|████████▏ | 412/500 [02:03<00:26,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  83%|████████▎ | 413/500 [02:03<00:25,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  83%|████████▎ | 414/500 [02:03<00:25,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  83%|████████▎ | 415/500 [02:03<00:25,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  83%|████████▎ | 416/500 [02:04<00:25,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  83%|████████▎ | 417/500 [02:04<00:24,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  84%|████████▎ | 418/500 [02:04<00:24,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  84%|████████▍ | 419/500 [02:05<00:24,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  84%|████████▍ | 420/500 [02:05<00:23,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  84%|████████▍ | 421/500 [02:05<00:23,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  84%|████████▍ | 422/500 [02:05<00:23,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  85%|████████▍ | 423/500 [02:06<00:22,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  85%|████████▍ | 424/500 [02:06<00:22,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  85%|████████▌ | 425/500 [02:06<00:22,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  85%|████████▌ | 426/500 [02:07<00:22,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  85%|████████▌ | 427/500 [02:07<00:21,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  86%|████████▌ | 428/500 [02:07<00:21,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  86%|████████▌ | 429/500 [02:07<00:21,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  86%|████████▌ | 430/500 [02:08<00:20,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  86%|████████▌ | 431/500 [02:08<00:20,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  86%|████████▋ | 432/500 [02:08<00:20,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  87%|████████▋ | 433/500 [02:09<00:19,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  87%|████████▋ | 434/500 [02:09<00:19,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  87%|████████▋ | 435/500 [02:09<00:19,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  87%|████████▋ | 436/500 [02:09<00:19,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  87%|████████▋ | 437/500 [02:10<00:18,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  88%|████████▊ | 438/500 [02:10<00:18,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  88%|████████▊ | 439/500 [02:10<00:18,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  88%|████████▊ | 440/500 [02:11<00:17,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  88%|████████▊ | 441/500 [02:11<00:17,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  88%|████████▊ | 442/500 [02:11<00:17,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  89%|████████▊ | 443/500 [02:12<00:16,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  89%|████████▉ | 444/500 [02:12<00:16,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  89%|████████▉ | 445/500 [02:12<00:16,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  89%|████████▉ | 446/500 [02:12<00:16,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  89%|████████▉ | 447/500 [02:13<00:15,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  90%|████████▉ | 448/500 [02:13<00:15,  3.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  90%|████████▉ | 449/500 [02:13<00:15,  3.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  90%|█████████ | 450/500 [02:14<00:14,  3.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  90%|█████████ | 451/500 [02:14<00:14,  3.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  90%|█████████ | 452/500 [02:14<00:14,  3.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  91%|█████████ | 453/500 [02:14<00:13,  3.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  91%|█████████ | 454/500 [02:15<00:13,  3.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  91%|█████████ | 455/500 [02:15<00:13,  3.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  91%|█████████ | 456/500 [02:15<00:13,  3.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  91%|█████████▏| 457/500 [02:15<00:12,  3.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  92%|█████████▏| 458/500 [02:16<00:12,  3.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  92%|█████████▏| 459/500 [02:16<00:12,  3.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  92%|█████████▏| 460/500 [02:16<00:11,  3.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  92%|█████████▏| 461/500 [02:16<00:11,  3.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  92%|█████████▏| 462/500 [02:17<00:11,  3.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  93%|█████████▎| 463/500 [02:17<00:10,  3.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  93%|█████████▎| 464/500 [02:17<00:10,  3.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  93%|█████████▎| 465/500 [02:17<00:10,  3.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  93%|█████████▎| 466/500 [02:18<00:10,  3.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  93%|█████████▎| 467/500 [02:18<00:09,  3.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  94%|█████████▎| 468/500 [02:18<00:09,  3.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  94%|█████████▍| 469/500 [02:18<00:09,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  94%|█████████▍| 470/500 [02:19<00:08,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  94%|█████████▍| 471/500 [02:19<00:08,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  94%|█████████▍| 472/500 [02:19<00:08,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  95%|█████████▍| 473/500 [02:19<00:07,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  95%|█████████▍| 474/500 [02:20<00:07,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  95%|█████████▌| 475/500 [02:20<00:07,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  95%|█████████▌| 476/500 [02:20<00:07,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  95%|█████████▌| 477/500 [02:21<00:06,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  96%|█████████▌| 478/500 [02:21<00:06,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  96%|█████████▌| 479/500 [02:21<00:06,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  96%|█████████▌| 480/500 [02:22<00:05,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  96%|█████████▌| 481/500 [02:22<00:05,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  96%|█████████▋| 482/500 [02:22<00:05,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  97%|█████████▋| 483/500 [02:22<00:05,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  97%|█████████▋| 484/500 [02:23<00:04,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  97%|█████████▋| 485/500 [02:23<00:04,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  97%|█████████▋| 486/500 [02:23<00:04,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  97%|█████████▋| 487/500 [02:24<00:03,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  98%|█████████▊| 488/500 [02:24<00:03,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  98%|█████████▊| 489/500 [02:24<00:03,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  98%|█████████▊| 490/500 [02:24<00:02,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  98%|█████████▊| 491/500 [02:25<00:02,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  98%|█████████▊| 492/500 [02:25<00:02,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  99%|█████████▊| 493/500 [02:25<00:02,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  99%|█████████▉| 494/500 [02:26<00:01,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  99%|█████████▉| 495/500 [02:26<00:01,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  99%|█████████▉| 496/500 [02:26<00:01,  3.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  99%|█████████▉| 497/500 [02:26<00:00,  3.39it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0: 100%|█████████▉| 498/500 [02:27<00:00,  3.39it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0: 100%|█████████▉| 499/500 [02:27<00:00,  3.39it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0: 100%|██████████| 500/500 [02:27<00:00,  3.39it/s][NeMo I 2024-12-10 16:13:30 megatron_gpt_sft_model:551] Total deduplicated inference data size: 500 to 500\n",
      "[NeMo I 2024-12-10 16:13:30 megatron_gpt_sft_model:702] Predictions saved to pubmedQA_result__test_pubmedqa_inputs_preds_labels.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 16:13:30 megatron_gpt_sft_model:642] No training data found, reconfiguring microbatches based on validation batch sizes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of micro-batches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 16:13:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-10 16:13:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('test_loss_pubmedqa', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-10 16:13:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 500/500 [02:27<00:00,  3.39it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2418619990348816    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m   test_loss_pubmedqa    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2418619990348816    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2418619990348816    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "PRECISION=bf16\n",
    "MODEL=\"models/llama-3.1/8B/nemo/$PRECISION/Llama-3_1-8B-Instruct.nemo\"\n",
    "OUTPUT_DIR=\"results/llama-3.1/8B/$PRECISION\"\n",
    "TEST_DS=\"[pubmedqa/data/pubmedqa_test.jsonl]\"\n",
    "TEST_NAMES=\"[pubmedqa]\"\n",
    "SCHEME=\"lora\"\n",
    "GPUS=1\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# This is where your LoRA checkpoint was saved\n",
    "PATH_TO_TRAINED_MODEL=\"$OUTPUT_DIR/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "\n",
    "# The generation run will save the generated outputs over the test dataset in a file prefixed like so\n",
    "OUTPUT_PREFIX=\"pubmedQA_result_\"\n",
    "\n",
    "export TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "torchrun --nproc_per_node=${GPUS} \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.peft.restore_from_path=${PATH_TO_TRAINED_MODEL} \\\n",
    "    trainer.devices=${GPUS} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    model.data.test_ds.file_names=${TEST_DS} \\\n",
    "    model.data.test_ds.names=${TEST_NAMES} \\\n",
    "    model.data.test_ds.global_batch_size=1 \\\n",
    "    model.data.test_ds.micro_batch_size=1 \\\n",
    "    model.data.test_ds.tokens_to_generate=3 \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.output_file_path_prefix=${OUTPUT_PREFIX} \\\n",
    "    model.data.test_ds.write_predictions_to_file=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe048f9",
   "metadata": {},
   "source": [
    "### Step 4: Check the model accuracy\n",
    "\n",
    "Now that the results are in, let's read the results and calculate the accuracy on the pubmedQA task. You can compare your accuracy results with the public leaderboard at https://pubmedqa.github.io/.\n",
    "\n",
    "Let's take a look at one of the predictions in the generated output file. The `pred` key indicates what was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa5c0fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"OBJECTIVES: Outcome feedback is the process of learning patient outcomes after their care within the emergency department. We conducted a national survey of Canadian Royal College emergency medicine (EM) residents and program directors to determine the extent to which active outcome feedback and follow-up occurred. We also compared the perceived educational value of outcome feedback between residents and program directors.\\nMETHODS: We distributed surveys to all Royal College-accredited adult and pediatric EM training programs using a modified Dillman method. We analyzed the data using student's t-test for continuous variables and Fisher's exact test for categorical variables.\\nRESULTS: We received 210 completed surveys from 260 eligible residents (80.8%) and 21 of 24 program directors (87.5%) (overall 81.3%). Mandatory active outcome feedback was not present in any EM training program for admitted or discharged patients (0/21). Follow-up was performed electively by 89.4% of residents for patients admitted to the hospital, and by 44.2% of residents for patients discharged home. A majority of residents (76.9%) believed that patient follow-up should be mandatory compared to 42.9% of program directors (p=0.002). The perceived educational value of outcome feedback was 5.8/7 for residents and 5.1/7 for program directors (difference 0.7; p=0.002) based on a seven-point Likert scale (1=not important; 7=very important).\\nQUESTION: Outcome Feedback within Emergency Medicine Training Programs: An Opportunity to Apply the Theory of Deliberate Practice?\\n ### ANSWER (yes|no|maybe):\", \"pred\": \" <<< yes >>>\", \"label\": \" <<< maybe >>>\"}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "tail -n 1 pubmedQA_result__test_pubmedqa_inputs_preds_labels.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c91df7",
   "metadata": {},
   "source": [
    "Note that the model produces output in the specified format, such as `<<< no >>>`.\n",
    "\n",
    "The following snippet loads the generated output and calculates accuracy in comparison to the test set using the `evaluation.py` script included in the PubMedQA repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "900f81c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "answers = []\n",
    "with open(\"pubmedQA_result__test_pubmedqa_inputs_preds_labels.jsonl\",'rt') as f:\n",
    "    st = f.readline()\n",
    "    while st:\n",
    "        answers.append(json.loads(st))\n",
    "        st = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74e1bbce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_test = json.load(open(\"./pubmedqa/data/test_set.json\",'rt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a85926e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "sample_id = list(data_test.keys())\n",
    "\n",
    "for i, key in enumerate(sample_id):\n",
    "    answer = answers[i]['pred']\n",
    "    if 'yes' in answer:\n",
    "        results[key] = 'yes'\n",
    "    elif 'no' in answer:\n",
    "        results[key] = 'no'\n",
    "    elif 'maybe' in answer:\n",
    "        results[key] = 'maybe'\n",
    "    else:\n",
    "        print(\"Malformed answer: \", answer)\n",
    "        results[key] = 'maybe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "768074cf-d189-4b19-bf28-dc7149029ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.552000\n",
      "Macro-F1 0.237113\n"
     ]
    }
   ],
   "source": [
    "# Dump results in a format that can be ingested by PubMedQA evaluation file\n",
    "FILENAME=\"pubmedqa-llama-3-8b-lora.json\"\n",
    "with(open(FILENAME, \"w\")) as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Evaluation\n",
    "!cp $FILENAME ./pubmedqa/\n",
    "!cd ./pubmedqa/ && python evaluation.py $FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9909283e-e1f8-450e-a730-403e22f621ad",
   "metadata": {},
   "source": [
    "For the Llama-3-8B-Instruct model, you should see accuracy comparable to the below:\n",
    "```\n",
    "Accuracy 0.792000\n",
    "Macro-F1 0.594778\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713898c-8194-41f4-86ef-81b55c518035",
   "metadata": {},
   "source": [
    "## Export Model to TensorRT-LLM Format for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e908dc27-0588-464c-8108-6bfef53666d2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 19:32:50 nemo_logging:349] /tmp/ipykernel_3763/964923014.py:12: DeprecationWarning: Parameter n_gpus is deprecated and will be removed in the next release. Please use tensor_parallelism_size and pipeline_parallelism_size parameters instead.\n",
      "      trt_llm_exporter.export(\n",
      "    \n",
      "[NeMo W 2024-12-10 19:32:50 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/state_dict_loader.py:27: UserWarning: 'load_state_dict' is deprecated and will be removed in future versions. Please use 'load' instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-10 19:32:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "saving weights: 100%|██████████| 193/193 [00:22<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:25] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:25.021300 140016166184064 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:25] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:25.022238 140016166184064 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:25] [TRT-LLM] [I] Set multi_block_mode to False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:25.022658 140016166184064 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:25] [TRT-LLM] [I] Set paged_kv_cache to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:25.023044 140016166184064 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:25] [TRT-LLM] [I] Set tokens_per_block to 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:25.023427 140016166184064 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:25] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
      "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 19:33:25.023816 140016166184064 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
      "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:25] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 19:33:25.024170 140016166184064 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:53] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 35139, GPU 16556 (MiB)\n",
      "[12/10/2024-19:33:57] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4312, GPU +1148, now: CPU 39584, GPU 17706 (MiB)\n",
      "[12/10/2024-19:33:57] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n",
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Set nccl_plugin to None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.562298 140016166184064 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.563257 140016166184064 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with i[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "nputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:33:57] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.751935 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.753167 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.753668 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.754108 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.754545 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.754979 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.755409 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.755842 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.756255 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.757529 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.757969 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.758401 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.758834 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.759242 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.759636 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.760049 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.16.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.760453 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.16.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.17.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.760867 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.17.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.18.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.761236 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.18.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.19.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.761660 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.19.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.20.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.763089 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.20.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.21.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.763520 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.21.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.22.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.763967 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.22.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.23.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.764358 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.23.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.24.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.764776 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.24.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.25.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.765185 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.25.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.26.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.765588 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.26.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.27.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.765981 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.27.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.28.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.766387 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.28.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.29.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.766775 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.29.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.30.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.767216 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.30.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Parameter transformer.layers.31.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.767641 140016166184064 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.31.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:33:57.768037 140016166184064 logger.py:92] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:33:57] [TRT] [W] Unused Input: position_ids\n",
      "[12/10/2024-19:33:57] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[12/10/2024-19:33:57] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[12/10/2024-19:34:03] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[12/10/2024-19:34:03] [TRT] [I] Detected 14 inputs and 1 output network tensors.\n",
      "[12/10/2024-19:34:09] [TRT] [I] Total Host Persistent Memory: 103744\n",
      "[12/10/2024-19:34:09] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[12/10/2024-19:34:09] [TRT] [I] Total Scratch Memory: 33565056\n",
      "[12/10/2024-19:34:09] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 590 steps to complete.\n",
      "[12/10/2024-19:34:09] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 20.819ms to assign 17 blocks to 590 nodes requiring 402658816 bytes.\n",
      "[12/10/2024-19:34:09] [TRT] [I] Total Activation Memory: 402657280\n",
      "[12/10/2024-19:34:09] [TRT] [I] Total Weights Memory: 16127631360\n",
      "[12/10/2024-19:34:09] [TRT] [I] Engine generation completed in 11.7935 seconds.\n",
      "[12/10/2024-19:34:09] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 2004 MiB, GPU 30761 MiB\n",
      "[12/10/2024-19:34:16] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 56421 MiB\n",
      "[12/10/2024-19:34:16] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:16.961966 140016166184064 logger.py:92] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:16] [TRT-LLM] [I] Serializing engine to models/llama-3.1/8B/trt_llm/bf16/tp_1/rank0.engine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:16.967163 140016166184064 logger.py:92] [TRT-LLM] [I] Serializing engine to models/llama-3.1/8B/trt_llm/bf16/tp_1/rank0.engine...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:26] [TRT-LLM] [I] Engine serialized. Total time: 00:00:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:26.120570 140016166184064 logger.py:92] [TRT-LLM] [I] Engine serialized. Total time: 00:00:09\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.050756 140016166184064 logger.py:92] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.051626 140016166184064 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.052032 140016166184064 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set smooth_quant_gemm_plugin to None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.052399 140016166184064 logger.py:92] [TRT-LLM] [I] Set smooth_quant_gemm_plugin to None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set identity_plugin to None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.052774 140016166184064 logger.py:92] [TRT-LLM] [I] Set identity_plugin to None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set layernorm_quantization_plugin to None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.053117 140016166184064 logger.py:92] [TRT-LLM] [I] Set layernorm_quantization_plugin to None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set rmsnorm_quantization_plugin to None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.053474 140016166184064 logger.py:92] [TRT-LLM] [I] Set rmsnorm_quantization_plugin to None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set nccl_plugin to None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.053820 140016166184064 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set lookup_plugin to None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.054188 140016166184064 logger.py:92] [TRT-LLM] [I] Set lookup_plugin to None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set lora_plugin to None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.055349 140016166184064 logger.py:92] [TRT-LLM] [I] Set lora_plugin to None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set weight_only_groupwise_quant_matmul_plugin to None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.055704 140016166184064 logger.py:92] [TRT-LLM] [I] Set weight_only_groupwise_quant_matmul_plugin to None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.056076 140016166184064 logger.py:92] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set quantize_per_token_plugin to False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.056402 140016166184064 logger.py:92] [TRT-LLM] [I] Set quantize_per_token_plugin to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set quantize_tensor_plugin to False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.056735 140016166184064 logger.py:92] [TRT-LLM] [I] Set quantize_tensor_plugin to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set moe_plugin to float16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.057059 140016166184064 logger.py:92] [TRT-LLM] [I] Set moe_plugin to float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.057392 140016166184064 logger.py:92] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set context_fmha to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.057753 140016166184064 logger.py:92] [TRT-LLM] [I] Set context_fmha to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.058078 140016166184064 logger.py:92] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set paged_kv_cache to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.058415 140016166184064 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set remove_input_padding to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.058729 140016166184064 logger.py:92] [TRT-LLM] [I] Set remove_input_padding to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.059078 140016166184064 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set multi_block_mode to False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.059432 140016166184064 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set enable_xqa to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.059765 140016166184064 logger.py:92] [TRT-LLM] [I] Set enable_xqa to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.060095 140016166184064 logger.py:92] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set tokens_per_block to 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.060425 140016166184064 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.060770 140016166184064 logger.py:92] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.061083 140016166184064 logger.py:92] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.061447 140016166184064 logger.py:92] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set multiple_profiles to False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.061794 140016166184064 logger.py:92] [TRT-LLM] [I] Set multiple_profiles to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set paged_state to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.062124 140016166184064 logger.py:92] [TRT-LLM] [I] Set paged_state to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Set streamingllm to False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.062477 140016166184064 logger.py:92] [TRT-LLM] [I] Set streamingllm to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT] [I] Loaded engine size: 15383 MiB\n",
      "[12/10/2024-19:34:35] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 30761 (MiB)\n",
      "[12/10/2024-19:34:35] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 30761 (MiB)\n",
      "[12/10/2024-19:34:35] [TRT-LLM] [W] The paged KV cache in Python runtime is experimental. For performance and correctness, please, use C++ runtime.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 19:34:35.973186 140016166184064 logger.py:92] [TRT-LLM] [W] The paged KV cache in Python runtime is experimental. For performance and correctness, please, use C++ runtime.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:34:35] [TRT-LLM] [I] Load engine takes: 13.958819389343262 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:34:35.974622 140016166184064 logger.py:92] [TRT-LLM] [I] Load engine takes: 13.958819389343262 sec\n"
     ]
    }
   ],
   "source": [
    "from nemo.export.tensorrt_llm import TensorRTLLM\n",
    "\n",
    "MODEL_DIR=\"models/llama-3.1/8B/trt_llm/bf16/tp_1\"\n",
    "MODEL_CKPT=\"models/llama-3.1/8B/nemo/bf16/Llama-3_1-8B-Instruct.nemo\"\n",
    "LORA_CKPT=\"results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "\n",
    "trt_llm_exporter = TensorRTLLM(\n",
    "    model_dir=MODEL_DIR,\n",
    "    lora_ckpt_list=[LORA_CKPT],\n",
    ")\n",
    "\n",
    "trt_llm_exporter.export(\n",
    "    nemo_checkpoint_path=MODEL_CKPT,\n",
    "    model_type=\"llama\",\n",
    "    n_gpus=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c5d07-9b43-434f-bdb1-821ee465a3b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ucloud/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1210 19:55:35.402386 140579951768704 logger.py:92] [TRT-LLM] [W] Found pynvml==11.4.1. Please use pynvml>=11.5.0 to get accurate memory usage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:55:36.112548 140579951768704 deploy_triton.py:344] Logging level set to 20\n",
      "I1210 19:55:36.112711 140579951768704 deploy_triton.py:345] Namespace(nemo_checkpoint='models/llama-3.1/8B/nemo/bf16/Llama-3_1-8B-Instruct.nemo', ptuning_nemo_checkpoint=None, task_ids=None, model_type='llama', triton_model_name='llama3-pubmedqa', triton_model_version=1, triton_port=8000, triton_http_address='0.0.0.0', triton_request_timeout=60, triton_model_repository='models/llama-3.1/8B/trt_llm/bf16/tp_1', num_gpus=1, tensor_parallelism_size=1, pipeline_parallelism_size=1, dtype='bfloat16', max_input_len=256, max_output_len=256, max_batch_size=8, max_num_tokens=None, opt_num_tokens=None, max_prompt_embedding_table_size=None, no_paged_kv_cache=False, disable_remove_input_padding=False, use_parallel_embedding=False, multi_block_mode=False, enable_streaming=False, use_lora_plugin=None, lora_target_modules=None, max_lora_rank=64, lora_ckpt=['results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning.nemo'], use_cpp_runtime=False, backend='TensorRT-LLM', start_rest_service=False, service_http_address='0.0.0.0', service_port=8080, openai_format_response=False, debug_mode=False)\n",
      "I1210 19:55:36.112766 140579951768704 deploy_triton.py:280] Export operation will be started to export the nemo checkpoint to TensorRT-LLM.\n",
      "[NeMo W 2024-12-10 19:55:36 nemo_logging:349] /opt/NeMo/scripts/deploy/nlp/deploy_triton.py:281: DeprecationWarning: Parameter n_gpus is deprecated and will be removed in the next release. Please use tensor_parallelism_size and pipeline_parallelism_size parameters instead.\n",
      "      trt_llm_exporter.export(\n",
      "    \n",
      "[NeMo W 2024-12-10 19:55:36 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/state_dict_loader.py:27: UserWarning: 'load_state_dict' is deprecated and will be removed in future versions. Please use 'load' instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-10 19:55:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded mpi lib /usr/local/lib/python3.10/dist-packages/mpi4py/MPI.cpython-310-x86_64-linux-gnu.so successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "saving weights: 100%|██████████| 193/193 [00:28<00:00,  6.81it/s]\n",
      "I1210 19:56:16.408668 140579951768704 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
      "I1210 19:56:16.408972 140579951768704 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
      "I1210 19:56:16.409007 140579951768704 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n",
      "I1210 19:56:16.409029 140579951768704 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "I1210 19:56:16.409046 140579951768704 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
      "W1210 19:56:16.409075 140579951768704 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
      "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n",
      "W1210 19:56:16.409097 140579951768704 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:56:47] [TRT] [I] [MemUsageChange] Init CUDA: CPU +16, GPU +0, now: CPU 34991, GPU 18482 (MiB)\n",
      "[12/10/2024-19:56:51] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4312, GPU +1150, now: CPU 39439, GPU 19634 (MiB)\n",
      "[12/10/2024-19:56:51] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:56:51.692784 140579951768704 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to None.\n",
      "I1210 19:56:51.692900 140579951768704 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-19:56:51] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:56:51.933476 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.933691 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.933785 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.933868 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.933943 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.934018 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.934097 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.934184 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.934254 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.934329 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.934396 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.934463 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.934530 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.934602 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.934670 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.934734 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.934803 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.16.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.934870 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.17.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.934934 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.18.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.934998 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.19.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.935067 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.20.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.935132 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.21.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.935201 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.22.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.935261 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.23.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.935326 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.24.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.935386 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.25.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.935450 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.26.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.935510 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.27.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.935576 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.28.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.935645 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.29.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.935722 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.30.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.935783 140579951768704 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.31.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1210 19:56:51.935852 140579951768704 logger.py:92] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:56:51] [TRT] [W] Unused Input: position_ids\n",
      "[12/10/2024-19:56:51] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[12/10/2024-19:56:51] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[12/10/2024-19:56:57] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[12/10/2024-19:56:57] [TRT] [I] Detected 14 inputs and 1 output network tensors.\n",
      "[12/10/2024-19:57:05] [TRT] [I] Total Host Persistent Memory: 103744\n",
      "[12/10/2024-19:57:05] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[12/10/2024-19:57:05] [TRT] [I] Total Scratch Memory: 33565056\n",
      "[12/10/2024-19:57:05] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 590 steps to complete.\n",
      "[12/10/2024-19:57:05] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 26.6302ms to assign 17 blocks to 590 nodes requiring 402658816 bytes.\n",
      "[12/10/2024-19:57:05] [TRT] [I] Total Activation Memory: 402657280\n",
      "[12/10/2024-19:57:05] [TRT] [I] Total Weights Memory: 16127631360\n",
      "[12/10/2024-19:57:05] [TRT] [I] Engine generation completed in 13.1926 seconds.\n",
      "[12/10/2024-19:57:05] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 2004 MiB, GPU 15380 MiB\n",
      "[12/10/2024-19:57:13] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 56276 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 19:57:13.314188 140579951768704 logger.py:92] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:21\n",
      "I1210 19:57:13.319579 140579951768704 logger.py:92] [TRT-LLM] [I] Serializing engine to models/llama-3.1/8B/trt_llm/bf16/tp_1/rank0.engine...\n",
      "I1210 19:57:24.630646 140579951768704 logger.py:92] [TRT-LLM] [I] Engine serialized. Total time: 00:00:11\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "I1210 19:57:33.846337 140579951768704 logger.py:92] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n",
      "I1210 19:57:33.846453 140579951768704 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
      "I1210 19:57:33.846481 140579951768704 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
      "I1210 19:57:33.846505 140579951768704 logger.py:92] [TRT-LLM] [I] Set smooth_quant_gemm_plugin to None.\n",
      "I1210 19:57:33.846524 140579951768704 logger.py:92] [TRT-LLM] [I] Set identity_plugin to None.\n",
      "I1210 19:57:33.846545 140579951768704 logger.py:92] [TRT-LLM] [I] Set layernorm_quantization_plugin to None.\n",
      "I1210 19:57:33.846561 140579951768704 logger.py:92] [TRT-LLM] [I] Set rmsnorm_quantization_plugin to None.\n",
      "I1210 19:57:33.846581 140579951768704 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to None.\n",
      "I1210 19:57:33.846602 140579951768704 logger.py:92] [TRT-LLM] [I] Set lookup_plugin to None.\n",
      "I1210 19:57:33.846619 140579951768704 logger.py:92] [TRT-LLM] [I] Set lora_plugin to None.\n",
      "I1210 19:57:33.846635 140579951768704 logger.py:92] [TRT-LLM] [I] Set weight_only_groupwise_quant_matmul_plugin to None.\n",
      "I1210 19:57:33.846652 140579951768704 logger.py:92] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to None.\n",
      "I1210 19:57:33.846672 140579951768704 logger.py:92] [TRT-LLM] [I] Set quantize_per_token_plugin to False.\n",
      "I1210 19:57:33.846688 140579951768704 logger.py:92] [TRT-LLM] [I] Set quantize_tensor_plugin to False.\n",
      "I1210 19:57:33.846705 140579951768704 logger.py:92] [TRT-LLM] [I] Set moe_plugin to float16.\n",
      "I1210 19:57:33.846721 140579951768704 logger.py:92] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.\n",
      "I1210 19:57:33.846738 140579951768704 logger.py:92] [TRT-LLM] [I] Set context_fmha to True.\n",
      "I1210 19:57:33.846755 140579951768704 logger.py:92] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.\n",
      "I1210 19:57:33.846771 140579951768704 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "I1210 19:57:33.846786 140579951768704 logger.py:92] [TRT-LLM] [I] Set remove_input_padding to True.\n",
      "I1210 19:57:33.846801 140579951768704 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n",
      "I1210 19:57:33.846818 140579951768704 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n",
      "I1210 19:57:33.846834 140579951768704 logger.py:92] [TRT-LLM] [I] Set enable_xqa to True.\n",
      "I1210 19:57:33.846849 140579951768704 logger.py:92] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.\n",
      "I1210 19:57:33.846869 140579951768704 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
      "I1210 19:57:33.846886 140579951768704 logger.py:92] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n",
      "I1210 19:57:33.846900 140579951768704 logger.py:92] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
      "I1210 19:57:33.846916 140579951768704 logger.py:92] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.\n",
      "I1210 19:57:33.846932 140579951768704 logger.py:92] [TRT-LLM] [I] Set multiple_profiles to False.\n",
      "I1210 19:57:33.846948 140579951768704 logger.py:92] [TRT-LLM] [I] Set paged_state to True.\n",
      "I1210 19:57:33.846965 140579951768704 logger.py:92] [TRT-LLM] [I] Set streamingllm to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-19:57:33] [TRT] [I] Loaded engine size: 15383 MiB\n",
      "[12/10/2024-19:57:34] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 15380 (MiB)\n",
      "[12/10/2024-19:57:34] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 15380 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 19:57:34.818560 140579951768704 logger.py:92] [TRT-LLM] [W] The paged KV cache in Python runtime is experimental. For performance and correctness, please, use C++ runtime.\n",
      "I1210 19:57:34.820907 140579951768704 logger.py:92] [TRT-LLM] [I] Load engine takes: 8.207995176315308 sec\n",
      "I1210 19:57:35.950140 140579951768704 deploy_triton.py:377] Triton deploy function will be called.\n",
      "I1210 19:57:35.952382 140579951768704 deploy_triton.py:384] Model serving on Triton is will be started.\n",
      "[NeMo W 2024-12-10 20:04:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/nested/__init__.py:166: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
      "      return _nested.nested_tensor(\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "PRECISION=bf16\n",
    "MODEL_DIR=\"models/llama-3.1/8B/trt_llm/$PRECISION/tp_1\"\n",
    "mkdir -p \"$MODEL_DIR\"\n",
    "MODEL_CKPT=\"models/llama-3.1/8B/nemo/$PRECISION/Llama-3_1-8B-Instruct.nemo\"\n",
    "LORA_CKPT=\"results/llama-3.1/8B/$PRECISION/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "\n",
    "python /opt/NeMo/scripts/deploy/nlp/deploy_triton.py \\\n",
    "    --nemo_checkpoint \"$MODEL_CKPT\" \\\n",
    "    --lora_ckpt \"$LORA_CKPT\" \\\n",
    "    --use_lora_plugin \\\n",
    "    --model_type llama \\\n",
    "    --triton_model_name llama3-pubmedqa \\\n",
    "    --triton_model_repository \"$MODEL_DIR\" \\\n",
    "    --num_gpus 1 \\\n",
    "    --tensor_parallelism_size 1 \\\n",
    "    --pipeline_parallelism_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af1ce8-2e2c-4c6c-94e7-85e543a9f1f7",
   "metadata": {},
   "source": [
    "Open a terminal to query the model:\n",
    "\n",
    "```shell\n",
    "QUERY=\"Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?\"\n",
    "\n",
    "python /opt/NeMo/scripts/deploy/nlp/query.py \\\n",
    "    -mn llama3-pubmedqa \\\n",
    "    -p \"$QUERY\" \\\n",
    "    -mol 5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff309979-7f9e-4b17-ba3d-5058602c76c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
