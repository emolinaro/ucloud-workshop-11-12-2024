{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f74ada-05b3-43da-b6de-c7e12745c66a",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"https://escience.sdu.dk/index.php/ucloud/\">\n",
    "    <img src=\"https://escience.sdu.dk/wp-content/uploads/2020/03/logo_esc.svg\" width=\"400\" height=\"186\" />\n",
    "  </a>\n",
    "</center>\n",
    "<br>\n",
    "<p style=\"font-size: 1.2em;\">\n",
    "  This notebook was tested using <strong>NeMo Framework v24.07</strong> and machine type <code>u3-gpu4</code> on UCloud.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3323204-1463-4df3-8c75-5e95b6d66ba1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Building a Llama-3.3 LoRA Adapter with the NeMo Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3d632-44a0-4e6c-9229-b70bbcff1e99",
   "metadata": {},
   "source": [
    "This notebook showcases performing LoRA PEFT [**Llama 3.1 8B**](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/tree/main) on [PubMedQA](https://pubmedqa.github.io/) using NeMo Framework. PubMedQA is a Question-Answering dataset for biomedical texts.\n",
    "\n",
    "In this notebook, we demonstrate how to apply Low-Rank Adaptation (LoRA) Parameter-Efficient Fine-Tuning (PEFT) techniques to the Llama 3.3 70B model using the NeMo Framework. We use [PubMedQA](https://pubmedqa.github.io/), a specialized question-answering dataset derived from biomedical literature, to illustrate how LoRA adapters can efficiently enhance model performance within a domain-specific context.\n",
    "\n",
    "**Disclaimer**: This notebook is adapted from the [NVIDIA NeMo tutorial on biomedical QA with Llama-3](https://github.com/NVIDIA/NeMo/blob/main/tutorials/llm/llama-3/biomedical-qa/llama3-lora-nemofw.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee38a79-b107-494e-b8e7-3d1f6d26b412",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Estimating GPU Memory Requirements for Serving LLMs\n",
    "\n",
    "\n",
    "### **1. Model Size**\n",
    "Before you begin, it’s essential to understand how much GPU memory you’ll need to serve a large language model (LLM). A commonly used formula is:\n",
    "\n",
    "$$\n",
    "M_{\\text{model}} = \\frac{(P \\times 4B)}{(32 / Q)}\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- **M**: The GPU memory required (in Gigabytes)  \n",
    "- **P**: The number of parameters in the model (e.g., 7 billion parameters for a 7B model)  \n",
    "- **4B**: 4 bytes, representing the size of each parameter at full precision (32 bits)  \n",
    "- **32**: The number of bits in 4 bytes (32 bits)  \n",
    "- **Q**: The model precision in bits used during serving (e.g., 16 bits, 8 bits, or 4 bits)  \n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Start with $P \\times 4B$ to get the base memory needed for all parameters at full precision (FP32).\n",
    "- Divide by $(32/Q)$, which scales the memory requirement according to the lower-precision format you’re using. For example, loading a model in 16-bit precision effectively halves the memory usage compared to 32-bit.\n",
    "\n",
    "#### **Example:**\n",
    "\n",
    "For a 70B parameter model loaded in 8-bit precision:\n",
    "\n",
    "- $P = 70 \\times 10^9$ ($70$ billion)\n",
    "- $Q = 8$\n",
    "\n",
    "Plugging these in:\n",
    "\n",
    "$$\n",
    "M_{\\text{model}} = \\frac{(70 \\times 10^9 \\times 4B)}{(32 / 8)} \n",
    "= \\frac{(280 \\times 10^9 B)}{2} \n",
    "= 70 \\times 10^9 B\n",
    "$$\n",
    "\n",
    "Convert bytes to gigabytes (1 GB = $10^9$ bytes):\n",
    "\n",
    "$$\n",
    "M = 70 \\text{ GB}\n",
    "$$\n",
    "\n",
    "This rough calculation helps estimate the GPU memory needed for serving large models, ensuring you have the right hardware configuration before starting fine-tuning or inference steps.\n",
    "\n",
    "### **2. Context Window**\n",
    "\n",
    "The **context window** refers to the maximum number of tokens (words or subwords) the model can process in a single inference pass. During inference, the model needs to store activations for each token in the input sequence. This storage requirement scales linearly with the length of the context window.\n",
    "\n",
    "#### **Memory Calculation for Context Window**\n",
    "\n",
    "$$\n",
    "M_{\\text{context}} = L \\times H \\times D \\times N\n",
    "$$\n",
    "\n",
    "- **$M_{\\text{context}}$**: Memory required for the context window (in Gigabytes)\n",
    "- **$L$**: Length of the context window (number of tokens)\n",
    "- **$H$**: Hidden size (dimensionality of the model's hidden layers)\n",
    "- **$D$**: Data type size (bytes per element, e.g., 2 for FP16)\n",
    "- **$N$**: Number of transformer layers\n",
    "\n",
    "#### **Example:**\n",
    "\n",
    "Assume:\n",
    "- **$L = 1024$** tokens\n",
    "- **$H = 8192$** dimensions\n",
    "- **$D = 1$** bytes (for INT8 precision)\n",
    "- **$N = 80$** number of hidden layers\n",
    "\n",
    "$$\n",
    "M_{\\text{context}} = 1024 \\times 8192 \\times 1 \\times 80 = 671,088,640 \\text{ bytes} \\approx 671 \\text{ MB}\n",
    "$$\n",
    "\n",
    "### **3. Batch Size**\n",
    "\n",
    "**Batch size** determines how many input sequences the model processes simultaneously. Increasing the batch size can lead to higher GPU memory usage because the model needs to store activations for each sequence in the batch.\n",
    "\n",
    "#### **Memory Calculation for Batch Size**\n",
    "\n",
    "$$\n",
    "M_{\\text{batch}} = B \\times M_{\\text{context}}\n",
    "$$\n",
    "\n",
    "- **$M_{\\text{batch}}$**: Additional memory required for batching (in Gigabytes)\n",
    "- **$B$**: Batch size (number of sequences)\n",
    "- **$M_{\\text{context}}$**: Memory per sequence (from context window calculation)\n",
    "\n",
    "#### **Example:**\n",
    "\n",
    "Using the previous **$M_{\\text{context}} =  671 \\text{ MB}$** and a **batch size $B = 8$**:\n",
    "\n",
    "$$\n",
    "M_{\\text{batch}} = 8 \\times  671 \\text{ MB} = 5.4 \\text{ GB}\n",
    "$$\n",
    "\n",
    "### **4. Total Inference Memory Estimation**\n",
    "\n",
    "Combining all these factors gives a more comprehensive estimate of the GPU memory required for inference:\n",
    "\n",
    "$$\n",
    "M_{\\text{total}} = M_{\\text{model}} + M_{\\text{context}} \\times B + M_{\\text{overhead}}\n",
    "$$\n",
    "\n",
    "- **$M_{\\text{total}}$**: Total GPU memory required (in Gigabytes)\n",
    "- **$M_{\\text{model}}$**: Memory for the model\n",
    "- **$M_{\\text{context}}$**: Memory per token sequence\n",
    "- **$B$**: Batch size\n",
    "- **$M_{\\text{overhead}}$**: Additional overhead for operations like caching, temporary buffers, etc. (typically 10-20%)\n",
    "\n",
    "#### Example\n",
    "\n",
    "Using the previous results:\n",
    "\n",
    "$$\n",
    "M_{\\text{total}} \\approx 90 \\text{ GB}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f729b57-6b66-46ff-ae26-8f0e7f80428b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import estimate_gpu_memory\n",
    "\n",
    "Q = 16  # 16-bit precision (bfloat16)\n",
    "L = 1024  # Context window\n",
    "B = 8  # Batch size\n",
    "\n",
    "# Example usage for LLama-3.1 8B\n",
    "P_8B = 8_000_000_000  # 8B parameters\n",
    "H_8B = 4096  # Hidden size\n",
    "N_8B = 32\n",
    "\n",
    "estimated_memory_8B = estimate_gpu_memory(P_8B, Q, L, H_8B, B, N_8B)\n",
    "print(f\"Estimated GPU Memory Required for LLama-3 8B: {estimated_memory_8B:.2f} GB\")\n",
    "\n",
    "# Example usage for LLama-3.1 70B\n",
    "P_70B = 70_000_000_000  # 70B parameters\n",
    "H_70B = 8192  # Hidden size\n",
    "N_70B = 80\n",
    "\n",
    "estimated_memory_70B = estimate_gpu_memory(P_70B, Q, L, H_70B, B, N_70B)\n",
    "print(f\"Estimated GPU Memory Required for LLama-3 70B: {estimated_memory_70B:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b285d5a-d838-423b-9d6c-65add61f48ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download the Model\n",
    "Before you begin, ensure you have a local copy of the Meta Llama3.3 70B Instruct model. If you haven’t already downloaded it, you can obtain it from the official [Hugging Face repository](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/tree/main). This step is crucial to ensure that all subsequent operations in the notebook run smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93614eb2-8f1f-49c0-a5bd-c455b4549f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Password\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "pwd = Password(description=\"Hugging Face Token:\")\n",
    "display(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7be368-f5db-4c34-87ec-00f574cd8ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token = pwd.value\n",
    "hf_model=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "hf_model_path=\"models/llama-3.1/8B/hf\"\n",
    "snapshot_download(\n",
    "    repo_id=hf_model,\n",
    "    local_dir=hf_model_path,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3057e525-7957-45c0-bedc-c347d4811081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$hf_model_path\"\n",
    "\n",
    "ls $1\n",
    "du -sh $1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba9d09-2412-404e-9bd9-45e67724a46c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert the Model in NeMo Format\n",
    "\n",
    "To fully leverage the NeMo toolkit and its ecosystem of training, inference, and deployment tools, it’s often necessary to convert your model into NeMo’s native `.nemo` format. For detailed, step-by-step instructions on performing such conversions, refer to the [NeMo user guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/checkpoints/user_guide.html) on checkpoint conversion.\n",
    "\n",
    "This conversion will help ensure compatibility and streamline the process of fine-tuning, evaluating, and deploying your NeMo-based LLM workflows.\n",
    "\n",
    "In this case, we will use the `convert_llama_hf_to_nemo.py` script provided by NeMo:\n",
    "\n",
    "```\n",
    "$ python /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py --help\n",
    "```\n",
    "\n",
    "```text\n",
    "    usage: convert_llama_hf_to_nemo.py [-h] --input_name_or_path INPUT_NAME_OR_PATH --output_path OUTPUT_PATH [--hparams_file HPARAMS_FILE] [--precision PRECISION]\n",
    "\n",
    "    options:\n",
    "      -h, --help            show this help message and exit\n",
    "      --input_name_or_path INPUT_NAME_OR_PATH\n",
    "                            Path to Huggingface LLaMA checkpoints\n",
    "      --output_path OUTPUT_PATH\n",
    "                            Path to output .nemo file.\n",
    "      --hparams_file HPARAMS_FILE\n",
    "                            Path config for restoring (hparams.yaml).\n",
    "      --precision PRECISION\n",
    "                            Model precision\n",
    "```\n",
    "\n",
    "Below is a summary of different model precision choices, along with their key trade-offs:\n",
    "- **FP32 (32-bit Float):** Maximum precision, but slower and uses more memory.\n",
    "- **FP16 (16-bit Float):** Reduces memory usage and speeds up training, but can be numerically unstable if used alone.\n",
    "- **BF16 (BFloat16):** Offers similar speed and memory benefits to FP16, but with greater numerical stability due to a larger exponent range, making it more robust than pure FP16.\n",
    "- **FP16 Mixed Precision:** Employs FP16 for most operations and FP32 for critical ones, striking a balance between performance and stability.\n",
    "- **BF16 Mixed Precision:** Similar to FP16 mixed, but even more stable, leveraging BF16 for most operations and FP32 where necessary for optimal stability, performance, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e25ab64-a613-4d9d-b2f4-0cc9a31f7e16",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "HF_MODEL=\"models/llama-3.1/8B/hf\"\n",
    "PRECISION=bf16\n",
    "NeMo_MODEL=\"models/llama-3.1/8B/nemo/$PRECISION/Llama-3_1-8B-Instruct.nemo\"\n",
    "\n",
    "# Modify rope_scaling properties\n",
    "[ ! -f \"$HF_MODEL/config.json.bak\" ] && cp \"$HF_MODEL/config.json\" \"$HF_MODEL/config.json.bak\"\n",
    "jq '.rope_scaling = {\"factor\": 8.000000001, \"type\": \"linear\"}' \"$HF_MODEL/config.json\" > /tmp/config.tmp && mv /tmp/config.tmp \"$HF_MODEL/config.json\"\n",
    "\n",
    "export TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "# Convert model to .nemo \n",
    "python3 /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py \\\n",
    "        --input_name_or_path \"$HF_MODEL\" \\\n",
    "        --output_path \"$NeMo_MODEL\" \\\n",
    "        --precision \"$PRECISION\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e10e9-5c2f-4f97-b145-40b7d0c524e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "PRECISION=bf16\n",
    "NeMo_MODEL=\"models/llama-3.1/8B/nemo/$PRECISION/Llama-3_1-8B-Instruct.nemo\"\n",
    "\n",
    "file \"$NeMo_MODEL\"\n",
    "du -sh \"$NeMo_MODEL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6a910-a05e-4ae1-aac4-56e5092be2b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Step-by-Step Instructions\n",
    "\n",
    "This notebook is organized into four main steps:\n",
    "\n",
    "1. **Prepare the Dataset:**\n",
    "   Load and preprocess the PubMedQA dataset, ensuring that it’s correctly formatted and ready for fine-tuning.\n",
    "\n",
    "2. **Run the PEFT Fine-Tuning Script:**\n",
    "   Apply Low-Rank Adaptation (LoRA) Parameter-Efficient Fine-Tuning methods to tailor the Llama 3.3 70B model to the PubMedQA domain.\n",
    "\n",
    "3. **Perform Inference with the NeMo Framework:**\n",
    "   Use the trained model to generate answers to biomedical questions and observe how it performs on real queries.\n",
    "\n",
    "4. **Evaluate Model Accuracy:**\n",
    "   Assess the quality and correctness of the model’s responses to measure improvements gained through the fine-tuning process.\n",
    "   \n",
    "5. **Export Model to TensorRT-LLM Format for Inference:**\n",
    "   use the APIs in the export module to export a NeMo checkpoint to TensorRT-LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5bd31",
   "metadata": {},
   "source": [
    "### Step 1: Prepare the dataset\n",
    "\n",
    "Download the PubMedQA dataset and run the pre-processing script in the cloned directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b43c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download the dataset and prep. scripts\n",
    "git clone https://github.com/pubmedqa/pubmedqa.git\n",
    "\n",
    "# split it into train/val/test datasets\n",
    "cd pubmedqa/preprocess\n",
    "python split_dataset.py pqal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025b2d4",
   "metadata": {},
   "source": [
    "The following example shows what a single row looks inside of the PubMedQA train, validation and test splits.\n",
    "\n",
    "```json\n",
    "\"18251357\": {\n",
    "    \"QUESTION\": \"Does histologic chorioamnionitis correspond to clinical chorioamnionitis?\",\n",
    "    \"CONTEXTS\": [\n",
    "        \"To evaluate the degree to which histologic chorioamnionitis, a frequent finding in placentas submitted for histopathologic evaluation, correlates with clinical indicators of infection in the mother.\",\n",
    "        \"A retrospective review was performed on 52 cases with a histologic diagnosis of acute chorioamnionitis from 2,051 deliveries at University Hospital, Newark, from January 2003 to July 2003. Third-trimester placentas without histologic chorioamnionitis (n = 52) served as controls. Cases and controls were selected sequentially. Maternal medical records were reviewed for indicators of maternal infection.\",\n",
    "        \"Histologic chorioamnionitis was significantly associated with the usage of antibiotics (p = 0.0095) and a higher mean white blood cell count (p = 0.018). The presence of 1 or more clinical indicators was significantly associated with the presence of histologic chorioamnionitis (p = 0.019).\"\n",
    "    ],\n",
    "    \"reasoning_required_pred\": \"yes\",\n",
    "    \"reasoning_free_pred\": \"yes\",\n",
    "    \"final_decision\": \"yes\",\n",
    "    \"LONG_ANSWER\": \"Histologic chorioamnionitis is a reliable indicator of infection whether or not it is clinically apparent.\"\n",
    "},\n",
    "```\n",
    "\n",
    "Use the following code to convert the train, validation, and test PubMedQA data into the `JSONL` format that NeMo needs for PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f69729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl(fname):\n",
    "    obj = []\n",
    "    with open(fname, 'rt') as f:\n",
    "        st = f.readline()\n",
    "        while st:\n",
    "            obj.append(json.loads(st))\n",
    "            st = f.readline()\n",
    "    return obj\n",
    "\n",
    "def write_jsonl(fname, json_objs):\n",
    "    with open(fname, 'wt') as f:\n",
    "        for o in json_objs:\n",
    "            f.write(json.dumps(o)+\"\\n\")\n",
    "            \n",
    "def form_question(obj):\n",
    "    st = \"\"    \n",
    "    for i, label in enumerate(obj['LABELS']):\n",
    "        st += f\"{label}: {obj['CONTEXTS'][i]}\\n\"\n",
    "    st += f\"QUESTION: {obj['QUESTION']}\\n\"\n",
    "    st += f\" ### ANSWER (yes|no|maybe): \"\n",
    "    return st\n",
    "\n",
    "def convert_to_jsonl(data_path, output_path):\n",
    "    data = json.load(open(data_path, 'rt'))\n",
    "    json_objs = []\n",
    "    for k in data.keys():\n",
    "        obj = data[k]\n",
    "        prompt = form_question(obj)\n",
    "        completion = obj['final_decision']\n",
    "        json_objs.append({\"input\": prompt, \"output\": f\"<<< {completion} >>>\"})\n",
    "    write_jsonl(output_path, json_objs)\n",
    "    return json_objs\n",
    "\n",
    "\n",
    "test_json_objs = convert_to_jsonl(\"pubmedqa/data/test_set.json\", \"pubmedqa/data/pubmedqa_test.jsonl\")\n",
    "train_json_objs = convert_to_jsonl(\"pubmedqa/data/pqal_fold0/train_set.json\", \"pubmedqa/data/pubmedqa_train.jsonl\")\n",
    "dev_json_objs = convert_to_jsonl(\"pubmedqa/data/pqal_fold0/dev_set.json\", \"pubmedqa/data/pubmedqa_val.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62777542",
   "metadata": {},
   "source": [
    "> `Note:` In the output, we enforce the inclusion of “<<<” and “>>>“ markers which would allow verification of the LoRA tuned model during inference. This is  because the base model can produce “yes” / “no” responses based on zero-shot templates as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddd0f2a",
   "metadata": {},
   "source": [
    "After running the above script, you will see  `pubmedqa_train.jsonl`, `pubmedqa_val.jsonl`, and `pubmedqa_test.jsonl` files appear in the data directory.\n",
    "\n",
    "This is what an example will be formatted like after the script has converted the PubMedQA data into `JSONL` -\n",
    "\n",
    "```json\n",
    "{\"input\": \"QUESTION: Failed IUD insertions in community practice: an under-recognized problem?\\nCONTEXT: The data analysis was conducted to describe the rate of unsuccessful copper T380A intrauterine device (IUD) insertions among women using the IUD for emergency contraception (EC) at community family planning clinics in Utah.\\n ...  ### ANSWER (yes|no|maybe): \",\n",
    "\"output\": \"<<< yes >>>\"}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c5d4e-fafb-40df-806d-7c366071d08d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# clear up cached mem-map file\n",
    "rm pubmedqa/data/*idx*\n",
    "\n",
    "wc -l pubmedqa/data/pubmedqa_train.jsonl\n",
    "wc -l pubmedqa/data/pubmedqa_val.jsonl\n",
    "wc -l pubmedqa/data/pubmedqa_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1d887",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Run PEFT finetuning script for LoRA\n",
    "\n",
    "NeMo framework includes a high level python script for fine-tuning  [megatron_gpt_finetuning.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py) that can abstract away some of the lower level API calls. Once you have your model downloaded and the dataset ready, LoRA fine-tuning with NeMo is essentially just running this script!\n",
    "\n",
    "For this demonstration, this training run is capped by `max_steps`, and validation is carried out every `val_check_interval` steps. If the validation loss does not improve after a few checks, training is halted to avoid overfitting.\n",
    "\n",
    "> `NOTE:` In the block of code below, pass the paths to your train, test and validation data files as well as path to the .nemo model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939482b9-40ce-4aac-876c-894a04fe0742",
   "metadata": {},
   "source": [
    "#### Understanding Global Batch Size (GBS) in Multi-GPU Training\n",
    "\n",
    "\n",
    "##### **1. Global Batch Size (GBS)**\n",
    "- **Definition:**\n",
    "  - The **total number of training samples** processed in **one training step** across **all GPUs** involved.\n",
    "\n",
    "##### **2. Data Parallelism (DP)**\n",
    "- **Definition:**\n",
    "  - The **number of GPUs** that each hold a **replica** of the entire model.\n",
    "  - **Function:** Distributes different data batches to each GPU simultaneously.\n",
    "  - **GAS (Gradient Accumulation Steps):** The number of mini-batches over which gradients are accumulated before performing a parameter update.\n",
    "  - **DP formula:**\n",
    "      $$\n",
    "      \\text{Data Parallelism (DP)} = \\frac{\\text{Total GPUs} \\times \\text{Gradient Accumulation Step (GAS)}}{\\text{Tensor Parallelism (TP)} \\times \\text{Pipeline Parallelism (PP)}}\n",
    "      $$\n",
    "\n",
    "\n",
    "##### **3. Micro Batch Size (MB)**\n",
    "- **Definition:**\n",
    "  - The **number of samples** processed **per GPU** in a single forward/backward pass.\n",
    "\n",
    "##### **4. GBS Formula**\n",
    "$$\n",
    "\\text{Global Batch Size (GBS)} = \\text{Data Parallelism (DP)} \\times \\text{Micro Batch Size (MB)}\n",
    "$$\n",
    "\n",
    "##### **5. How to Set GBS**\n",
    "1. **Determine Available GPUs:**\n",
    "   - Total GPUs (e.g., 4 GPUs).\n",
    "2. **Choose Data Parallelism (DP):**\n",
    "   - Decide how many GPUs to use for DP (e.g., DP = 4).\n",
    "3. **Set Micro Batch Size (MB):**\n",
    "   - Based on GPU memory capacity (e.g., MB = 8).\n",
    "4. **Calculate GBS:**\n",
    "   - Use the formula to find GBS (e.g., GBS = 4 × 8 = 32).\n",
    "\n",
    "##### **Best Practices**\n",
    "- **Align GBS with DP and MB:**\n",
    "  - Ensure $\\text{GBS} = \\text{DP} \\times \\text{MB}$.\n",
    "- **Monitor GPU Utilization:**\n",
    "  - Use tools like `nvidia-smi` to ensure all GPUs are effectively utilized.\n",
    "- **Adjust Batch Sizes as Needed:**\n",
    "  - Optimize **MB** based on memory constraints and **GBS** to balance load.\n",
    "- **Utilize Gradient Accumulation:**\n",
    "  - When larger **GBS** is desired but constrained by memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a230d-51b2-467e-9ea2-aa59a23f08e6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "# Set paths to the model, train, validation and test sets.\n",
    "PRECISION=bf16\n",
    "MODEL=\"models/llama-3.1/8B/nemo/$PRECISION/Llama-3_1-8B-Instruct.nemo\"\n",
    "OUTPUT_DIR=\"results/llama-3.1/8B/$PRECISION\"\n",
    "rm -rf \"$OUTPUT_DIR\"\n",
    "\n",
    "TRAIN_DS=\"[pubmedqa/data/pubmedqa_train.jsonl]\"\n",
    "VALID_DS=\"[pubmedqa/data/pubmedqa_val.jsonl]\"\n",
    "\n",
    "SCHEME=\"lora\"\n",
    "GPUS=1       # set equal to 4 for 70B model\n",
    "TP_SIZE=1    # set equal to 4 for 70B model\n",
    "PP_SIZE=1\n",
    "\n",
    "torchrun --nproc_per_node=${GPUS} \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    exp_manager.exp_dir=${OUTPUT_DIR} \\\n",
    "    exp_manager.explicit_log_dir=${OUTPUT_DIR} \\\n",
    "    trainer.devices=${GPUS} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=${PRECISION} \\\n",
    "    trainer.val_check_interval=20 \\\n",
    "    trainer.max_steps=1000 \\\n",
    "    model.megatron_amp_O2=False \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    model.global_batch_size=8 \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.data.train_ds.num_workers=10 \\\n",
    "    model.data.validation_ds.num_workers=10 \\\n",
    "    model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "    model.peft.peft_scheme=${SCHEME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4331fd-da30-4e29-8477-3085118e4a7b",
   "metadata": {},
   "source": [
    "This will create a LoRA adapter - a file named `megatron_gpt_peft_lora_tuning.nemo` in `./results/.../checkpoints/`. We'll use this later.\n",
    "\n",
    "To further configure the run above -\n",
    "\n",
    "* **A different PEFT technique**: The `peft.peft_scheme` parameter determines the technique being used. In this case, we did LoRA, but NeMo Framework supports other techniques as well - such as P-tuning, Adapters, and IA3. For more information, refer to the [PEFT support matrix](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/nlp/nemo_megatron/peft/landing_page.html). For example, for P-tuning, simply set \n",
    "\n",
    "```bash\n",
    "model.peft.peft_scheme=\"ptuning\" # instead of \"lora\"\n",
    "```\n",
    "\n",
    "* **Tuning Llama-3.1 70B**: You will need 4xH100 GPUs. Provide the path to it's .nemo checkpoint (similar to the download and conversion steps earlier), and change the model parallelization settings for Llama-3 70B PEFT to distribute across the GPUs. It is also recommended to run the fine-tuning script from a terminal directly instead of Jupyter when using more than 1 GPU.\n",
    "```bash\n",
    "model.tensor_model_parallel_size=4\n",
    "model.pipeline_model_parallel_size=1\n",
    "```\n",
    "\n",
    "You can override many such configurations while running the script. A full set of possible configurations is located in [NeMo Framework Github](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/conf/megatron_gpt_finetuning_config.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53979a4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Inference with NeMo Framework\n",
    "\n",
    "Running text generation within the framework is also possible with running a Python script. Note that is more for testing and validation, not a full-fledged  deployment solution like NVIDIA NIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d1e3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check that the LORA model file exists\n",
    "\n",
    "python -c \"import torch; torch.cuda.empty_cache()\"\n",
    "\n",
    "PRECISION=bf16\n",
    "OUTPUT_DIR=\"results/llama-3.1/8B/$PRECISION\"\n",
    "ls -l $OUTPUT_DIR/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3430a0b0-05a0-4179-8750-151d492bb9ae",
   "metadata": {},
   "source": [
    "In the code snippet below, the following configurations are worth noting: \n",
    "\n",
    "1. `model.restore_from_path` to the path for the Meta-Llama-3-8B-Instruct.nemo file.\n",
    "2. `model.peft.restore_from_path` to the path for the PEFT checkpoint that was created in the fine-tuning run in the last step.\n",
    "3. `model.test_ds.file_names` to the path of the pubmedqa_test.jsonl file\n",
    "\n",
    "If you have made any changes in model or experiment paths, please ensure they are configured correctly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93108124-32a5-4c8f-ab25-52dbe9b26ac6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "PRECISION=bf16\n",
    "MODEL=\"models/llama-3.1/8B/nemo/$PRECISION/Llama-3_1-8B-Instruct.nemo\"\n",
    "OUTPUT_DIR=\"results/llama-3.1/8B/$PRECISION\"\n",
    "TEST_DS=\"[pubmedqa/data/pubmedqa_test.jsonl]\"\n",
    "TEST_NAMES=\"[pubmedqa]\"\n",
    "SCHEME=\"lora\"\n",
    "GPUS=1\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# This is where your LoRA checkpoint was saved\n",
    "PATH_TO_TRAINED_MODEL=\"$OUTPUT_DIR/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "\n",
    "# The generation run will save the generated outputs over the test dataset in a file prefixed like so\n",
    "OUTPUT_PREFIX=\"pubmedQA_result_\"\n",
    "\n",
    "export TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "torchrun --nproc_per_node=${GPUS} \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.peft.restore_from_path=${PATH_TO_TRAINED_MODEL} \\\n",
    "    trainer.devices=${GPUS} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    model.data.test_ds.file_names=${TEST_DS} \\\n",
    "    model.data.test_ds.names=${TEST_NAMES} \\\n",
    "    model.data.test_ds.global_batch_size=1 \\\n",
    "    model.data.test_ds.micro_batch_size=1 \\\n",
    "    model.data.test_ds.tokens_to_generate=3 \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.output_file_path_prefix=${OUTPUT_PREFIX} \\\n",
    "    model.data.test_ds.write_predictions_to_file=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe048f9",
   "metadata": {},
   "source": [
    "### Step 4: Check the model accuracy\n",
    "\n",
    "Now that the results are in, let's read the results and calculate the accuracy on the pubmedQA task. You can compare your accuracy results with the public leaderboard at https://pubmedqa.github.io/.\n",
    "\n",
    "Let's take a look at one of the predictions in the generated output file. The `pred` key indicates what was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c0fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "tail -n 1 pubmedQA_result__test_pubmedqa_inputs_preds_labels.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c91df7",
   "metadata": {},
   "source": [
    "Note that the model produces output in the specified format, such as `<<< no >>>`.\n",
    "\n",
    "The following snippet loads the generated output and calculates accuracy in comparison to the test set using the `evaluation.py` script included in the PubMedQA repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f81c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "answers = []\n",
    "with open(\"pubmedQA_result__test_pubmedqa_inputs_preds_labels.jsonl\",'rt') as f:\n",
    "    st = f.readline()\n",
    "    while st:\n",
    "        answers.append(json.loads(st))\n",
    "        st = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1bbce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_test = json.load(open(\"./pubmedqa/data/test_set.json\",'rt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85926e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "sample_id = list(data_test.keys())\n",
    "\n",
    "for i, key in enumerate(sample_id):\n",
    "    answer = answers[i]['pred']\n",
    "    if 'yes' in answer:\n",
    "        results[key] = 'yes'\n",
    "    elif 'no' in answer:\n",
    "        results[key] = 'no'\n",
    "    elif 'maybe' in answer:\n",
    "        results[key] = 'maybe'\n",
    "    else:\n",
    "        print(\"Malformed answer: \", answer)\n",
    "        results[key] = 'maybe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768074cf-d189-4b19-bf28-dc7149029ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dump results in a format that can be ingested by PubMedQA evaluation file\n",
    "FILENAME=\"pubmedqa-llama-3-8b-lora.json\"\n",
    "with(open(FILENAME, \"w\")) as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Evaluation\n",
    "!cp $FILENAME ./pubmedqa/\n",
    "!cd ./pubmedqa/ && python evaluation.py $FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9909283e-e1f8-450e-a730-403e22f621ad",
   "metadata": {},
   "source": [
    "For the Llama-3-8B-Instruct model, you should see accuracy comparable to the below:\n",
    "```\n",
    "Accuracy 0.792000\n",
    "Macro-F1 0.594778\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713898c-8194-41f4-86ef-81b55c518035",
   "metadata": {},
   "source": [
    "## Export Model to TensorRT-LLM Format for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e908dc27-0588-464c-8108-6bfef53666d2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo.export.tensorrt_llm import TensorRTLLM\n",
    "\n",
    "MODEL_DIR=\"models/llama-3.1/8B/trt_llm/bf16/tp_1\"\n",
    "MODEL_CKPT=\"models/llama-3.1/8B/nemo/bf16/Llama-3_1-8B-Instruct.nemo\"\n",
    "LORA_CKPT=\"results/llama-3.1/8B/bf16/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "\n",
    "trt_llm_exporter = TensorRTLLM(\n",
    "    model_dir=MODEL_DIR,\n",
    "    lora_ckpt_list=[LORA_CKPT],\n",
    ")\n",
    "\n",
    "trt_llm_exporter.export(\n",
    "    nemo_checkpoint_path=MODEL_CKPT,\n",
    "    model_type=\"llama\",\n",
    "    n_gpus=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c5d07-9b43-434f-bdb1-821ee465a3b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "PRECISION=bf16\n",
    "MODEL_DIR=\"models/llama-3.1/8B/trt_llm/$PRECISION/tp_1\"\n",
    "mkdir -p \"$MODEL_DIR\"\n",
    "MODEL_CKPT=\"models/llama-3.1/8B/nemo/$PRECISION/Llama-3_1-8B-Instruct.nemo\"\n",
    "LORA_CKPT=\"results/llama-3.1/8B/$PRECISION/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "\n",
    "python /opt/NeMo/scripts/deploy/nlp/deploy_triton.py \\\n",
    "    --nemo_checkpoint \"$MODEL_CKPT\" \\\n",
    "    --lora_ckpt \"$LORA_CKPT\" \\\n",
    "    --use_lora_plugin \\\n",
    "    --model_type llama \\\n",
    "    --triton_model_name llama3-pubmedqa \\\n",
    "    --triton_model_repository \"$MODEL_DIR\" \\\n",
    "    --num_gpus 1 \\\n",
    "    --tensor_parallelism_size 1 \\\n",
    "    --pipeline_parallelism_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af1ce8-2e2c-4c6c-94e7-85e543a9f1f7",
   "metadata": {},
   "source": [
    "Open a terminal to query the model:\n",
    "\n",
    "```shell\n",
    "QUERY=\"Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?\"\n",
    "\n",
    "python /opt/NeMo/scripts/deploy/nlp/query.py \\\n",
    "    -mn llama3-pubmedqa \\\n",
    "    -p \"$QUERY\" \\\n",
    "    -mol 5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff309979-7f9e-4b17-ba3d-5058602c76c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
