{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323f65fd-c390-4b61-804f-e5cfcb70a7ef",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"https://escience.sdu.dk/index.php/ucloud/\">\n",
    "    <img src=\"https://escience.sdu.dk/wp-content/uploads/2020/03/logo_esc.svg\" width=\"400\" height=\"186\" />\n",
    "  </a>\n",
    "</center>\n",
    "<br>\n",
    "<p style=\"font-size: 1.2em;\">\n",
    "  This notebook was tested using <strong>Triton Inference Server (TRT-LLM) v24.08</strong> and machine type <code>u3-gpu4</code> on UCloud.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0462d8-0a60-4c48-b8dd-06ae560be1dd",
   "metadata": {},
   "source": [
    "## Hugging Face Authentication\n",
    "\n",
    "The following code creates a secure input widget for your Hugging Face token, which is required to authenticate and download the [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) model from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec322c9-9520-40bd-a75f-459ec0b6bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Password\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "pwd = Password(description=\"Hugging Face Token:\")\n",
    "display(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcbe2c4-d9ce-4708-bbf4-479b56db40d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token = pwd.value\n",
    "hf_model=\"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "hf_model_path=\"models/llama-3.3/70B/hf\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=hf_model,\n",
    "    local_dir=hf_model_path,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb284d44-c990-4b16-b650-acb3e3be370d",
   "metadata": {},
   "source": [
    "## Convert the model to TensorRT Format\n",
    "\n",
    "The following Bash script sets up the required directories and executes the conversion of the Llama-3.3-70B-Instruct model checkpoint from Hugging Face format to TensorRT for optimized performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced79cc-20cb-43af-adec-fabd75697062",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "HF_MODEL=\"models/llama-3.3/70B/hf\"\n",
    "\n",
    "# Modify rope_scaling properties\n",
    "[ ! -f \"$HF_MODEL/config.json.bak\" ] && cp \"$HF_MODEL/config.json\" \"$HF_MODEL/config.json.bak\"\n",
    "jq '.rope_scaling = {\"factor\": 8.000000001, \"type\": \"linear\"}' \"$HF_MODEL/config.json\" > /tmp/config.tmp && mv /tmp/config.tmp \"$HF_MODEL/config.json\"\n",
    "\n",
    "du -sh \"$HF_MODEL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc9a83-3659-4ce9-be7a-c077098d1511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "HF_MODEL=\"models/llama-3.3/70B/hf\"\n",
    "TRT_CKPT=\"models/llama-3.3/70B/trt_ckpt/tp2_pp2\"\n",
    "mkdir -p \"$TRT_CKPT\"\n",
    "\n",
    "python ~/llama/convert_checkpoint.py \\\n",
    "      --model_dir \"$HF_MODEL\" \\\n",
    "      --output_dir \"$TRT_CKPT\" \\\n",
    "      --dtype bfloat16 \\\n",
    "      --tp_size 2 \\\n",
    "      --pp_size 2 \\\n",
    "      --load_by_shard \\\n",
    "      --workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5411031-5aa0-4e27-935b-c847fb345a44",
   "metadata": {},
   "source": [
    "## Build TensorRT Engine\n",
    "\n",
    "The following Bash script constructs the TensorRT engine from the previously converted Llama-3.3-70B-Instruct model checkpoint. This optimization enhances the model's inference performance by leveraging TensorRT's efficient execution capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b55df1-420c-4530-bab0-b010efc6c9fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "TRT_CKPT=\"models/llama-3.3/70B/trt_ckpt/tp2_pp2\"\n",
    "TRT_ENGINE=\"models/llama-3.3/70B/trt_llm/tp2_pp2\"\n",
    "\n",
    "trtllm-build --checkpoint_dir \"$TRT_CKPT\" \\\n",
    "      --output_dir \"$TRT_ENGINE\" \\\n",
    "      --max_num_tokens 4096 \\\n",
    "      --max_input_len 255000 \\\n",
    "      --max_seq_len 256000 \\\n",
    "      --use_paged_context_fmha enable \\\n",
    "      --workers 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15bf2f1-750f-4d4c-bff0-e4dd39a540c9",
   "metadata": {},
   "source": [
    "## Local Testing of TensorRT-Optimized model\n",
    "\n",
    "The following Bash script performs a local test of the optimized Llama-3.3-70B-Instruct model. It sets the necessary environment variables and runs the `run.py` script with a sample prompt to evaluate the model's inference performance.\n",
    "\n",
    "If you get an error in the cell below, update the Transformer library:\n",
    "```bash\n",
    "pip install -U Transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb42e3-6dc6-4692-a11e-d887dd8321ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "HF_MODEL=\"models/llama-3.3/70B/hf\"\n",
    "TRT_ENGINE=\"models/llama-3.3/70B/trt_llm/tp2_pp2\"\n",
    "\n",
    "PROMPT=\"The capital of Indonesia is\"\n",
    "\n",
    "mpirun -n 4 python ~/run.py \\\n",
    "    --max_output_len=10 \\\n",
    "    --tokenizer_dir $HF_MODEL \\\n",
    "    --engine_dir $TRT_ENGINE \\\n",
    "    --input_text \"$PROMPT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba3398-107e-4856-afc2-113070a1e808",
   "metadata": {},
   "source": [
    "## Deploying Triton with Inflight Batching\n",
    "\n",
    "The following Bash scripts set up and configure Triton Inference Server for the Llama-3.3-70B-Instruct model using inflight batching. This deployment optimizes inference performance by managing batch sizes and instance counts effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb947618-8ac0-4f98-b2ff-e2400cf28120",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "TRTITON_REPO=\"models/llama-3.3/70B/triton\"\n",
    "mkdir -p \"$TRTITON_REPO\"\n",
    "\n",
    "cp -r ~/all_models/inflight_batcher_llm/* \"$TRTITON_REPO\"\n",
    "\n",
    "ls \"$TRTITON_REPO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a7902-effd-4235-a1a8-0dec875b9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ENGINE_DIR=\"models/llama-3.3/70B/trt_llm/tp2_pp2\"\n",
    "TOKENIZER_DIR=\"models/llama-3.3/70B/hf\"\n",
    "MODEL_FOLDER=\"models/llama-3.3/70B/triton\"\n",
    "TRITON_MAX_BATCH_SIZE=4\n",
    "INSTANCE_COUNT=4\n",
    "MAX_QUEUE_DELAY_MS=0\n",
    "MAX_QUEUE_SIZE=0\n",
    "FILL_TEMPLATE_SCRIPT=\"$HOME/tools/fill_template.py\"\n",
    "DECOUPLED_MODE=false\n",
    "\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/ensemble/config.pbtxt triton_max_batch_size:${TRITON_MAX_BATCH_SIZE}\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/preprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},preprocessing_instance_count:${INSTANCE_COUNT}\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm/config.pbtxt triton_backend:tensorrtllm,triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},engine_dir:${ENGINE_DIR},max_queue_delay_microseconds:${MAX_QUEUE_DELAY_MS},batching_strategy:inflight_fused_batching,max_queue_size:${MAX_QUEUE_SIZE}\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/postprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},postprocessing_instance_count:${INSTANCE_COUNT},max_queue_size:${MAX_QUEUE_SIZE}\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},bls_instance_count:${INSTANCE_COUNT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb0716-5424-41e4-a5da-78d043fc7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Start the Triton server\n",
    "\n",
    "MODEL_FOLDER=\"models/llama-3.3/70B/triton\"\n",
    "stop_tritonserver\n",
    "nohup mpirun -np 4 tritonserver --model-repository=$MODEL_FOLDER &> /work/triton-server-log.txt &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45e251-b116-4b5e-80ba-4a183eb98914",
   "metadata": {},
   "source": [
    "## Testing the Triton Inference Server\n",
    "\n",
    "The following Bash commands verify that the Triton server and the deployed Llama-3.3-70B-Instruct model are running correctly. The first command checks the repository status, and the second sends a sample generation request to the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df9769-4297-4417-b23e-184bb4565e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "LOG_FILE=\"/work/triton-server-log.txt\"\n",
    "\n",
    "# Function to wait for Triton to start by monitoring the log file\n",
    "wait_for_triton_start() {\n",
    "    echo \"Waiting for Triton Inference Server to start...\"\n",
    "    while true; do\n",
    "        # Check for all required startup messages\n",
    "        if grep -q 'Started GRPCInferenceService at 0.0.0.0:8001' \"$LOG_FILE\" &&\n",
    "           grep -q 'Started HTTPService at 0.0.0.0:8000' \"$LOG_FILE\" &&\n",
    "           grep -q 'Started Metrics Service at 0.0.0.0:8002' \"$LOG_FILE\"; then\n",
    "                echo \"Triton Inference Server is ready.\"\n",
    "                break\n",
    "                \n",
    "        else\n",
    "            echo \"Triton not ready yet. Retrying in 5 seconds...\"\n",
    "            sleep 5   \n",
    "        fi\n",
    "    done\n",
    "}\n",
    "\n",
    "# Wait for Triton to start\n",
    "wait_for_triton_start\n",
    "\n",
    "curl -X POST http://localhost:8000/v2/repository/index -H \"Content-Type: application/json\" -d '{\"ready\": true}'| jq '.[] | select(.state == \"READY\")'\n",
    "\n",
    "curl -X POST localhost:8000/v2/models/ensemble/generate -d '{\"text_input\": \"What is ML?\", \"max_tokens\": 50, \"bad_words\": \"\", \"stop_words\": \"\"}' | jq -r '.text_output'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e523f-8fc2-4d59-a987-9ed0544e32a7",
   "metadata": {},
   "source": [
    "## Performance Profiling with `genai-perf`\n",
    "\n",
    "To evaluate the performance of the deployed Llama-3.3-70B-Instruct model on Triton Inference Server, execute the following Bash commands in a terminal session within Jupyter. This script uses [GenAI-Perf](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/perf_analyzer/genai-perf/README.html) to profile the model, generating performance metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a1db57-d2c4-4e27-99ec-742236f24593",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "TOKENIZER_DIR=\"models/llama-3.3/70B/hf\"\n",
    "\n",
    "genai-perf profile -m ensemble \\\n",
    "    --service-kind triton \\\n",
    "    --backend tensorrtllm \\\n",
    "    --num-prompts 100 \\\n",
    "    --random-seed 1234 \\\n",
    "    --synthetic-input-tokens-mean 200 \\\n",
    "    --synthetic-input-tokens-stddev 0 \\\n",
    "    --output-tokens-mean 100 \\\n",
    "    --output-tokens-stddev 0 \\\n",
    "    --output-tokens-mean-deterministic \\\n",
    "    --tokenizer $TOKENIZER_DIR \\\n",
    "    --concurrency 500 \\\n",
    "    --measurement-interval 4000 \\\n",
    "    --profile-export-file model_profile.json \\\n",
    "    --url localhost:8001 \\\n",
    "    --generate-plots\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
