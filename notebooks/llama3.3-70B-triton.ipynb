{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323f65fd-c390-4b61-804f-e5cfcb70a7ef",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"https://escience.sdu.dk/index.php/ucloud/\">\n",
    "    <img src=\"https://escience.sdu.dk/wp-content/uploads/2020/03/logo_esc.svg\" width=\"400\" height=\"186\" />\n",
    "  </a>\n",
    "</center>\n",
    "<br>\n",
    "<p style=\"font-size: 1.2em;\">\n",
    "  This notebook was tested using <strong>Triton Inference Server (TRT-LLM) v24.08</strong> and machine type <code>u3-gpu4</code> on UCloud.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0462d8-0a60-4c48-b8dd-06ae560be1dd",
   "metadata": {},
   "source": [
    "## Hugging Face Authentication\n",
    "\n",
    "The following code creates a secure input widget for your Hugging Face token, which is required to authenticate and download the [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) model from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec322c9-9520-40bd-a75f-459ec0b6bc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff7d102f9194f3c893e0e034731d7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Password(description='Hugging Face Token:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Password\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "pwd = Password(description=\"Hugging Face Token:\")\n",
    "display(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbcbe2c4-d9ce-4708-bbf4-479b56db40d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142a8aba1e2f4cb8a66cefa774b54111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 53 files:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa4c2e3bdc341baa7df57899b61e87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/879 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/work/ucloud-workshop-11-12-2024/models/llama-3.3/70B/hf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = pwd.value\n",
    "hf_model=\"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "hf_model_path=\"models/llama-3.3/70B/hf\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=hf_model,\n",
    "    local_dir=hf_model_path,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb284d44-c990-4b16-b650-acb3e3be370d",
   "metadata": {},
   "source": [
    "## Convert the model to TensorRT Format\n",
    "\n",
    "The following Bash script sets up the required directories and executes the conversion of the Llama-3.3-70B-Instruct model checkpoint from Hugging Face format to TensorRT for optimized performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ced79cc-20cb-43af-adec-fabd75697062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263G\tmodels/llama-3.3/70B/hf\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HF_MODEL=\"models/llama-3.3/70B/hf\"\n",
    "\n",
    "# Modify rope_scaling properties\n",
    "[ ! -f \"$HF_MODEL/config.json.bak\" ] && cp \"$HF_MODEL/config.json\" \"$HF_MODEL/config.json.bak\"\n",
    "jq '.rope_scaling = {\"factor\": 8.000000001, \"type\": \"linear\"}' \"$HF_MODEL/config.json\" > /tmp/config.tmp && mv /tmp/config.tmp \"$HF_MODEL/config.json\"\n",
    "\n",
    "du -sh \"$HF_MODEL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58fc9a83-3659-4ce9-be7a-c077098d1511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.12.0.dev2024080600\n",
      "0.12.0.dev2024080600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "5it [00:00, 17.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "5it [00:00, 14.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "4it [00:00,  8.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "4it [00:00,  8.85it/s]\u001b[A\n",
      "5it [00:00,  8.17it/s]\u001b[A\n",
      "\n",
      "8it [00:00,  8.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "6it [00:01,  3.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "13it [00:02,  5.23it/s][A\n",
      "13it [00:02,  5.29it/s]\u001b[A\n",
      "\n",
      "\n",
      "12it [00:02,  3.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "16it [00:02,  6.59it/s]\u001b[A\n",
      "\n",
      "\n",
      "13it [00:02,  3.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "12it [00:02,  3.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "15it [00:02,  4.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "21it [00:03,  6.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "19it [00:03,  5.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "19it [00:03,  5.56it/s]\u001b[A\u001b[A\n",
      "20it [00:03,  5.65it/s]\u001b[A\n",
      "23it [00:03,  6.73it/s]\u001b[A\n",
      "\n",
      "22it [00:03,  6.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "27it [00:03,  8.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "27it [00:04,  8.19it/s]\u001b[A\n",
      "\n",
      "26it [00:04,  4.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "34it [00:04,  7.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "35it [00:05,  7.60it/s]\u001b[A\n",
      "36it [00:05,  8.38it/s]\u001b[A\n",
      "\n",
      "\n",
      "29it [00:05,  5.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "29it [00:05,  5.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "33it [00:05,  8.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "33it [00:05,  7.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "36it [00:05,  9.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "35it [00:05,  8.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "41it [00:05,  7.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "41it [00:05,  7.46it/s]\u001b[A\n",
      "\n",
      "40it [00:05,  9.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "43it [00:06, 11.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "48it [00:06,  8.48it/s]\u001b[A\u001b[A\n",
      "48it [00:06,  7.19it/s]\u001b[A\n",
      "\n",
      "\n",
      "47it [00:06,  7.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "47it [00:07,  7.35it/s]\u001b[A\u001b[A\n",
      "55it [00:07,  7.29it/s]\u001b[A\n",
      "\n",
      "54it [00:07,  7.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "62it [00:08,  7.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "62it [00:08,  6.98it/s]\u001b[A\n",
      "\n",
      "61it [00:08,  7.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "61it [00:08,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "65it [00:08,  7.99it/s]\u001b[A\n",
      "\n",
      "68it [00:09,  7.40it/s]\u001b[A\u001b[A\n",
      "69it [00:09,  6.72it/s]\u001b[A\n",
      "\n",
      "\n",
      "69it [00:09,  6.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "75it [00:10,  7.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "75it [00:11,  6.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "76it [00:11,  5.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "76it [00:11,  6.49it/s]\u001b[A\u001b[A\n",
      "79it [00:11,  6.79it/s]\u001b[A\n",
      "79it [00:11,  6.77it/s]\u001b[A\n",
      "\n",
      "82it [00:11,  7.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "83it [00:12,  5.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "83it [00:12,  5.59it/s]\u001b[A\n",
      "86it [00:12,  6.35it/s]\u001b[A\n",
      "\n",
      "89it [00:12,  7.19it/s]\u001b[A\u001b[A\n",
      "90it [00:13,  6.89it/s]\u001b[A\n",
      "\n",
      "\n",
      "89it [00:13,  6.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "96it [00:13,  8.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "96it [00:13,  7.45it/s]\u001b[A\u001b[A\n",
      "100it [00:14,  7.53it/s][A\n",
      "\n",
      "\n",
      "103it [00:14,  7.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "103it [00:15,  6.62it/s]\u001b[A\u001b[A\n",
      "104it [00:15,  7.14it/s]\u001b[A\n",
      "111it [00:16,  6.90it/s]\u001b[A\n",
      "\n",
      "\n",
      "110it [00:16,  6.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "110it [00:16,  6.56it/s]\u001b[A\u001b[A\n",
      "118it [00:16,  7.46it/s]\u001b[A\n",
      "118it [00:16,  7.44it/s]\u001b[A\n",
      "\n",
      "\n",
      "117it [00:17,  6.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "125it [00:17,  8.57it/s]\u001b[A\u001b[A\n",
      "125it [00:17,  8.49it/s]\u001b[A\n",
      "129it [00:17, 10.14it/s]\u001b[A\n",
      "\n",
      "\n",
      "124it [00:18,  6.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "124it [00:18,  6.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "127it [00:18,  6.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "132it [00:19,  6.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "139it [00:20,  6.02it/s]\u001b[A\n",
      "139it [00:20,  5.59it/s]\u001b[A\n",
      "\n",
      "\n",
      "131it [00:20,  4.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "131it [00:20,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "132it [00:21,  3.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "138it [00:21,  5.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "139it [00:21,  6.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "146it [00:21,  5.73it/s]\u001b[A\u001b[A\n",
      "146it [00:21,  5.43it/s]\u001b[A\n",
      "153it [00:22,  6.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "145it [00:22,  5.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "145it [00:22,  5.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "160it [00:23,  7.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "160it [00:23,  7.28it/s]\u001b[A\n",
      "\n",
      "\n",
      "152it [00:23,  5.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "167it [00:23,  8.47it/s]\u001b[A\n",
      "\n",
      "\n",
      "153it [00:23,  5.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "152it [00:23,  5.78it/s]\u001b[A\u001b[A\n",
      "174it [00:24,  7.90it/s]\u001b[A\n",
      "177it [00:24,  8.74it/s]\u001b[A\n",
      "\n",
      "\n",
      "159it [00:24,  5.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "160it [00:25,  5.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "159it [00:25,  5.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "181it [00:25,  7.18it/s]\u001b[A\u001b[A\n",
      "181it [00:25,  6.69it/s]\u001b[A\n",
      "\n",
      "\n",
      "166it [00:25,  6.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "166it [00:26,  6.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "188it [00:26,  7.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "188it [00:26,  6.73it/s]\u001b[A\n",
      "189it [00:27,  6.52it/s]\u001b[A\n",
      "191it [00:27,  6.61it/s]\u001b[A\n",
      "\n",
      "\n",
      "173it [00:27,  5.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "173it [00:27,  5.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "174it [00:27,  5.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "195it [00:28,  5.54it/s]\u001b[A\u001b[A\n",
      "195it [00:28,  4.55it/s]\u001b[A\n",
      "\n",
      "\n",
      "196it [00:28,  5.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "180it [00:28,  5.14it/s]\u001b[A\u001b[A\n",
      "202it [00:30,  4.59it/s]\u001b[A\n",
      "\n",
      "187it [00:30,  4.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "202it [00:30,  4.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "209it [00:31,  5.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "209it [00:31,  5.28it/s]\u001b[A\n",
      "\n",
      "212it [00:31,  6.41it/s]\u001b[A\u001b[A\n",
      "216it [00:32,  6.17it/s]\u001b[A\n",
      "\n",
      "\n",
      "201it [00:32,  5.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "201it [00:32,  5.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "204it [00:32,  6.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "223it [00:33,  5.85it/s]\u001b[A\u001b[A\n",
      "223it [00:33,  5.84it/s]\u001b[A\n",
      "\n",
      "\n",
      "208it [00:33,  5.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "208it [00:33,  5.74it/s]\u001b[A\u001b[A\n",
      "233it [00:34,  8.07it/s]\u001b[A\n",
      "233it [00:34,  7.79it/s]\u001b[A\n",
      "\n",
      "\n",
      "237it [00:35,  6.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "237it [00:35,  6.09it/s]\u001b[A\n",
      "\n",
      "215it [00:35,  5.01it/s]\u001b[A\u001b[A\n",
      "258it [00:35, 14.52it/s]\u001b[A\n",
      "258it [00:35, 13.62it/s]\u001b[A\n",
      "266it [00:35, 18.28it/s]\u001b[A\n",
      "\n",
      "\n",
      "222it [00:36,  5.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "272it [00:36, 13.96it/s]\u001b[A\u001b[A\n",
      "272it [00:36, 12.56it/s]\u001b[A\n",
      "284it [00:37,  7.56it/s]\u001b[A\n",
      "\n",
      "280it [00:37,  9.89it/s]\u001b[A\n",
      "\n",
      "\n",
      "229it [00:38,  4.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "229it [00:38,  4.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "230it [00:39,  4.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "283it [00:39,  6.16it/s]\u001b[A\n",
      "\n",
      "284it [00:39,  7.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "231it [00:39,  3.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "231it [00:39,  3.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "232it [00:40,  3.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "233it [00:40,  3.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "232it [00:40,  3.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "234it [00:40,  3.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "235it [00:40,  3.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "233it [00:40,  3.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "234it [00:41,  3.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "235it [00:41,  3.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "236it [00:42,  1.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "236it [00:43,  1.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "237it [00:43,  1.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "238it [00:43,  2.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "237it [00:43,  1.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "239it [00:43,  2.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "238it [00:44,  1.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "240it [00:44,  2.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "241it [00:44,  2.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "239it [00:44,  1.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "242it [00:44,  2.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "240it [00:44,  2.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "241it [00:44,  2.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "242it [00:45,  3.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "243it [00:46,  1.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "244it [00:47,  1.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "245it [00:47,  1.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "246it [00:47,  1.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "247it [00:47,  2.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "243it [00:48,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "248it [00:48,  2.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "249it [01:12,  2.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "244it [01:12,  7.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "246it [01:12,  4.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "247it [01:12,  3.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "248it [01:14,  2.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "250it [01:14,  7.99s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "249it [01:15,  2.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "251it [01:15,  5.80s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "252it [01:15,  4.25s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "253it [01:16,  3.20s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "250it [01:16,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "254it [01:16,  2.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "255it [01:16,  1.70s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "251it [01:17,  1.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "252it [01:17,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "253it [01:17,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "254it [01:18,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "255it [01:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "256it [01:19,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "257it [01:19,  1.52s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "258it [01:38,  1.20s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "259it [01:38,  5.87s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "260it [01:39,  4.37s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "257it [01:39,  6.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "264it [01:40,  1.84s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "264it [01:40,  1.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "271it [01:41,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "271it [01:41,  1.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "278it [01:43,  1.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "285it [01:43,  2.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "285it [01:43,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time of converting checkpoints: 00:02:53\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HF_MODEL=\"models/llama-3.3/70B/hf\"\n",
    "TRT_CKPT=\"models/llama-3.3/70B/trt_ckpt/tp2_pp2\"\n",
    "mkdir -p \"$TRT_CKPT\"\n",
    "\n",
    "python ~/llama/convert_checkpoint.py \\\n",
    "      --model_dir \"$HF_MODEL\" \\\n",
    "      --output_dir \"$TRT_CKPT\" \\\n",
    "      --dtype bfloat16 \\\n",
    "      --tp_size 2 \\\n",
    "      --pp_size 2 \\\n",
    "      --load_by_shard \\\n",
    "      --workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5411031-5aa0-4e27-935b-c847fb345a44",
   "metadata": {},
   "source": [
    "## Build TensorRT Engine\n",
    "\n",
    "The following Bash script constructs the TensorRT engine from the previously converted Llama-3.3-70B-Instruct model checkpoint. This optimization enhances the model's inference performance by leveraging TensorRT's efficient execution capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42b55df1-420c-4530-bab0-b010efc6c9fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.12.0.dev2024080600\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set bert_attention_plugin to auto.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set gpt_attention_plugin to auto.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set gemm_plugin to None.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set gemm_swiglu_plugin to None.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set fp8_rowwise_gemm_plugin to None.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set nccl_plugin to auto.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set lookup_plugin to None.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set lora_plugin to None.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set moe_plugin to auto.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set mamba_conv1d_plugin to auto.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set context_fmha to True.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set bert_context_fmha_fp32_acc to False.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set remove_input_padding to True.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set reduce_fusion to False.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set enable_xqa to True.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set tokens_per_block to 64.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set use_paged_context_fmha to True.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set multiple_profiles to False.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set paged_state to True.\n",
      "[12/17/2024-11:45:05] [TRT-LLM] [I] Set streamingllm to False.\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.12.0.dev2024080600\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.12.0.dev2024080600\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.12.0.dev2024080600\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.12.0.dev2024080600\n",
      "[12/17/2024-11:45:13] [TRT-LLM] [I] Set dtype to bfloat16.\n",
      "[12/17/2024-11:45:13] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n",
      "[12/17/2024-11:45:13] [TRT-LLM] [W] padding removal and fMHA are both enabled, max_input_len is not required and will be ignored\n",
      "[12/17/2024-11:45:14] [TRT] [I] [MemUsageChange] Init CUDA: CPU +17, GPU +0, now: CPU 157, GPU 528 (MiB)\n",
      "[12/17/2024-11:45:14] [TRT-LLM] [I] Set dtype to bfloat16.\n",
      "[12/17/2024-11:45:14] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n",
      "[12/17/2024-11:45:14] [TRT-LLM] [W] padding removal and fMHA are both enabled, max_input_len is not required and will be ignored\n",
      "[12/17/2024-11:45:14] [TRT] [I] [MemUsageChange] Init CUDA: CPU +1, GPU +0, now: CPU 161, GPU 528 (MiB)\n",
      "[12/17/2024-11:45:14] [TRT-LLM] [I] Set dtype to bfloat16.\n",
      "[12/17/2024-11:45:14] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n",
      "[12/17/2024-11:45:14] [TRT-LLM] [W] padding removal and fMHA are both enabled, max_input_len is not required and will be ignored\n",
      "[12/17/2024-11:45:14] [TRT] [I] [MemUsageChange] Init CUDA: CPU +1, GPU +0, now: CPU 161, GPU 528 (MiB)\n",
      "[12/17/2024-11:45:14] [TRT-LLM] [I] Set dtype to bfloat16.\n",
      "[12/17/2024-11:45:14] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n",
      "[12/17/2024-11:45:14] [TRT-LLM] [W] padding removal and fMHA are both enabled, max_input_len is not required and will be ignored\n",
      "[12/17/2024-11:45:14] [TRT] [I] [MemUsageChange] Init CUDA: CPU +1, GPU +0, now: CPU 161, GPU 528 (MiB)\n",
      "[12/17/2024-11:45:18] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4582, GPU +1268, now: CPU 4894, GPU 1796 (MiB)\n",
      "[12/17/2024-11:45:18] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n",
      "[12/17/2024-11:45:18] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "[12/17/2024-11:45:18] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4582, GPU +1268, now: CPU 4899, GPU 1796 (MiB)\n",
      "[12/17/2024-11:45:18] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n",
      "[12/17/2024-11:45:18] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "[12/17/2024-11:45:18] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4582, GPU +1268, now: CPU 4899, GPU 1796 (MiB)\n",
      "[12/17/2024-11:45:18] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4582, GPU +1268, now: CPU 4899, GPU 1796 (MiB)\n",
      "[12/17/2024-11:45:18] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n",
      "[12/17/2024-11:45:18] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "[12/17/2024-11:45:18] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n",
      "[12/17/2024-11:45:18] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "[12/17/2024-11:45:18] [TRT-LLM] [I] Total optimization profiles added: 1\n",
      "[12/17/2024-11:45:18] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n",
      "[12/17/2024-11:45:18] [TRT] [W] Unused Input: position_ids\n",
      "[12/17/2024-11:45:18] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[12/17/2024-11:45:18] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[12/17/2024-11:45:18] [TRT-LLM] [I] Total optimization profiles added: 1\n",
      "[12/17/2024-11:45:18] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n",
      "[12/17/2024-11:45:18] [TRT-LLM] [I] Total optimization profiles added: 1\n",
      "[12/17/2024-11:45:18] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n",
      "[12/17/2024-11:45:18] [TRT] [W] Unused Input: position_ids\n",
      "[12/17/2024-11:45:18] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[12/17/2024-11:45:18] [TRT-LLM] [I] Total optimization profiles added: 1\n",
      "[12/17/2024-11:45:18] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n",
      "[12/17/2024-11:45:18] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[12/17/2024-11:45:18] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[12/17/2024-11:45:18] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[12/17/2024-11:45:59] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[12/17/2024-11:45:59] [TRT] [I] Detected 15 inputs and 1 output network tensors.\n",
      "[12/17/2024-11:46:03] [TRT] [I] Total Host Persistent Memory: 88544\n",
      "[12/17/2024-11:46:03] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[12/17/2024-11:46:03] [TRT] [I] Total Scratch Memory: 419430912\n",
      "[12/17/2024-11:46:03] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 494 steps to complete.\n",
      "[12/17/2024-11:46:03] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 21.5781ms to assign 18 blocks to 494 nodes requiring 687873024 bytes.\n",
      "[12/17/2024-11:46:03] [TRT] [I] Total Activation Memory: 687871488\n",
      "[12/17/2024-11:46:06] [TRT] [I] Total Weights Memory: 36395287296\n",
      "[12/17/2024-11:46:06] [TRT] [I] Engine generation completed in 47.4297 seconds.\n",
      "[12/17/2024-11:46:06] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 64 MiB, GPU 34710 MiB\n",
      "[12/17/2024-11:46:08] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[12/17/2024-11:46:08] [TRT] [I] Detected 15 inputs and 1 output network tensors.\n",
      "[12/17/2024-11:46:11] [TRT] [I] Total Host Persistent Memory: 88768\n",
      "[12/17/2024-11:46:11] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[12/17/2024-11:46:11] [TRT] [I] Total Scratch Memory: 419430912\n",
      "[12/17/2024-11:46:11] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 496 steps to complete.\n",
      "[12/17/2024-11:46:11] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 23.8436ms to assign 18 blocks to 496 nodes requiring 687873024 bytes.\n",
      "[12/17/2024-11:46:11] [TRT] [I] Total Activation Memory: 687871488\n",
      "[12/17/2024-11:46:11] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 76368 MiB\n",
      "[12/17/2024-11:46:11] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:53\n",
      "[12/17/2024-11:46:11] [TRT-LLM] [I] Build phase peak memory: 76370.96 MB, children: 17.38 MB\n",
      "[12/17/2024-11:46:12] [TRT-LLM] [I] Serializing engine to models/llama-3.3/70B/trt_llm/tp2_pp2/rank1.engine...\n",
      "[12/17/2024-11:46:14] [TRT] [I] Total Weights Memory: 35344630528\n",
      "[12/17/2024-11:46:14] [TRT] [I] Engine generation completed in 55.8399 seconds.\n",
      "[12/17/2024-11:46:14] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 64 MiB, GPU 33708 MiB\n",
      "[12/17/2024-11:46:15] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[12/17/2024-11:46:15] [TRT] [I] Detected 15 inputs and 1 output network tensors.\n",
      "[12/17/2024-11:46:18] [TRT] [I] Total Host Persistent Memory: 88768\n",
      "[12/17/2024-11:46:18] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[12/17/2024-11:46:18] [TRT] [I] Total Scratch Memory: 419430912\n",
      "[12/17/2024-11:46:18] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 496 steps to complete.\n",
      "[12/17/2024-11:46:18] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 24.0856ms to assign 18 blocks to 496 nodes requiring 687873024 bytes.\n",
      "[12/17/2024-11:46:18] [TRT] [I] Total Activation Memory: 687871488\n",
      "[12/17/2024-11:46:19] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 74508 MiB\n",
      "[12/17/2024-11:46:19] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:01:01\n",
      "[12/17/2024-11:46:19] [TRT-LLM] [I] Build phase peak memory: 74510.45 MB, children: 17.35 MB\n",
      "[12/17/2024-11:46:20] [TRT-LLM] [I] Serializing engine to models/llama-3.3/70B/trt_llm/tp2_pp2/rank3.engine...\n",
      "[12/17/2024-11:46:23] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[12/17/2024-11:46:23] [TRT] [I] Detected 15 inputs and 1 output network tensors.\n",
      "[12/17/2024-11:46:25] [TRT] [I] Total Weights Memory: 35344630528\n",
      "[12/17/2024-11:46:25] [TRT] [I] Engine generation completed in 67.0057 seconds.\n",
      "[12/17/2024-11:46:25] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 64 MiB, GPU 33708 MiB\n",
      "[12/17/2024-11:46:26] [TRT] [I] Total Host Persistent Memory: 88544\n",
      "[12/17/2024-11:46:26] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[12/17/2024-11:46:26] [TRT] [I] Total Scratch Memory: 419430912\n",
      "[12/17/2024-11:46:26] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 494 steps to complete.\n",
      "[12/17/2024-11:46:26] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 20.4455ms to assign 18 blocks to 494 nodes requiring 687873024 bytes.\n",
      "[12/17/2024-11:46:26] [TRT] [I] Total Activation Memory: 687871488\n",
      "[12/17/2024-11:46:30] [TRT] [I] Total Weights Memory: 36395287296\n",
      "[12/17/2024-11:46:30] [TRT] [I] Engine generation completed in 71.9117 seconds.\n",
      "[12/17/2024-11:46:30] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 64 MiB, GPU 34710 MiB\n",
      "[12/17/2024-11:46:32] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 74368 MiB\n",
      "[12/17/2024-11:46:32] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:01:13\n",
      "[12/17/2024-11:46:32] [TRT-LLM] [I] Build phase peak memory: 74371.11 MB, children: 17.49 MB\n",
      "[12/17/2024-11:46:32] [TRT-LLM] [I] Serializing engine to models/llama-3.3/70B/trt_llm/tp2_pp2/rank2.engine...\n",
      "[12/17/2024-11:46:41] [TRT-LLM] [I] Engine serialized. Total time: 00:00:29\n",
      "[12/17/2024-11:46:41] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 76237 MiB\n",
      "[12/17/2024-11:46:41] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:01:23\n",
      "[12/17/2024-11:46:41] [TRT] [I] Serialized 1210 bytes of code generator cache.\n",
      "[12/17/2024-11:46:41] [TRT] [I] Serialized 451957 bytes of compilation cache.\n",
      "[12/17/2024-11:46:41] [TRT] [I] Serialized 8 timing cache entries\n",
      "[12/17/2024-11:46:41] [TRT-LLM] [I] Timing cache serialized to model.cache\n",
      "[12/17/2024-11:46:41] [TRT-LLM] [I] Build phase peak memory: 76240.17 MB, children: 17.34 MB\n",
      "[12/17/2024-11:46:42] [TRT-LLM] [I] Serializing engine to models/llama-3.3/70B/trt_llm/tp2_pp2/rank0.engine...\n",
      "[12/17/2024-11:46:43] [TRT-LLM] [I] Engine serialized. Total time: 00:00:22\n",
      "[12/17/2024-11:46:57] [TRT-LLM] [I] Engine serialized. Total time: 00:00:25\n",
      "[12/17/2024-11:47:06] [TRT-LLM] [I] Engine serialized. Total time: 00:00:24\n",
      "[12/17/2024-11:47:10] [TRT-LLM] [I] Total time of building all engines: 00:02:05\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "TRT_CKPT=\"models/llama-3.3/70B/trt_ckpt/tp2_pp2\"\n",
    "TRT_ENGINE=\"models/llama-3.3/70B/trt_llm/tp2_pp2\"\n",
    "\n",
    "trtllm-build --checkpoint_dir \"$TRT_CKPT\" \\\n",
    "      --output_dir \"$TRT_ENGINE\" \\\n",
    "      --max_num_tokens 4096 \\\n",
    "      --max_input_len 255000 \\\n",
    "      --max_seq_len 256000 \\\n",
    "      --use_paged_context_fmha enable \\\n",
    "      --workers 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15bf2f1-750f-4d4c-bff0-e4dd39a540c9",
   "metadata": {},
   "source": [
    "## Local Testing of TensorRT-Optimized model\n",
    "\n",
    "The following Bash script performs a local test of the optimized Llama-3.3-70B-Instruct model. It sets the necessary environment variables and runs the `run.py` script with a sample prompt to evaluate the model's inference performance.\n",
    "\n",
    "If you get an error in the cell below, update the Transformer library:\n",
    "```bash\n",
    "pip install -U Transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01eb42e3-6dc6-4692-a11e-d887dd8321ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.12.0.dev2024080600\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.12.0.dev2024080600\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.12.0.dev2024080600\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.12.0.dev2024080600\n",
      "[TensorRT-LLM][INFO] Engine version 0.12.0.dev2024080600 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Engine version 0.12.0.dev2024080600 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] MPI size: 4, MPI local size: 4, rank: 1\n",
      "[TensorRT-LLM][INFO] Engine version 0.12.0.dev2024080600 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Engine version 0.12.0.dev2024080600 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Engine version 0.12.0.dev2024080600 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] MPI size: 4, MPI local size: 4, rank: 3\n",
      "[TensorRT-LLM][INFO] Engine version 0.12.0.dev2024080600 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Engine version 0.12.0.dev2024080600 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Engine version 0.12.0.dev2024080600 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] MPI size: 4, MPI local size: 4, rank: 2\n",
      "[TensorRT-LLM][INFO] Engine version 0.12.0.dev2024080600 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Engine version 0.12.0.dev2024080600 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Engine version 0.12.0.dev2024080600 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] MPI size: 4, MPI local size: 4, rank: 0\n",
      "[TensorRT-LLM][INFO] Engine version 0.12.0.dev2024080600 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] MPI size: 4, MPI local size: 4, rank: 0\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] MPI size: 4, MPI local size: 4, rank: 2\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] MPI size: 4, MPI local size: 4, rank: 3\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] MPI size: 4, MPI local size: 4, rank: 1\n",
      "[TensorRT-LLM][INFO] Rank 1 is using GPU 1\n",
      "[TensorRT-LLM][INFO] Rank 3 is using GPU 3\n",
      "[TensorRT-LLM][INFO] Rank 0 is using GPU 0\n",
      "[TensorRT-LLM][INFO] Rank 2 is using GPU 2\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 512\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 256\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 256000\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: 256000\n",
      "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 4096\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 4096 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n",
      "[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n",
      "[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n",
      "[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 512\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 256\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 256000\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: 256000\n",
      "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 4096\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 4096 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n",
      "[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n",
      "[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n",
      "[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 512\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 256\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 256000\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: 256000\n",
      "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 4096\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 4096 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n",
      "[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n",
      "[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n",
      "[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 512\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 256\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 256000\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: 256000\n",
      "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 4096\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 4096 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n",
      "[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n",
      "[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n",
      "[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n",
      "[TensorRT-LLM][INFO] Loaded engine size: 33727 MiB\n",
      "[TensorRT-LLM][INFO] Loaded engine size: 33727 MiB\n",
      "[TensorRT-LLM][INFO] Loaded engine size: 34728 MiB\n",
      "[TensorRT-LLM][INFO] Loaded engine size: 34728 MiB\n",
      "[TensorRT-LLM][INFO] Detecting local TP group for rank 1\n",
      "[TensorRT-LLM][INFO] Detecting local TP group for rank 0\n",
      "[TensorRT-LLM][INFO] TP group is intra-node for rank 0\n",
      "[TensorRT-LLM][INFO] TP group is intra-node for rank 1\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 656.01 MiB for execution context memory.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 656.01 MiB for execution context memory.\n",
      "[TensorRT-LLM][INFO] Detecting local TP group for rank 3\n",
      "[TensorRT-LLM][INFO] Detecting local TP group for rank 2\n",
      "[TensorRT-LLM][INFO] TP group is intra-node for rank 2\n",
      "[TensorRT-LLM][INFO] TP group is intra-node for rank 3\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 656.01 MiB for execution context memory.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 656.01 MiB for execution context memory.\n",
      "[TensorRT-LLM][INFO] [MS] Running engine with multi stream info\n",
      "[TensorRT-LLM][INFO] [MS] Number of aux streams is 1\n",
      "[TensorRT-LLM][INFO] [MS] Number of total worker streams is 2\n",
      "[TensorRT-LLM][INFO] [MS] The main stream provided by execute/enqueue calls is the first worker stream\n",
      "[TensorRT-LLM][INFO] [MS] Running engine with multi stream info\n",
      "[TensorRT-LLM][INFO] [MS] Number of aux streams is 1\n",
      "[TensorRT-LLM][INFO] [MS] Number of total worker streams is 2\n",
      "[TensorRT-LLM][INFO] [MS] The main stream provided by execute/enqueue calls is the first worker stream\n",
      "[TensorRT-LLM][INFO] [MS] Running engine with multi stream info\n",
      "[TensorRT-LLM][INFO] [MS] Number of aux streams is 1\n",
      "[TensorRT-LLM][INFO] [MS] Number of total worker streams is 2\n",
      "[TensorRT-LLM][INFO] [MS] The main stream provided by execute/enqueue calls is the first worker stream\n",
      "[TensorRT-LLM][INFO] [MS] Running engine with multi stream info\n",
      "[TensorRT-LLM][INFO] [MS] Number of aux streams is 1\n",
      "[TensorRT-LLM][INFO] [MS] Number of total worker streams is 2\n",
      "[TensorRT-LLM][INFO] [MS] The main stream provided by execute/enqueue calls is the first worker stream\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 34709 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 33707 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 34709 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 33707 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 3.05 GB GPU memory for runtime buffers.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 0.00 B GPU memory for decoder.\n",
      "[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 79.10 GiB, available: 39.93 GiB\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 3.30 GB GPU memory for runtime buffers.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 3.30 GB GPU memory for runtime buffers.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 3.05 GB GPU memory for runtime buffers.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 0.00 B GPU memory for decoder.\n",
      "[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 79.10 GiB, available: 39.93 GiB\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 3.21 GB GPU memory for decoder.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 3.21 GB GPU memory for decoder.\n",
      "[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 79.10 GiB, available: 37.45 GiB\n",
      "[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 79.10 GiB, available: 37.45 GiB\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 6904\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 6904\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 6904\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 6904\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n",
      "[TensorRT-LLM][INFO] Max KV cache pages per sequence: 4000\n",
      "[TensorRT-LLM][INFO] Max KV cache pages per sequence: 4000\n",
      "[TensorRT-LLM][INFO] Max KV cache pages per sequence: 4000\n",
      "[TensorRT-LLM][INFO] Max KV cache pages per sequence: 4000\n",
      "[TensorRT-LLM][INFO] Number of tokens per block: 64.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 33.71 GiB for max tokens in paged KV cache (441856).\n",
      "[TensorRT-LLM][INFO] Number of tokens per block: 64.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 33.71 GiB for max tokens in paged KV cache (441856).\n",
      "[TensorRT-LLM][INFO] Number of tokens per block: 64.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 33.71 GiB for max tokens in paged KV cache (441856).\n",
      "[TensorRT-LLM][INFO] Number of tokens per block: 64.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 33.71 GiB for max tokens in paged KV cache (441856).\n",
      "[12/17/2024-11:47:42] [TRT-LLM] [I] Load engine takes: 21.333313703536987 sec\n",
      "[12/17/2024-11:47:42] [TRT-LLM] [I] Load engine takes: 21.334816932678223 sec\n",
      "[12/17/2024-11:47:42] [TRT-LLM] [I] Load engine takes: 21.330867767333984 sec\n",
      "[12/17/2024-11:47:42] [TRT-LLM] [I] Load engine takes: 21.332868576049805 sec\n",
      "Input [Text 0]: \"<|begin_of_text|>The capital of Indonesia is\"\n",
      "Output [Text 0 Beam 0]: \" Jakarta is a city of Indonesia, Jakarta, which\"\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HF_MODEL=\"models/llama-3.3/70B/hf\"\n",
    "TRT_ENGINE=\"models/llama-3.3/70B/trt_llm/tp2_pp2\"\n",
    "\n",
    "PROMPT=\"The capital of Indonesia is\"\n",
    "\n",
    "mpirun -n 4 python ~/run.py \\\n",
    "    --max_output_len=10 \\\n",
    "    --tokenizer_dir $HF_MODEL \\\n",
    "    --engine_dir $TRT_ENGINE \\\n",
    "    --input_text \"$PROMPT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba3398-107e-4856-afc2-113070a1e808",
   "metadata": {},
   "source": [
    "## Deploying Triton with Inflight Batching\n",
    "\n",
    "The following Bash scripts set up and configure Triton Inference Server for the Llama-3.3-70B-Instruct model using inflight batching. This deployment optimizes inference performance by managing batch sizes and instance counts effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb947618-8ac0-4f98-b2ff-e2400cf28120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble\n",
      "postprocessing\n",
      "preprocessing\n",
      "tensorrt_llm\n",
      "tensorrt_llm_bls\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "TRTITON_REPO=\"models/llama-3.3/70B/triton\"\n",
    "mkdir -p \"$TRTITON_REPO\"\n",
    "\n",
    "cp -r ~/all_models/inflight_batcher_llm/* \"$TRTITON_REPO\"\n",
    "\n",
    "ls \"$TRTITON_REPO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "897a7902-effd-4235-a1a8-0dec875b9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ENGINE_DIR=\"models/llama-3.3/70B/trt_llm/tp2_pp2\"\n",
    "TOKENIZER_DIR=\"models/llama-3.3/70B/hf\"\n",
    "MODEL_FOLDER=\"models/llama-3.3/70B/triton\"\n",
    "TRITON_MAX_BATCH_SIZE=4\n",
    "INSTANCE_COUNT=4\n",
    "MAX_QUEUE_DELAY_MS=0\n",
    "MAX_QUEUE_SIZE=0\n",
    "FILL_TEMPLATE_SCRIPT=\"$HOME/tools/fill_template.py\"\n",
    "DECOUPLED_MODE=false\n",
    "\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/ensemble/config.pbtxt triton_max_batch_size:${TRITON_MAX_BATCH_SIZE}\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/preprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},preprocessing_instance_count:${INSTANCE_COUNT}\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm/config.pbtxt triton_backend:tensorrtllm,triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},engine_dir:${ENGINE_DIR},max_queue_delay_microseconds:${MAX_QUEUE_DELAY_MS},batching_strategy:inflight_fused_batching,max_queue_size:${MAX_QUEUE_SIZE}\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/postprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},postprocessing_instance_count:${INSTANCE_COUNT},max_queue_size:${MAX_QUEUE_SIZE}\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},bls_instance_count:${INSTANCE_COUNT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87bb0716-5424-41e4-a5da-78d043fc7e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Terminated\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Start the Triton server\n",
    "\n",
    "MODEL_FOLDER=\"models/llama-3.3/70B/triton\"\n",
    "stop_tritonserver\n",
    "nohup mpirun -np 4 tritonserver --model-repository=$MODEL_FOLDER &> /work/triton-server-log.txt &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45e251-b116-4b5e-80ba-4a183eb98914",
   "metadata": {},
   "source": [
    "## Testing the Triton Inference Server\n",
    "\n",
    "The following Bash commands verify that the Triton server and the deployed Llama-3.3-70B-Instruct model are running correctly. The first command checks the repository status, and the second sends a sample generation request to the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13df9769-4297-4417-b23e-184bb4565e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Inference Server to start...\n",
      "Triton not ready yet. Retrying in 5 seconds...\n",
      "Triton not ready yet. Retrying in 5 seconds...\n",
      "Triton not ready yet. Retrying in 5 seconds...\n",
      "Triton not ready yet. Retrying in 5 seconds...\n",
      "Triton not ready yet. Retrying in 5 seconds...\n",
      "Triton not ready yet. Retrying in 5 seconds...\n",
      "Triton not ready yet. Retrying in 5 seconds...\n",
      "Triton Inference Server is ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   289  100   274  100    15   269k  15120 --:--:-- --:--:-- --:--:--  282k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"ensemble\",\n",
      "  \"version\": \"1\",\n",
      "  \"state\": \"READY\"\n",
      "}\n",
      "{\n",
      "  \"name\": \"postprocessing\",\n",
      "  \"version\": \"1\",\n",
      "  \"state\": \"READY\"\n",
      "}\n",
      "{\n",
      "  \"name\": \"preprocessing\",\n",
      "  \"version\": \"1\",\n",
      "  \"state\": \"READY\"\n",
      "}\n",
      "{\n",
      "  \"name\": \"tensorrt_llm\",\n",
      "  \"version\": \"1\",\n",
      "  \"state\": \"READY\"\n",
      "}\n",
      "{\n",
      "  \"name\": \"tensorrt_llm_bls\",\n",
      "  \"version\": \"1\",\n",
      "  \"state\": \"READY\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   625    0   543  100    82    124     18  0:00:04  0:00:04 --:--:--   143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is ML? Machine Learning Analytics?\n",
      "Machine learning is a subset of Machine learning is a subset of AI that involves training algorithms, which enables systems using data analysis of data to learn from data and improve their performance on their performance on some task, can be able to improve\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "LOG_FILE=\"/work/triton-server-log.txt\"\n",
    "\n",
    "# Function to wait for Triton to start by monitoring the log file\n",
    "wait_for_triton_start() {\n",
    "    echo \"Waiting for Triton Inference Server to start...\"\n",
    "    while true; do\n",
    "        # Check for all required startup messages\n",
    "        if grep -q 'Started GRPCInferenceService at 0.0.0.0:8001' \"$LOG_FILE\" &&\n",
    "           grep -q 'Started HTTPService at 0.0.0.0:8000' \"$LOG_FILE\" &&\n",
    "           grep -q 'Started Metrics Service at 0.0.0.0:8002' \"$LOG_FILE\"; then\n",
    "                echo \"Triton Inference Server is ready.\"\n",
    "                break\n",
    "                \n",
    "        else\n",
    "            echo \"Triton not ready yet. Retrying in 5 seconds...\"\n",
    "            sleep 5   \n",
    "        fi\n",
    "    done\n",
    "}\n",
    "\n",
    "# Wait for Triton to start\n",
    "wait_for_triton_start\n",
    "\n",
    "curl -X POST http://localhost:8000/v2/repository/index -H \"Content-Type: application/json\" -d '{\"ready\": true}'| jq '.[] | select(.state == \"READY\")'\n",
    "\n",
    "curl -X POST localhost:8000/v2/models/ensemble/generate -d '{\"text_input\": \"What is ML?\", \"max_tokens\": 50, \"bad_words\": \"\", \"stop_words\": \"\"}' | jq -r '.text_output'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e523f-8fc2-4d59-a987-9ed0544e32a7",
   "metadata": {},
   "source": [
    "## Performance Profiling with `genai-perf`\n",
    "\n",
    "To evaluate the performance of the deployed Llama-3.3-70B-Instruct model on Triton Inference Server, execute the following Bash commands in a terminal session within Jupyter. This script uses [GenAI-Perf](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/perf_analyzer/genai-perf/README.html) to profile the model, generating performance metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57a1db57-d2c4-4e27-99ec-742236f24593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 11:51 [INFO] genai_perf.parser:90 - Profiling these models: ensemble\n",
      "2024-12-17 11:51 [INFO] genai_perf.wrapper:147 - Running Perf Analyzer : 'perf_analyzer -m ensemble --async --input-data artifacts/ensemble-triton-tensorrtllm-concurrency1/llm_inputs.json -i grpc --streaming -u localhost:8001 --shape max_tokens:1 --shape text_input:1 --concurrency-range 1 --service-kind triton --measurement-interval 10000 --stability-percentage 999 --profile-export-file artifacts/ensemble-triton-tensorrtllm-concurrency1/profile_export.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Pass contained only one request, so sample latency standard deviation will be infinity (UINT64_MAX).\n",
      "WARNING: Pass contained only one request, so sample latency standard deviation will be infinity (UINT64_MAX).\n",
      "WARNING: Pass contained only one request, so sample latency standard deviation will be infinity (UINT64_MAX).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[3m                                  LLM Metrics                                   \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m             Statistic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   avg\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   min\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   max\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   p99\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   p90\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   p75\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m  Request latency (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m12,03…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m12,03…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m12,04…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m12,04…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m12,04…\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m12,03…\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36mOutput sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m806.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m806.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m806.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m806.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m806.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m806.00\u001b[0m\u001b[32m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m Input sequence length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m550.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m550.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m550.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m550.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m550.00\u001b[0m\u001b[32m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m550.00\u001b[0m\u001b[32m \u001b[0m│\n",
      "└────────────────────────┴────────┴────────┴────────┴────────┴────────┴────────┘\n",
      "Output token throughput (per sec): 66.96\n",
      "Request throughput (per sec): 0.08\n",
      "2024-12-17 11:52 [INFO] genai_perf.export_data.json_exporter:58 - Generating artifacts/ensemble-triton-tensorrtllm-concurrency1/profile_export_genai_perf.json\n",
      "2024-12-17 11:52 [INFO] genai_perf.export_data.csv_exporter:69 - Generating artifacts/ensemble-triton-tensorrtllm-concurrency1/profile_export_genai_perf.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 6: --service-kind: command not found\n",
      "bash: line 8: --backend: command not found\n",
      "bash: line 10: --num-prompts: command not found\n",
      "bash: line 12: --random-seed: command not found\n",
      "bash: line 14: --synthetic-input-tokens-mean: command not found\n",
      "bash: line 16: --synthetic-input-tokens-stddev: command not found\n",
      "bash: line 18: --output-tokens-mean: command not found\n",
      "bash: line 20: --output-tokens-stddev: command not found\n",
      "bash: line 23: --output-tokens-mean-deterministic: command not found\n",
      "bash: line 24: --tokenizer: command not found\n",
      "bash: line 26: --concurrency: command not found\n",
      "bash: line 28: --measurement-interval: command not found\n",
      "bash: line 30: --profile-export-file: command not found\n",
      "bash: line 32: --url: command not found\n",
      "bash: line 34: --generate-plots: command not found\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\nTOKENIZER_DIR=\"models/llama-3.3/70B/hf\"\\n\\ngenai-perf profile -m ensemble \\\\\\n    # Specifies the use of Triton Inference Server.\\n    --service-kind triton \\\\\\n    # Utilizes the TensorRT backend optimized for large language models.\\n    --backend tensorrtllm \\\\\\n    # Number of prompts to use for the profiling test.\\n    --num-prompts 100 \\\\\\n    # Sets the seed for random number generation to ensure reproducibility.\\n    --random-seed 1234 \\\\\\n    # Defines the mean for synthetic input token lengths.\\n    --synthetic-input-tokens-mean 200 \\\\\\n    # Defines the standard deviation for synthetic input token lengths.\\n    --synthetic-input-tokens-stddev 0 \\\\\\n    # Defines the mean for synthetic output token lengths.\\n    --output-tokens-mean 100 \\\\\\n    # Defines the standard deviation for synthetic output token lengths.\\n    --output-tokens-stddev 0 \\\\\\n    # Ensures deterministic output token lengths.\\n    --output-tokens-mean-deterministic \\\\\\n    # Specifies the tokenizer configuration file.\\n    --tokenizer $TOKENIZER_DIR/tokenizer.json \\\\\\n    # Number of concurrent requests to simulate during profiling.\\n    --concurrency 500 \\\\\\n    # Time interval (in milliseconds) for measurements.\\n    --measurement-interval 4000 \\\\\\n    # File to export the profiling results.\\n    --profile-export-file model_profile.json \\\\\\n    # URL of the Triton Inference Server.\\n    --url localhost:8001 \\\\\\n    # Generates visual plots of the profiling results.\\n    --generate-plots\\n'' returned non-zero exit status 127.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mTOKENIZER_DIR=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/llama-3.3/70B/hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mgenai-perf profile -m ensemble \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Specifies the use of Triton Inference Server.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --service-kind triton \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Utilizes the TensorRT backend optimized for large language models.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --backend tensorrtllm \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Number of prompts to use for the profiling test.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --num-prompts 100 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Sets the seed for random number generation to ensure reproducibility.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --random-seed 1234 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Defines the mean for synthetic input token lengths.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --synthetic-input-tokens-mean 200 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Defines the standard deviation for synthetic input token lengths.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --synthetic-input-tokens-stddev 0 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Defines the mean for synthetic output token lengths.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --output-tokens-mean 100 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Defines the standard deviation for synthetic output token lengths.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --output-tokens-stddev 0 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Ensures deterministic output token lengths.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --output-tokens-mean-deterministic \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Specifies the tokenizer configuration file.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --tokenizer $TOKENIZER_DIR/tokenizer.json \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Number of concurrent requests to simulate during profiling.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --concurrency 500 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Time interval (in milliseconds) for measurements.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --measurement-interval 4000 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # File to export the profiling results.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --profile-export-file model_profile.json \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # URL of the Triton Inference Server.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --url localhost:8001 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Generates visual plots of the profiling results.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --generate-plots\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:155\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:315\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\nTOKENIZER_DIR=\"models/llama-3.3/70B/hf\"\\n\\ngenai-perf profile -m ensemble \\\\\\n    # Specifies the use of Triton Inference Server.\\n    --service-kind triton \\\\\\n    # Utilizes the TensorRT backend optimized for large language models.\\n    --backend tensorrtllm \\\\\\n    # Number of prompts to use for the profiling test.\\n    --num-prompts 100 \\\\\\n    # Sets the seed for random number generation to ensure reproducibility.\\n    --random-seed 1234 \\\\\\n    # Defines the mean for synthetic input token lengths.\\n    --synthetic-input-tokens-mean 200 \\\\\\n    # Defines the standard deviation for synthetic input token lengths.\\n    --synthetic-input-tokens-stddev 0 \\\\\\n    # Defines the mean for synthetic output token lengths.\\n    --output-tokens-mean 100 \\\\\\n    # Defines the standard deviation for synthetic output token lengths.\\n    --output-tokens-stddev 0 \\\\\\n    # Ensures deterministic output token lengths.\\n    --output-tokens-mean-deterministic \\\\\\n    # Specifies the tokenizer configuration file.\\n    --tokenizer $TOKENIZER_DIR/tokenizer.json \\\\\\n    # Number of concurrent requests to simulate during profiling.\\n    --concurrency 500 \\\\\\n    # Time interval (in milliseconds) for measurements.\\n    --measurement-interval 4000 \\\\\\n    # File to export the profiling results.\\n    --profile-export-file model_profile.json \\\\\\n    # URL of the Triton Inference Server.\\n    --url localhost:8001 \\\\\\n    # Generates visual plots of the profiling results.\\n    --generate-plots\\n'' returned non-zero exit status 127."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "TOKENIZER_DIR=\"models/llama-3.3/70B/hf\"\n",
    "\n",
    "genai-perf profile -m ensemble \\\n",
    "    # Specifies the use of Triton Inference Server.\n",
    "    --service-kind triton \\\n",
    "    # Utilizes the TensorRT backend optimized for large language models.\n",
    "    --backend tensorrtllm \\\n",
    "    # Number of prompts to use for the profiling test.\n",
    "    --num-prompts 100 \\\n",
    "    # Sets the seed for random number generation to ensure reproducibility.\n",
    "    --random-seed 1234 \\\n",
    "    # Defines the mean for synthetic input token lengths.\n",
    "    --synthetic-input-tokens-mean 200 \\\n",
    "    # Defines the standard deviation for synthetic input token lengths.\n",
    "    --synthetic-input-tokens-stddev 0 \\\n",
    "    # Defines the mean for synthetic output token lengths.\n",
    "    --output-tokens-mean 100 \\\n",
    "    # Defines the standard deviation for synthetic output token lengths.\n",
    "    --output-tokens-stddev 0 \\\n",
    "    # Ensures deterministic output token lengths.\n",
    "    --output-tokens-mean-deterministic \\\n",
    "    # Specifies the tokenizer configuration file.\n",
    "    --tokenizer $TOKENIZER_DIR/tokenizer.json \\\n",
    "    # Number of concurrent requests to simulate during profiling.\n",
    "    --concurrency 500 \\\n",
    "    # Time interval (in milliseconds) for measurements.\n",
    "    --measurement-interval 4000 \\\n",
    "    # File to export the profiling results.\n",
    "    --profile-export-file model_profile.json \\\n",
    "    # URL of the Triton Inference Server (gRPC API).\n",
    "    --url localhost:8001 \\\n",
    "    # Generates visual plots of the profiling results.\n",
    "    --generate-plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2adf3-cf85-41d0-9901-3b06642f29fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
