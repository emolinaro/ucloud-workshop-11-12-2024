{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f74ada-05b3-43da-b6de-c7e12745c66a",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"https://escience.sdu.dk/index.php/ucloud/\">\n",
    "    <img src=\"https://escience.sdu.dk/wp-content/uploads/2020/03/logo_esc.svg\" width=\"400\" height=\"186\" />\n",
    "  </a>\n",
    "</center>\n",
    "<br>\n",
    "<p style=\"font-size: 1.2em;\">\n",
    "  This notebook was tested using <strong>NeMo Framework v24.07</strong> and machine type <code>u3-gpu4</code> on UCloud.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3323204-1463-4df3-8c75-5e95b6d66ba1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Building a Llama-3.1 LoRA Adapter with the NeMo Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3d632-44a0-4e6c-9229-b70bbcff1e99",
   "metadata": {},
   "source": [
    "This notebook showcases performing LoRA PEFT [**Llama 3.1 70B**](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct) on [PubMedQA](https://pubmedqa.github.io/) using NeMo Framework. PubMedQA is a Question-Answering dataset for biomedical texts.\n",
    "\n",
    "In this notebook, we demonstrate how to apply Low-Rank Adaptation (LoRA) Parameter-Efficient Fine-Tuning (PEFT) techniques to the Llama 3.3 70B model using the NeMo Framework. We use [PubMedQA](https://pubmedqa.github.io/), a specialized question-answering dataset derived from biomedical literature, to illustrate how LoRA adapters can efficiently enhance model performance within a domain-specific context.\n",
    "\n",
    "**Disclaimer**: This notebook is adapted from the [NVIDIA NeMo tutorial on biomedical QA with Llama-3](https://github.com/NVIDIA/NeMo/blob/main/tutorials/llm/llama-3/biomedical-qa/llama3-lora-nemofw.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee38a79-b107-494e-b8e7-3d1f6d26b412",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Estimating GPU Memory Requirements for Serving LLMs\n",
    "\n",
    "\n",
    "### **1. Model Size**\n",
    "Before you begin, it’s essential to understand how much GPU memory you’ll need to serve a large language model (LLM). A commonly used formula is:\n",
    "\n",
    "$$\n",
    "M_{\\text{model}} = \\frac{(P \\times 4B)}{(32 / Q)}\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- **M**: The GPU memory required (in Gigabytes)  \n",
    "- **P**: The number of parameters in the model (e.g., 7 billion parameters for a 7B model)  \n",
    "- **4B**: 4 bytes, representing the size of each parameter at full precision (32 bits)  \n",
    "- **32**: The number of bits in 4 bytes (32 bits)  \n",
    "- **Q**: The model precision in bits used during serving (e.g., 16 bits, 8 bits, or 4 bits)  \n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Start with $P \\times 4B$ to get the base memory needed for all parameters at full precision (FP32).\n",
    "- Divide by $(32/Q)$, which scales the memory requirement according to the lower-precision format you’re using. For example, loading a model in 16-bit precision effectively halves the memory usage compared to 32-bit.\n",
    "\n",
    "#### **Example:**\n",
    "\n",
    "For a 70B parameter model loaded in 8-bit precision:\n",
    "\n",
    "- $P = 70 \\times 10^9$ ($70$ billion)\n",
    "- $Q = 8$\n",
    "\n",
    "Plugging these in:\n",
    "\n",
    "$$\n",
    "M_{\\text{model}} = \\frac{(70 \\times 10^9 \\times 4B)}{(32 / 8)} \n",
    "= \\frac{(280 \\times 10^9 B)}{2} \n",
    "= 70 \\times 10^9 B\n",
    "$$\n",
    "\n",
    "Convert bytes to gigabytes (1 GB = $10^9$ bytes):\n",
    "\n",
    "$$\n",
    "M = 70 \\text{ GB}\n",
    "$$\n",
    "\n",
    "This rough calculation helps estimate the GPU memory needed for serving large models, ensuring you have the right hardware configuration before starting fine-tuning or inference steps.\n",
    "\n",
    "### **2. Context Window**\n",
    "\n",
    "The **context window** refers to the maximum number of tokens (words or subwords) the model can process in a single inference pass. During inference, the model needs to store activations for each token in the input sequence. This storage requirement scales linearly with the length of the context window.\n",
    "\n",
    "#### **Memory Calculation for Context Window**\n",
    "\n",
    "$$\n",
    "M_{\\text{context}} = L \\times H \\times D \\times N\n",
    "$$\n",
    "\n",
    "- **$M_{\\text{context}}$**: Memory required for the context window (in Gigabytes)\n",
    "- **$L$**: Length of the context window (number of tokens)\n",
    "- **$H$**: Hidden size (dimensionality of the model's hidden layers)\n",
    "- **$D$**: Data type size (bytes per element, e.g., 2 for FP16)\n",
    "- **$N$**: Number of transformer layers\n",
    "\n",
    "#### **Example:**\n",
    "\n",
    "Assume:\n",
    "- **$L = 1024$** tokens\n",
    "- **$H = 8192$** dimensions\n",
    "- **$D = 1$** bytes (for INT8 precision)\n",
    "- **$N = 80$** number of hidden layers\n",
    "\n",
    "$$\n",
    "M_{\\text{context}} = 1024 \\times 8192 \\times 1 \\times 80 = 671,088,640 \\text{ bytes} \\approx 671 \\text{ MB}\n",
    "$$\n",
    "\n",
    "### **3. Batch Size**\n",
    "\n",
    "**Batch size** determines how many input sequences the model processes simultaneously. Increasing the batch size can lead to higher GPU memory usage because the model needs to store activations for each sequence in the batch.\n",
    "\n",
    "#### **Memory Calculation for Batch Size**\n",
    "\n",
    "$$\n",
    "M_{\\text{batch}} = B \\times M_{\\text{context}}\n",
    "$$\n",
    "\n",
    "- **$M_{\\text{batch}}$**: Additional memory required for batching (in Gigabytes)\n",
    "- **$B$**: Batch size (number of sequences)\n",
    "- **$M_{\\text{context}}$**: Memory per sequence (from context window calculation)\n",
    "\n",
    "#### **Example:**\n",
    "\n",
    "Using the previous **$M_{\\text{context}} =  671 \\text{ MB}$** and a **batch size $B = 8$**:\n",
    "\n",
    "$$\n",
    "M_{\\text{batch}} = 8 \\times  671 \\text{ MB} = 5.4 \\text{ GB}\n",
    "$$\n",
    "\n",
    "### **4. Total Inference Memory Estimation**\n",
    "\n",
    "Combining all these factors gives a more comprehensive estimate of the GPU memory required for inference:\n",
    "\n",
    "$$\n",
    "M_{\\text{total}} = M_{\\text{model}} + M_{\\text{context}} \\times B + M_{\\text{overhead}}\n",
    "$$\n",
    "\n",
    "- **$M_{\\text{total}}$**: Total GPU memory required (in Gigabytes)\n",
    "- **$M_{\\text{model}}$**: Memory for the model\n",
    "- **$M_{\\text{context}}$**: Memory per token sequence\n",
    "- **$B$**: Batch size\n",
    "- **$M_{\\text{overhead}}$**: Additional overhead for operations like caching, temporary buffers, etc. (typically 10-20%)\n",
    "\n",
    "#### Example\n",
    "\n",
    "Using the previous results:\n",
    "\n",
    "$$\n",
    "M_{\\text{total}} \\approx 90 \\text{ GB}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f729b57-6b66-46ff-ae26-8f0e7f80428b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated GPU Memory Required for LLama-3 8B: 21.78 GB\n",
      "Estimated GPU Memory Required for LLama-3 70B: 180.88 GB\n"
     ]
    }
   ],
   "source": [
    "from utils import estimate_gpu_memory\n",
    "\n",
    "Q = 16  # 16-bit precision (bfloat16)\n",
    "L = 1024  # Context window\n",
    "B = 8  # Batch size\n",
    "\n",
    "# Example usage for LLama-3.1 8B\n",
    "P_8B = 8_000_000_000  # 8B parameters\n",
    "H_8B = 4096  # Hidden size\n",
    "N_8B = 32\n",
    "\n",
    "estimated_memory_8B = estimate_gpu_memory(P_8B, Q, L, H_8B, B, N_8B)\n",
    "print(f\"Estimated GPU Memory Required for LLama-3 8B: {estimated_memory_8B:.2f} GB\")\n",
    "\n",
    "# Example usage for LLama-3.1 70B\n",
    "P_70B = 70_000_000_000  # 70B parameters\n",
    "H_70B = 8192  # Hidden size\n",
    "N_70B = 80\n",
    "\n",
    "estimated_memory_70B = estimate_gpu_memory(P_70B, Q, L, H_70B, B, N_70B)\n",
    "print(f\"Estimated GPU Memory Required for LLama-3 70B: {estimated_memory_70B:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b285d5a-d838-423b-9d6c-65add61f48ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download the Model\n",
    "Before you begin, ensure you have a local copy of the Meta Llama3.3 70B Instruct model. If you haven’t already downloaded it, you can obtain it from the official [Hugging Face repository](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/tree/main). This step is crucial to ensure that all subsequent operations in the notebook run smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93614eb2-8f1f-49c0-a5bd-c455b4549f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002ed728f37d41c3b18f4c6f3fb4f3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Password(description='Hugging Face Token:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Password\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "pwd = Password(description=\"Hugging Face Token:\")\n",
    "display(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec7be368-f5db-4c34-87ec-00f574cd8ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6866f26e8a7e4dd8996529de4df47bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3711 files:   0%|          | 0/3711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/work/ucloud-workshop-11-12-2024/models/llama-3.1-nemotron/70B/hf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = pwd.value\n",
    "hf_model=\"nvidia/Llama-3.1-Nemotron-70B-Instruct\"\n",
    "hf_model_path=\"models/llama-3.1-nemotron/70B/hf\"\n",
    "snapshot_download(\n",
    "    repo_id=hf_model,\n",
    "    local_dir=hf_model_path,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3057e525-7957-45c0-bedc-c347d4811081",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\n",
      "model_config.yaml\n",
      "model_weights\n",
      "132G\tmodels/llama-3.1-nemotron/70B/hf\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$hf_model_path\"\n",
    "\n",
    "ls $1\n",
    "du -sh $1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba9d09-2412-404e-9bd9-45e67724a46c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert the Model in NeMo Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e25ab64-a613-4d9d-b2f4-0cc9a31f7e16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing contents of models/llama-3.1-nemotron/70B/hf:\n",
      "total 16\n",
      "-rw-r--r--. 1 ucloud ucloud 9770 Dec 10 11:21 README.md\n",
      "-rw-r--r--. 1 ucloud ucloud 2936 Dec 10 11:21 model_config.yaml\n",
      "drwxr-xr-x. 1 ucloud ucloud    0 Dec 10 11:32 model_weights\n",
      "NeMo archive already exists at models/llama-3.1-nemotron/70B/nemo/Llama-3_1-Nemotron-70B-Instruct.nemo. Skipping creation.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Define paths\n",
    "HF_MODEL=\"models/llama-3.1-nemotron/70B/hf\"\n",
    "NeMo_MODEL=\"models/llama-3.1-nemotron/70B/nemo/Llama-3_1-Nemotron-70B-Instruct.nemo\"\n",
    "\n",
    "# List the contents of the Hugging Face model directory\n",
    "echo \"Listing contents of $HF_MODEL:\"\n",
    "ls -l \"$HF_MODEL\"\n",
    "\n",
    "# Check if the NeMo_MODEL archive already exists\n",
    "if [ ! -f \"$NeMo_MODEL\" ]; then\n",
    "    echo \"NeMo archive not found. Creating archive: $NeMo_MODEL\"\n",
    "    \n",
    "    # Ensure the destination directory exists\n",
    "    mkdir -p \"$(dirname \"$NeMo_MODEL\")\"\n",
    "    \n",
    "    # Create the .nemo archive using tar\n",
    "    tar cf \"$NeMo_MODEL\" \"$HF_MODEL/model_config.yaml\" \"$HF_MODEL/model_weights\"\n",
    "    \n",
    "    if [ $? -eq 0 ]; then\n",
    "        echo \"NeMo archive created successfully at $NeMo_MODEL.\"\n",
    "    else\n",
    "        echo \"Error: Failed to create NeMo archive.\"\n",
    "        exit 1\n",
    "    fi\n",
    "else\n",
    "    echo \"NeMo archive already exists at $NeMo_MODEL. Skipping creation.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd2e10e9-5c2f-4f97-b145-40b7d0c524e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/llama-3.1-nemotron/70B/nemo/Llama-3_1-Nemotron-70B-Instruct.nemo: POSIX tar archive (GNU)\n",
      "132G\tmodels/llama-3.1-nemotron/70B/nemo/Llama-3_1-Nemotron-70B-Instruct.nemo\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "NeMo_MODEL=\"models/llama-3.1-nemotron/70B/nemo/Llama-3_1-Nemotron-70B-Instruct.nemo\"\n",
    "\n",
    "file \"$NeMo_MODEL\"\n",
    "du -sh \"$NeMo_MODEL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6a910-a05e-4ae1-aac4-56e5092be2b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Step-by-Step Instructions\n",
    "\n",
    "This notebook is organized into four main steps:\n",
    "\n",
    "1. **Prepare the Dataset:**\n",
    "   Load and preprocess the PubMedQA dataset, ensuring that it’s correctly formatted and ready for fine-tuning.\n",
    "\n",
    "2. **Run the PEFT Fine-Tuning Script:**\n",
    "   Apply Low-Rank Adaptation (LoRA) Parameter-Efficient Fine-Tuning methods to tailor the Llama 3.3 70B model to the PubMedQA domain.\n",
    "\n",
    "3. **Perform Inference with the NeMo Framework:**\n",
    "   Use the trained model to generate answers to biomedical questions and observe how it performs on real queries.\n",
    "\n",
    "4. **Evaluate Model Accuracy:**\n",
    "   Assess the quality and correctness of the model’s responses to measure improvements gained through the fine-tuning process.\n",
    "   \n",
    "5. **Export Model to TensorRT-LLM Format for Inference:**\n",
    "   use the APIs in the export module to export a NeMo checkpoint to TensorRT-LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5bd31",
   "metadata": {},
   "source": [
    "### Step 1: Prepare the dataset\n",
    "\n",
    "Download the PubMedQA dataset and run the pre-processing script in the cloned directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944b43c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pubmedqa' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Download the dataset and prep. scripts\n",
    "git clone https://github.com/pubmedqa/pubmedqa.git\n",
    "\n",
    "# split it into train/val/test datasets\n",
    "cd pubmedqa/preprocess\n",
    "python split_dataset.py pqal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025b2d4",
   "metadata": {},
   "source": [
    "The following example shows what a single row looks inside of the PubMedQA train, validation and test splits.\n",
    "\n",
    "```json\n",
    "\"18251357\": {\n",
    "    \"QUESTION\": \"Does histologic chorioamnionitis correspond to clinical chorioamnionitis?\",\n",
    "    \"CONTEXTS\": [\n",
    "        \"To evaluate the degree to which histologic chorioamnionitis, a frequent finding in placentas submitted for histopathologic evaluation, correlates with clinical indicators of infection in the mother.\",\n",
    "        \"A retrospective review was performed on 52 cases with a histologic diagnosis of acute chorioamnionitis from 2,051 deliveries at University Hospital, Newark, from January 2003 to July 2003. Third-trimester placentas without histologic chorioamnionitis (n = 52) served as controls. Cases and controls were selected sequentially. Maternal medical records were reviewed for indicators of maternal infection.\",\n",
    "        \"Histologic chorioamnionitis was significantly associated with the usage of antibiotics (p = 0.0095) and a higher mean white blood cell count (p = 0.018). The presence of 1 or more clinical indicators was significantly associated with the presence of histologic chorioamnionitis (p = 0.019).\"\n",
    "    ],\n",
    "    \"reasoning_required_pred\": \"yes\",\n",
    "    \"reasoning_free_pred\": \"yes\",\n",
    "    \"final_decision\": \"yes\",\n",
    "    \"LONG_ANSWER\": \"Histologic chorioamnionitis is a reliable indicator of infection whether or not it is clinically apparent.\"\n",
    "},\n",
    "```\n",
    "\n",
    "Use the following code to convert the train, validation, and test PubMedQA data into the `JSONL` format that NeMo needs for PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90f69729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl(fname):\n",
    "    obj = []\n",
    "    with open(fname, 'rt') as f:\n",
    "        st = f.readline()\n",
    "        while st:\n",
    "            obj.append(json.loads(st))\n",
    "            st = f.readline()\n",
    "    return obj\n",
    "\n",
    "def write_jsonl(fname, json_objs):\n",
    "    with open(fname, 'wt') as f:\n",
    "        for o in json_objs:\n",
    "            f.write(json.dumps(o)+\"\\n\")\n",
    "            \n",
    "def form_question(obj):\n",
    "    st = \"\"    \n",
    "    for i, label in enumerate(obj['LABELS']):\n",
    "        st += f\"{label}: {obj['CONTEXTS'][i]}\\n\"\n",
    "    st += f\"QUESTION: {obj['QUESTION']}\\n\"\n",
    "    st += f\" ### ANSWER (yes|no|maybe): \"\n",
    "    return st\n",
    "\n",
    "def convert_to_jsonl(data_path, output_path):\n",
    "    data = json.load(open(data_path, 'rt'))\n",
    "    json_objs = []\n",
    "    for k in data.keys():\n",
    "        obj = data[k]\n",
    "        prompt = form_question(obj)\n",
    "        completion = obj['final_decision']\n",
    "        json_objs.append({\"input\": prompt, \"output\": f\"<<< {completion} >>>\"})\n",
    "    write_jsonl(output_path, json_objs)\n",
    "    return json_objs\n",
    "\n",
    "\n",
    "test_json_objs = convert_to_jsonl(\"pubmedqa/data/test_set.json\", \"pubmedqa/data/pubmedqa_test.jsonl\")\n",
    "train_json_objs = convert_to_jsonl(\"pubmedqa/data/pqal_fold0/train_set.json\", \"pubmedqa/data/pubmedqa_train.jsonl\")\n",
    "dev_json_objs = convert_to_jsonl(\"pubmedqa/data/pqal_fold0/dev_set.json\", \"pubmedqa/data/pubmedqa_val.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62777542",
   "metadata": {},
   "source": [
    "> `Note:` In the output, we enforce the inclusion of “<<<” and “>>>“ markers which would allow verification of the LoRA tuned model during inference. This is  because the base model can produce “yes” / “no” responses based on zero-shot templates as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddd0f2a",
   "metadata": {},
   "source": [
    "After running the above script, you will see  `pubmedqa_train.jsonl`, `pubmedqa_val.jsonl`, and `pubmedqa_test.jsonl` files appear in the data directory.\n",
    "\n",
    "This is what an example will be formatted like after the script has converted the PubMedQA data into `JSONL` -\n",
    "\n",
    "```json\n",
    "{\"input\": \"QUESTION: Failed IUD insertions in community practice: an under-recognized problem?\\nCONTEXT: The data analysis was conducted to describe the rate of unsuccessful copper T380A intrauterine device (IUD) insertions among women using the IUD for emergency contraception (EC) at community family planning clinics in Utah.\\n ...  ### ANSWER (yes|no|maybe): \",\n",
    "\"output\": \"<<< yes >>>\"}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef7c5d4e-fafb-40df-806d-7c366071d08d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 pubmedqa/data/pubmedqa_train.jsonl\n",
      "50 pubmedqa/data/pubmedqa_val.jsonl\n",
      "500 pubmedqa/data/pubmedqa_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# clear up cached mem-map file\n",
    "rm pubmedqa/data/*idx*\n",
    "\n",
    "wc -l pubmedqa/data/pubmedqa_train.jsonl\n",
    "wc -l pubmedqa/data/pubmedqa_val.jsonl\n",
    "wc -l pubmedqa/data/pubmedqa_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1d887",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Run PEFT finetuning script for LoRA\n",
    "\n",
    "NeMo framework includes a high level python script for fine-tuning  [megatron_gpt_finetuning.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py) that can abstract away some of the lower level API calls. Once you have your model downloaded and the dataset ready, LoRA fine-tuning with NeMo is essentially just running this script!\n",
    "\n",
    "For this demonstration, this training run is capped by `max_steps`, and validation is carried out every `val_check_interval` steps. If the validation loss does not improve after a few checks, training is halted to avoid overfitting.\n",
    "\n",
    "> `NOTE:` In the block of code below, pass the paths to your train, test and validation data files as well as path to the .nemo model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939482b9-40ce-4aac-876c-894a04fe0742",
   "metadata": {},
   "source": [
    "#### Understanding Global Batch Size (GBS) in Multi-GPU Training\n",
    "\n",
    "\n",
    "##### **1. Global Batch Size (GBS)**\n",
    "- **Definition:**\n",
    "  - The **total number of training samples** processed in **one training step** across **all GPUs** involved.\n",
    "\n",
    "##### **2. Data Parallelism (DP)**\n",
    "- **Definition:**\n",
    "  - The **number of GPUs** that each hold a **replica** of the entire model.\n",
    "  - **Function:** Distributes different data batches to each GPU simultaneously.\n",
    "  - **GAS (Gradient Accumulation Steps):** The number of mini-batches over which gradients are accumulated before performing a parameter update.\n",
    "  - **DP formula:**\n",
    "      $$\n",
    "      \\text{Data Parallelism (DP)} = \\frac{\\text{Total GPUs} \\times \\text{Gradient Accumulation Step (GAS)}}{\\text{Tensor Parallelism (TP)} \\times \\text{Pipeline Parallelism (PP)}}\n",
    "      $$\n",
    "\n",
    "\n",
    "##### **3. Micro Batch Size (MB)**\n",
    "- **Definition:**\n",
    "  - The **number of samples** processed **per GPU** in a single forward/backward pass.\n",
    "\n",
    "##### **4. GBS Formula**\n",
    "$$\n",
    "\\text{Global Batch Size (GBS)} = \\text{Data Parallelism (DP)} \\times \\text{Micro Batch Size (MB)}\n",
    "$$\n",
    "\n",
    "##### **5. How to Set GBS**\n",
    "1. **Determine Available GPUs:**\n",
    "   - Total GPUs (e.g., 4 GPUs).\n",
    "2. **Choose Data Parallelism (DP):**\n",
    "   - Decide how many GPUs to use for DP (e.g., DP = 4).\n",
    "3. **Set Micro Batch Size (MB):**\n",
    "   - Based on GPU memory capacity (e.g., MB = 8).\n",
    "4. **Calculate GBS:**\n",
    "   - Use the formula to find GBS (e.g., GBS = 4 × 8 = 32).\n",
    "\n",
    "##### **Best Practices**\n",
    "- **Align GBS with DP and MB:**\n",
    "  - Ensure $\\text{GBS} = \\text{DP} \\times \\text{MB}$.\n",
    "- **Monitor GPU Utilization:**\n",
    "  - Use tools like `nvidia-smi` to ensure all GPUs are effectively utilized.\n",
    "- **Adjust Batch Sizes as Needed:**\n",
    "  - Optimize **MB** based on memory constraints and **GBS** to balance load.\n",
    "- **Utilize Gradient Accumulation:**\n",
    "  - When larger **GBS** is desired but constrained by memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899a230d-51b2-467e-9ea2-aa59a23f08e6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ucloud/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-10 13:24:17,324] torch.distributed.run: [WARNING] \n",
      "[2024-12-10 13:24:17,324] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-12-10 13:24:17,324] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-12-10 13:24:17,324] torch.distributed.run: [WARNING] *****************************************\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "[NeMo W 2024-12-10 13:24:32 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 13:24:32 megatron_gpt_finetuning:56] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-12-10 13:24:32 megatron_gpt_finetuning:57] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 4\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: bf16-mixed\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 1000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 20\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: results/llama-3.1-nemotron/70B/bf16-mixed\n",
      "      exp_dir: results/llama-3.1-nemotron/70B/bf16-mixed\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: true\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 4\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 8\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: models/llama-3.1-nemotron/70B/nemo/Llama-3_1-Nemotron-70B-Instruct.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: true\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      fsdp: false\n",
      "      fsdp_sharding_strategy: full\n",
      "      fsdp_grad_reduce_dtype: fp32\n",
      "      fsdp_sharded_checkpoint: false\n",
      "      fsdp_use_orig_params: false\n",
      "      peft:\n",
      "        peft_scheme: lora\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          alpha: ${model.peft.lora_tuning.adapter_dim}\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "        selective_tuning:\n",
      "          tunable_base_param_names:\n",
      "          - self_attention\n",
      "          - word_embeddings\n",
      "      data:\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - pubmedqa/data/pubmedqa_train.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 10\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{input} {output}'\n",
      "          truncation_method: right\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - pubmedqa/data/pubmedqa_val.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 10\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 0.0\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "      mcore_gpt: true\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 13:24:32 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 13:24:32 exp_manager:396] ExpManager schema\n",
      "[NeMo I 2024-12-10 13:24:32 exp_manager:397] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2024-12-10 13:24:32 exp_manager:830] exp_manager received explicit_log_dir: results/llama-3.1-nemotron/70B/bf16-mixed and at least one of exp_dir: results/llama-3.1-nemotron/70B/bf16-mixed, or version: None. Please note that exp_dir, name, and version will be ignored.\n",
      "[NeMo W 2024-12-10 13:24:32 exp_manager:757] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 13:24:32 exp_manager:455] Experiments will be logged at results/llama-3.1-nemotron/70B/bf16-mixed\n",
      "[NeMo I 2024-12-10 13:24:32 exp_manager:983] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 13:24:32 exp_manager:1111] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 1000. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 13:28:25 megatron_init:269] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:280] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:283] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:291] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:294] All context parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:295] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:302] Rank 0 has model parallel group: [0, 1, 2, 3]\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:303] All model parallel group ranks: [[0, 1, 2, 3]]\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:312] Rank 0 has tensor model parallel group: [0, 1, 2, 3]\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:316] All tensor model parallel group ranks: [[0, 1, 2, 3]]\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:317] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:337] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:349] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:355] All pipeline model parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:356] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:357] All embedding group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-10 13:28:25 megatron_init:358] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2024-12-10 13:28:25 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 13:28:25 tokenizer_utils:183] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3.1-70B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 13:28:27 megatron_base_model:595] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:510] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 13:28:27 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[NeMo W 2024-12-10 13:28:28 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:582: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1\n",
      "      warnings.warn(\n",
      "    \n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distributed checkpoint with TensorStoreLoadShardedStrategy\n",
      "[NeMo I 2024-12-10 13:36:34 nlp_overrides:1346] Model MegatronGPTSFTModel was successfully restored from /work/ucloud-workshop-11-12-2024/models/llama-3.1-nemotron/70B/nemo/Llama-3_1-Nemotron-70B-Instruct.nemo.\n",
      "[NeMo I 2024-12-10 13:36:34 megatron_gpt_finetuning:72] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2024-12-10 13:36:34 nlp_adapter_mixins:240] Before adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 17.6 B | train\n",
      "    ------------------------------------------------\n",
      "    0         Trainable params\n",
      "    17.6 B    Non-trainable params\n",
      "    17.6 B    Total params\n",
      "    70,561.858Total estimated model params size (MB)\n",
      "[NeMo I 2024-12-10 13:36:38 nlp_adapter_mixins:245] After adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 17.7 B | train\n",
      "    ------------------------------------------------\n",
      "    11.8 M    Trainable params\n",
      "    17.6 B    Non-trainable params\n",
      "    17.7 B    Total params\n",
      "    70,609.043Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 13:36:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-12-10 13:36:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 13:36:41 megatron_gpt_sft_model:801] Building GPT SFT validation datasets.\n",
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.067161\n",
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.079545\n",
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:249] Loading pubmedqa/data/pubmedqa_val.jsonl\n",
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001530\n",
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-12-10 13:36:41 megatron_gpt_sft_model:805] Length of val dataset: 50\n",
      "[NeMo I 2024-12-10 13:36:41 megatron_gpt_sft_model:812] Building GPT SFT traing datasets.\n",
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.075593\n",
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.075534\n",
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:249] Loading pubmedqa/data/pubmedqa_train.jsonl\n",
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001608\n",
      "[NeMo I 2024-12-10 13:36:41 text_memmap_dataset:165] Computing global indices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 13:36:41 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1332: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:79.)\n",
      "      counts = torch.cuda.LongTensor([1])\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2024-12-10 13:36:44 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.06 (sec)\n",
      "[NeMo I 2024-12-10 13:36:44 megatron_gpt_sft_model:814] Length of train dataset: 8040\n",
      "[NeMo I 2024-12-10 13:36:44 megatron_gpt_sft_model:819] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-12-10 13:36:44 megatron_gpt_sft_model:819] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "[NeMo W 2024-12-10 13:36:44 megatron_base_model:1223] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-10 13:36:44 nlp_adapter_mixins:324] Optimizer groups set:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 17.7 B | train\n",
      "    ------------------------------------------------\n",
      "    11.8 M    Trainable params\n",
      "    17.6 B    Non-trainable params\n",
      "    17.7 B    Total params\n",
      "    70,609.043Total estimated model params size (MB)\n",
      "[NeMo I 2024-12-10 13:36:44 modelPT:786] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-12-10 13:36:44 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7efd64143340>\" \n",
      "    will be used during training (effective maximum steps = 1000) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 1000\n",
      "    )\n",
      "[NeMo I 2024-12-10 13:36:44 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7efd642b45e0>\" \n",
      "    will be used during training (effective maximum steps = 1000) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 1000\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | Float16Module | 17.7 B | train\n",
      "------------------------------------------------\n",
      "11.8 M    Trainable params\n",
      "17.6 B    Non-trainable params\n",
      "17.7 B    Total params\n",
      "70,609.043Total estimated model params size (MB)\n",
      "[NeMo W 2024-12-10 13:36:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s][NeMo I 2024-12-10 13:38:11 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "[NeMo W 2024-12-10 13:38:20 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:646: UserWarning: When using async grad allreduce it is recommended to set the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for maximum speedup\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:13<00:00,  0.15it/s][NeMo I 2024-12-10 13:38:24 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 13:38:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-10 13:38:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-10 13:38:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-10 13:38:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :   2%|▏         | 20/1000 [00:52<42:29, reduced_train_loss=1.390, global_step=19.00, consumed_samples=160.0, train_step_timing in s=2.150] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 13:40:45 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:01<00:06,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:02<00:05,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:03<00:04,  0.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:04<00:03,  0.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:05<00:02,  0.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:06<00:01,  0.88it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:07<00:00,  0.89it/s]\u001b[A[NeMo I 2024-12-10 13:40:53 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved. New best score: 0.908\n",
      "[rank: 1] Metric val_loss improved. New best score: 0.908\n",
      "[rank: 2] Metric val_loss improved. New best score: 0.908\n",
      "[rank: 3] Metric val_loss improved. New best score: 0.908\n",
      "Epoch 0, global step 20: 'validation_loss' reached 0.90807 (best 0.90807), saving model to '/work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.908-step=20-consumed_samples=160.0.ckpt' as top 1\n",
      "[NeMo W 2024-12-10 13:40:53 nlp_overrides:609] DistributedCheckpointIO configured but should not be used. Reverting back to TorchCheckpointIO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :   4%|▍         | 40/1000 [01:46<42:25, reduced_train_loss=0.0777, global_step=39.00, consumed_samples=320.0, train_step_timing in s=2.160, val_loss=0.908]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 13:41:39 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:01<00:06,  0.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:02<00:05,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:03<00:04,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:04<00:03,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:05<00:02,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:06<00:01,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:07<00:00,  0.91it/s]\u001b[A[NeMo I 2024-12-10 13:41:47 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.756 >= min_delta = 0.001. New best score: 0.152\n",
      "[rank: 2] Metric val_loss improved by 0.756 >= min_delta = 0.001. New best score: 0.152\n",
      "[rank: 1] Metric val_loss improved by 0.756 >= min_delta = 0.001. New best score: 0.152\n",
      "[rank: 3] Metric val_loss improved by 0.756 >= min_delta = 0.001. New best score: 0.152\n",
      "Epoch 0, global step 40: 'validation_loss' reached 0.15225 (best 0.15225), saving model to '/work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.152-step=40-consumed_samples=320.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :   4%|▍         | 40/1000 [01:53<45:30, reduced_train_loss=0.0777, global_step=39.00, consumed_samples=320.0, train_step_timing in s=2.160, val_loss=0.152][NeMo I 2024-12-10 13:41:47 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/mp_rank_00/megatron_gpt_peft_lora_tuning--validation_loss=0.908-step=20-consumed_samples=160.0.ckpt\n",
      "[NeMo I 2024-12-10 13:41:48 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/mp_rank_00/megatron_gpt_peft_lora_tuning--validation_loss=0.908-step=20-consumed_samples=160.0-last.ckpt\n",
      "Epoch 0: :   6%|▌         | 60/1000 [02:38<41:23, reduced_train_loss=0.0944, global_step=59.00, consumed_samples=480.0, train_step_timing in s=2.150, val_loss=0.152]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 13:42:32 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:01<00:06,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:02<00:05,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:03<00:04,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:04<00:03,  0.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:05<00:02,  0.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:06<00:01,  0.89it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:07<00:00,  0.90it/s]\u001b[A[NeMo I 2024-12-10 13:42:40 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 60: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :   6%|▌         | 60/1000 [02:46<43:26, reduced_train_loss=0.0944, global_step=59.00, consumed_samples=480.0, train_step_timing in s=2.150, val_loss=0.195][NeMo I 2024-12-10 13:42:40 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/mp_rank_00/megatron_gpt_peft_lora_tuning--validation_loss=0.152-step=40-consumed_samples=320.0-last.ckpt\n",
      "Epoch 0: :   8%|▊         | 80/1000 [03:31<40:31, reduced_train_loss=0.0283, global_step=79.00, consumed_samples=640.0, train_step_timing in s=2.140, val_loss=0.195] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 13:43:25 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:01<00:06,  0.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:02<00:05,  0.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:03<00:04,  0.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:04<00:03,  0.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:05<00:02,  0.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:06<00:01,  0.87it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:07<00:00,  0.88it/s]\u001b[A[NeMo I 2024-12-10 13:43:33 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 80: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :   8%|▊         | 80/1000 [03:39<42:03, reduced_train_loss=0.0283, global_step=79.00, consumed_samples=640.0, train_step_timing in s=2.140, val_loss=0.157][NeMo I 2024-12-10 13:43:33 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/mp_rank_00/megatron_gpt_peft_lora_tuning--validation_loss=0.195-step=60-consumed_samples=480.0-last.ckpt\n",
      "Epoch 0: :  10%|█         | 100/1000 [04:24<39:39, reduced_train_loss=0.123, global_step=99.00, consumed_samples=800.0, train_step_timing in s=2.120, val_loss=0.157]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 13:44:18 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:01<00:06,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:02<00:05,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:03<00:04,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:04<00:03,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:05<00:02,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:06<00:01,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:07<00:00,  0.91it/s]\u001b[A[NeMo I 2024-12-10 13:44:25 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 100: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  10%|█         | 100/1000 [04:32<40:48, reduced_train_loss=0.123, global_step=99.00, consumed_samples=800.0, train_step_timing in s=2.120, val_loss=0.168][NeMo I 2024-12-10 13:44:26 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/mp_rank_00/megatron_gpt_peft_lora_tuning--validation_loss=0.157-step=80-consumed_samples=640.0-last.ckpt\n",
      "Epoch 0: :  12%|█▏        | 120/1000 [05:17<38:47, reduced_train_loss=0.136, global_step=119.0, consumed_samples=960.0, train_step_timing in s=2.460, val_loss=0.168]  \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 13:45:11 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:01<00:06,  0.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:02<00:05,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:03<00:04,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:04<00:03,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:05<00:02,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:06<00:01,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:07<00:00,  0.91it/s]\u001b[A[NeMo I 2024-12-10 13:45:18 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 120: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  12%|█▏        | 120/1000 [05:25<39:43, reduced_train_loss=0.136, global_step=119.0, consumed_samples=960.0, train_step_timing in s=2.460, val_loss=0.237][NeMo I 2024-12-10 13:45:19 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/mp_rank_00/megatron_gpt_peft_lora_tuning--validation_loss=0.168-step=100-consumed_samples=800.0-last.ckpt\n",
      "Epoch 0: :  14%|█▍        | 140/1000 [06:09<37:51, reduced_train_loss=0.021, global_step=139.0, consumed_samples=1120.0, train_step_timing in s=2.140, val_loss=0.237]  \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 13:46:03 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:01<00:06,  0.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:02<00:05,  0.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:03<00:04,  0.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:04<00:03,  0.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:05<00:02,  0.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:06<00:01,  0.88it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:07<00:00,  0.88it/s]\u001b[A[NeMo I 2024-12-10 13:46:11 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 140: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  14%|█▍        | 140/1000 [06:17<38:40, reduced_train_loss=0.021, global_step=139.0, consumed_samples=1120.0, train_step_timing in s=2.140, val_loss=0.203][NeMo I 2024-12-10 13:46:11 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/mp_rank_00/megatron_gpt_peft_lora_tuning--validation_loss=0.237-step=120-consumed_samples=960.0-last.ckpt\n",
      "Epoch 0: :  16%|█▌        | 160/1000 [07:03<37:02, reduced_train_loss=0.0483, global_step=159.0, consumed_samples=1280.0, train_step_timing in s=2.470, val_loss=0.203] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 13:46:57 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:01<00:06,  0.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:02<00:05,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:03<00:04,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:04<00:03,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:05<00:02,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:06<00:01,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:07<00:00,  0.91it/s]\u001b[A[NeMo I 2024-12-10 13:47:04 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 160: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  16%|█▌        | 160/1000 [07:11<37:42, reduced_train_loss=0.0483, global_step=159.0, consumed_samples=1280.0, train_step_timing in s=2.470, val_loss=0.210][NeMo I 2024-12-10 13:47:05 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/mp_rank_00/megatron_gpt_peft_lora_tuning--validation_loss=0.203-step=140-consumed_samples=1120.0-last.ckpt\n",
      "Epoch 0: :  18%|█▊        | 180/1000 [07:55<36:06, reduced_train_loss=0.0557, global_step=179.0, consumed_samples=1440.0, train_step_timing in s=2.150, val_loss=0.210] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 13:47:49 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:01<00:06,  0.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:02<00:05,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:03<00:04,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:04<00:03,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:05<00:02,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:06<00:01,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:07<00:00,  0.91it/s]\u001b[A[NeMo I 2024-12-10 13:47:57 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 180: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  18%|█▊        | 180/1000 [08:03<36:41, reduced_train_loss=0.0557, global_step=179.0, consumed_samples=1440.0, train_step_timing in s=2.150, val_loss=0.213][NeMo I 2024-12-10 13:47:57 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/mp_rank_00/megatron_gpt_peft_lora_tuning--validation_loss=0.210-step=160-consumed_samples=1280.0-last.ckpt\n",
      "Epoch 0: :  20%|██        | 200/1000 [08:48<35:14, reduced_train_loss=0.00644, global_step=199.0, consumed_samples=1600.0, train_step_timing in s=2.150, val_loss=0.213] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 13:48:42 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:01<00:06,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:02<00:05,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:03<00:04,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:04<00:03,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:05<00:02,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:06<00:01,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:07<00:00,  0.91it/s]\u001b[A[NeMo I 2024-12-10 13:48:50 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 200: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  20%|██        | 200/1000 [08:56<35:45, reduced_train_loss=0.00644, global_step=199.0, consumed_samples=1600.0, train_step_timing in s=2.150, val_loss=0.272][NeMo I 2024-12-10 13:48:50 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/mp_rank_00/megatron_gpt_peft_lora_tuning--validation_loss=0.213-step=180-consumed_samples=1440.0-last.ckpt\n",
      "Epoch 0: :  22%|██▏       | 220/1000 [09:41<34:20, reduced_train_loss=0.017, global_step=219.0, consumed_samples=1760.0, train_step_timing in s=2.120, val_loss=0.272]   \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 13:49:34 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:01<00:06,  0.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:02<00:05,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:03<00:04,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:04<00:03,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:05<00:02,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:06<00:01,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:07<00:00,  0.90it/s]\u001b[A[NeMo I 2024-12-10 13:49:42 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 220: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  22%|██▏       | 220/1000 [09:48<34:48, reduced_train_loss=0.017, global_step=219.0, consumed_samples=1760.0, train_step_timing in s=2.120, val_loss=0.244][NeMo I 2024-12-10 13:49:42 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/mp_rank_00/megatron_gpt_peft_lora_tuning--validation_loss=0.272-step=200-consumed_samples=1600.0-last.ckpt\n",
      "Epoch 0: :  24%|██▍       | 240/1000 [10:34<33:28, reduced_train_loss=0.00261, global_step=239.0, consumed_samples=1920.0, train_step_timing in s=2.130, val_loss=0.244]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-10 13:50:28 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 1/7 [00:01<00:06,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 2/7 [00:02<00:05,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 3/7 [00:03<00:04,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 4/7 [00:04<00:03,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 5/7 [00:05<00:02,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 6/7 [00:06<00:01,  0.91it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 7/7 [00:07<00:00,  0.91it/s]\u001b[A[NeMo I 2024-12-10 13:50:35 num_microbatches_calculator:119] setting number of micro-batches to constant 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Monitored metric val_loss did not improve in the last 10 records. Best score: 0.152. Signaling Trainer to stop.\n",
      "[rank: 3] Monitored metric val_loss did not improve in the last 10 records. Best score: 0.152. Signaling Trainer to stop.\n",
      "[rank: 2] Monitored metric val_loss did not improve in the last 10 records. Best score: 0.152. Signaling Trainer to stop.\n",
      "[rank: 1] Monitored metric val_loss did not improve in the last 10 records. Best score: 0.152. Signaling Trainer to stop.\n",
      "Epoch 0, global step 240: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  24%|██▍       | 240/1000 [10:42<33:53, reduced_train_loss=0.00261, global_step=239.0, consumed_samples=1920.0, train_step_timing in s=2.130, val_loss=0.396][NeMo I 2024-12-10 13:50:36 nlp_overrides:593] Removing checkpoint: /work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/mp_rank_00/megatron_gpt_peft_lora_tuning--validation_loss=0.244-step=220-consumed_samples=1760.0-last.ckpt\n",
      "Epoch 0: :  24%|██▍       | 240/1000 [10:42<33:54, reduced_train_loss=0.00261, global_step=239.0, consumed_samples=1920.0, train_step_timing in s=2.130, val_loss=0.396]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.152-step=40-consumed_samples=320.0.ckpt\n",
      "Restored all states from the checkpoint at /work/ucloud-workshop-11-12-2024/results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.152-step=40-consumed_samples=320.0.ckpt\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "# Set paths to the model, train, validation and test sets.\n",
    "PRECISION=bf16-mixed\n",
    "MODEL=\"models/llama-3.1-nemotron/70B/nemo/Llama-3_1-Nemotron-70B-Instruct.nemo\"\n",
    "OUTPUT_DIR=\"results/llama-3.1-nemotron/70B/$PRECISION\"\n",
    "rm -rf \"$OUTPUT_DIR\"\n",
    "\n",
    "TRAIN_DS=\"[pubmedqa/data/pubmedqa_train.jsonl]\"\n",
    "VALID_DS=\"[pubmedqa/data/pubmedqa_val.jsonl]\"\n",
    "\n",
    "SCHEME=\"lora\"\n",
    "GPUS=4   \n",
    "TP_SIZE=4\n",
    "PP_SIZE=1\n",
    "\n",
    "torchrun --nproc_per_node=${GPUS} \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    exp_manager.exp_dir=${OUTPUT_DIR} \\\n",
    "    exp_manager.explicit_log_dir=${OUTPUT_DIR} \\\n",
    "    trainer.devices=${GPUS} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=${PRECISION} \\\n",
    "    trainer.val_check_interval=20 \\\n",
    "    trainer.max_steps=1000 \\\n",
    "    model.megatron_amp_O2=True \\  # enforce mixed precision\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    model.global_batch_size=8 \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.data.train_ds.num_workers=10 \\\n",
    "    model.data.validation_ds.num_workers=10 \\\n",
    "    model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "    model.peft.peft_scheme=${SCHEME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4331fd-da30-4e29-8477-3085118e4a7b",
   "metadata": {},
   "source": [
    "This will create a LoRA adapter - a file named `megatron_gpt_peft_lora_tuning.nemo` in `./results/.../checkpoints/`. We'll use this later.\n",
    "\n",
    "To further configure the run above -\n",
    "\n",
    "* **A different PEFT technique**: The `peft.peft_scheme` parameter determines the technique being used. In this case, we did LoRA, but NeMo Framework supports other techniques as well - such as P-tuning, Adapters, and IA3. For more information, refer to the [PEFT support matrix](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/nlp/nemo_megatron/peft/landing_page.html). For example, for P-tuning, simply set \n",
    "\n",
    "```bash\n",
    "model.peft.peft_scheme=\"ptuning\" # instead of \"lora\"\n",
    "```\n",
    "\n",
    "* **Tuning Llama-3.3 70B**: You will need 4xH100 GPUs. Provide the path to it's .nemo checkpoint (similar to the download and conversion steps earlier), and change the model parallelization settings for Llama-3 70B PEFT to distribute across the GPUs. It is also recommended to run the fine-tuning script from a terminal directly instead of Jupyter when using more than 1 GPU.\n",
    "```bash\n",
    "model.tensor_model_parallel_size=4\n",
    "model.pipeline_model_parallel_size=1\n",
    "```\n",
    "\n",
    "You can override many such configurations while running the script. A full set of possible configurations is located in [NeMo Framework Github](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/conf/megatron_gpt_finetuning_config.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53979a4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Inference with NeMo Framework\n",
    "\n",
    "Running text generation within the framework is also possible with running a Python script. Note that is more for testing and validation, not a full-fledged  deployment solution like NVIDIA NIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00d1e3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 92440\n",
      "-rw-r--r--. 1 ucloud ucloud 94658560 Dec 10 13:50 megatron_gpt_peft_lora_tuning.nemo\n",
      "drwxr-xr-x. 1 ucloud ucloud        0 Dec 10 13:50 mp_rank_00\n",
      "drwxr-xr-x. 1 ucloud ucloud        0 Dec 10 13:50 mp_rank_01\n",
      "drwxr-xr-x. 1 ucloud ucloud        0 Dec 10 13:50 mp_rank_02\n",
      "drwxr-xr-x. 1 ucloud ucloud        0 Dec 10 13:50 mp_rank_03\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Check that the LORA model file exists\n",
    "\n",
    "python -c \"import torch; torch.cuda.empty_cache()\"\n",
    "\n",
    "OUTPUT_DIR=\"results/llama-3.1-nemotron/70B/bf16-mixed\"\n",
    "ls -l $OUTPUT_DIR/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3430a0b0-05a0-4179-8750-151d492bb9ae",
   "metadata": {},
   "source": [
    "In the code snippet below, the following configurations are worth noting: \n",
    "\n",
    "1. `model.restore_from_path` to the path for the Meta-Llama-3-8B-Instruct.nemo file.\n",
    "2. `model.peft.restore_from_path` to the path for the PEFT checkpoint that was created in the fine-tuning run in the last step.\n",
    "3. `model.test_ds.file_names` to the path of the pubmedqa_test.jsonl file\n",
    "\n",
    "If you have made any changes in model or experiment paths, please ensure they are configured correctly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93108124-32a5-4c8f-ab25-52dbe9b26ac6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ucloud/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-10 13:56:21,208] torch.distributed.run: [WARNING] \n",
      "[2024-12-10 13:56:21,208] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-12-10 13:56:21,208] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-12-10 13:56:21,208] torch.distributed.run: [WARNING] *****************************************\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "[NeMo W 2024-12-10 13:56:36 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 13:56:36 megatron_gpt_generate:125] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-12-10 13:56:36 megatron_gpt_generate:126] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 4\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 20000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 200\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.test_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: max\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: true\n",
      "        save_best_model: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 4\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 1\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: models/llama-3.1-nemotron/70B/nemo/Llama-3_1-Nemotron-70B-Instruct.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: true\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: adapter\n",
      "        restore_from_path: results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/megatron_gpt_peft_lora_tuning.nemo\n",
      "        restore_from_ckpt:\n",
      "          checkpoint_dir: null\n",
      "          checkpoint_name: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        test_ds:\n",
      "          file_names:\n",
      "          - pubmedqa/data/pubmedqa_test.jsonl\n",
      "          names:\n",
      "          - pubmedqa\n",
      "          global_batch_size: 1\n",
      "          micro_batch_size: 1\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          context_key: input\n",
      "          label_key: ${data.train_ds.label_key}\n",
      "          add_eos: ${data.train_ds.add_eos}\n",
      "          add_sep: ${data.train_ds.add_sep}\n",
      "          add_bos: ${data.train_ds.add_bos}\n",
      "          write_predictions_to_file: true\n",
      "          output_file_path_prefix: pubmedQA_result_\n",
      "          truncation_field: ${data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 3\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "    inference:\n",
      "      greedy: true\n",
      "      top_k: 0\n",
      "      top_p: 0.9\n",
      "      temperature: 1.0\n",
      "      all_probs: false\n",
      "      repetition_penalty: 1.0\n",
      "      min_tokens_to_generate: 0\n",
      "      compute_logprob: false\n",
      "      outfile_path: output.txt\n",
      "      compute_attention_mask: true\n",
      "    server: false\n",
      "    port: 5555\n",
      "    web_server: false\n",
      "    share: true\n",
      "    username: test\n",
      "    password: test2\n",
      "    web_port: 9889\n",
      "    chat: false\n",
      "    chatbot_config:\n",
      "      value: false\n",
      "      attributes:\n",
      "      - name: Quality\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: quality\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Toxicity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: toxcity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Humor\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: humor\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Creativity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: creativity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Violence\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: violence\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Helpfulness\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: helpfulness\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Not_Appropriate\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: not_appropriate\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Language\n",
      "        choices:\n",
      "        - ar\n",
      "        - bg\n",
      "        - bn\n",
      "        - ca\n",
      "        - cs\n",
      "        - da\n",
      "        - de\n",
      "        - el\n",
      "        - en\n",
      "        - eo\n",
      "        - es\n",
      "        - eu\n",
      "        - fa\n",
      "        - fi\n",
      "        - fr\n",
      "        - gl\n",
      "        - he\n",
      "        - hu\n",
      "        - id\n",
      "        - it\n",
      "        - ja\n",
      "        - ko\n",
      "        - nb\n",
      "        - nl\n",
      "        - pl\n",
      "        - pt\n",
      "        - ro\n",
      "        - ru\n",
      "        - sk\n",
      "        - sv\n",
      "        - th\n",
      "        - tr\n",
      "        - uk\n",
      "        - vi\n",
      "        - zh\n",
      "        key: lang\n",
      "        type: list\n",
      "        default: en\n",
      "      user: User\n",
      "      assistant: Assistant\n",
      "      system: 'A chat between a curious human and an artificial intelligence assistant.\n",
      "        The assistant gives helpful, detailed, and polite answers to the human''s questions.\n",
      "    \n",
      "    \n",
      "        '\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 13:56:36 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 14:00:01 megatron_init:269] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:280] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:283] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:291] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:294] All context parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:295] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:302] Rank 0 has model parallel group: [0, 1, 2, 3]\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:303] All model parallel group ranks: [[0, 1, 2, 3]]\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:312] Rank 0 has tensor model parallel group: [0, 1, 2, 3]\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:316] All tensor model parallel group ranks: [[0, 1, 2, 3]]\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:317] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:337] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:349] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:355] All pipeline model parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:356] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:357] All embedding group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-10 14:00:01 megatron_init:358] Rank 0 has embedding rank: 0\n",
      "setting number of micro-batches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 14:00:01 tokenizer_utils:183] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3.1-70B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 14:00:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 14:00:01 megatron_base_model:595] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-10 14:00:01 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[NeMo W 2024-12-10 14:00:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:582: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1\n",
      "      warnings.warn(\n",
      "    \n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distributed checkpoint with TensorStoreLoadShardedStrategy\n",
      "[NeMo I 2024-12-10 14:07:23 nlp_overrides:1346] Model MegatronGPTSFTModel was successfully restored from /work/ucloud-workshop-11-12-2024/models/llama-3.1-nemotron/70B/nemo/Llama-3_1-Nemotron-70B-Instruct.nemo.\n",
      "[NeMo I 2024-12-10 14:07:23 nlp_adapter_mixins:240] Before adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 17.6 B | train\n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    17.6 B    Non-trainable params\n",
      "    17.6 B    Total params\n",
      "    70,561.858Total estimated model params size (MB)\n",
      "[NeMo I 2024-12-10 14:07:27 nlp_adapter_mixins:245] After adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 17.7 B | train\n",
      "    -------------------------------------------\n",
      "    11.8 M    Trainable params\n",
      "    17.6 B    Non-trainable params\n",
      "    17.7 B    Total params\n",
      "    70,609.043Total estimated model params size (MB)\n",
      "[NeMo I 2024-12-10 14:07:27 megatron_gpt_generate:156] Freezing parameters for PEFT eval:\n",
      "      | Name  | Type     | Params | Mode\n",
      "    ------------------------------------------\n",
      "    0 | model | GPTModel | 17.7 B | eval\n",
      "    ------------------------------------------\n",
      "    0         Trainable params\n",
      "    17.7 B    Non-trainable params\n",
      "    17.7 B    Total params\n",
      "    70,609.043Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 14:07:27 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-12-10 14:07:27 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 14:07:27 megatron_gpt_sft_model:793] Building GPT SFT test datasets.\n",
      "[NeMo I 2024-12-10 14:07:27 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-12-10 14:07:27 text_memmap_dataset:525] Processing 1 data files using 96 workers\n",
      "[NeMo I 2024-12-10 14:07:29 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:01.724135\n",
      "[NeMo I 2024-12-10 14:07:29 text_memmap_dataset:525] Processing 1 data files using 96 workers\n",
      "[NeMo I 2024-12-10 14:07:31 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:01.775241\n",
      "[NeMo I 2024-12-10 14:07:31 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-12-10 14:07:31 text_memmap_dataset:249] Loading pubmedqa/data/pubmedqa_test.jsonl\n",
      "[NeMo I 2024-12-10 14:07:31 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001674\n",
      "[NeMo I 2024-12-10 14:07:31 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-12-10 14:07:31 megatron_gpt_sft_model:796] Length of test dataset: 500\n",
      "[NeMo I 2024-12-10 14:07:31 megatron_gpt_sft_model:819] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "[NeMo W 2024-12-10 14:07:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-12-10 14:07:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `test_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: |          | 0/? [00:00<?, ?it/s]setting number of micro-batches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 14:07:38 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:484: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:79.)\n",
      "      input_info_tensor = torch.cuda.FloatTensor(input_info)\n",
      "    \n",
      "[NeMo W 2024-12-10 14:07:41 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:492: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "      string_tensor = torch.as_tensor(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/500 [00:00<?, ?it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   0%|          | 1/500 [00:14<1:57:57,  0.07it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   0%|          | 2/500 [00:14<1:01:30,  0.13it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   1%|          | 3/500 [00:15<42:38,  0.19it/s]  setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   1%|          | 4/500 [00:16<33:12,  0.25it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   1%|          | 5/500 [00:16<27:32,  0.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   1%|          | 6/500 [00:17<23:45,  0.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   1%|▏         | 7/500 [00:18<21:19,  0.39it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   2%|▏         | 8/500 [00:18<19:15,  0.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   2%|▏         | 9/500 [00:19<17:38,  0.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   2%|▏         | 10/500 [00:20<16:21,  0.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   2%|▏         | 11/500 [00:20<15:28,  0.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   2%|▏         | 12/500 [00:21<14:34,  0.56it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   3%|▎         | 13/500 [00:22<13:48,  0.59it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   3%|▎         | 14/500 [00:22<13:09,  0.62it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   3%|▎         | 15/500 [00:23<12:41,  0.64it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   3%|▎         | 16/500 [00:24<12:11,  0.66it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   3%|▎         | 17/500 [00:24<11:44,  0.69it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   4%|▎         | 18/500 [00:25<11:20,  0.71it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   4%|▍         | 19/500 [00:26<10:59,  0.73it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   4%|▍         | 20/500 [00:26<10:39,  0.75it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   4%|▍         | 21/500 [00:27<10:22,  0.77it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   4%|▍         | 22/500 [00:27<10:06,  0.79it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   5%|▍         | 23/500 [00:28<09:57,  0.80it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   5%|▍         | 24/500 [00:29<09:43,  0.82it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   5%|▌         | 25/500 [00:30<09:31,  0.83it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   5%|▌         | 26/500 [00:30<09:23,  0.84it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   5%|▌         | 27/500 [00:31<09:12,  0.86it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   6%|▌         | 28/500 [00:32<09:01,  0.87it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   6%|▌         | 29/500 [00:32<08:55,  0.88it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   6%|▌         | 30/500 [00:33<08:46,  0.89it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   6%|▌         | 31/500 [00:34<08:37,  0.91it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   6%|▋         | 32/500 [00:34<08:29,  0.92it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   7%|▋         | 33/500 [00:35<08:22,  0.93it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   7%|▋         | 34/500 [00:36<08:14,  0.94it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   7%|▋         | 35/500 [00:36<08:07,  0.95it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   7%|▋         | 36/500 [00:37<08:01,  0.96it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   7%|▋         | 37/500 [00:37<07:55,  0.97it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   8%|▊         | 38/500 [00:38<07:49,  0.98it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   8%|▊         | 39/500 [00:39<07:43,  0.99it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   8%|▊         | 40/500 [00:39<07:37,  1.00it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   8%|▊         | 41/500 [00:40<07:32,  1.01it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   8%|▊         | 42/500 [00:41<07:27,  1.02it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   9%|▊         | 43/500 [00:41<07:22,  1.03it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   9%|▉         | 44/500 [00:42<07:18,  1.04it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   9%|▉         | 45/500 [00:42<07:13,  1.05it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   9%|▉         | 46/500 [00:43<07:11,  1.05it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:   9%|▉         | 47/500 [00:44<07:07,  1.06it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  10%|▉         | 48/500 [00:45<07:04,  1.07it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  10%|▉         | 49/500 [00:45<07:00,  1.07it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  10%|█         | 50/500 [00:46<06:56,  1.08it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  10%|█         | 51/500 [00:46<06:52,  1.09it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  10%|█         | 52/500 [00:47<06:49,  1.09it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  11%|█         | 53/500 [00:48<06:46,  1.10it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  11%|█         | 54/500 [00:48<06:42,  1.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  11%|█         | 55/500 [00:49<06:39,  1.11it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  11%|█         | 56/500 [00:50<06:36,  1.12it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  11%|█▏        | 57/500 [00:50<06:33,  1.13it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  12%|█▏        | 58/500 [00:51<06:30,  1.13it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  12%|█▏        | 59/500 [00:51<06:27,  1.14it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  12%|█▏        | 60/500 [00:52<06:24,  1.14it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  12%|█▏        | 61/500 [00:53<06:22,  1.15it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  12%|█▏        | 62/500 [00:53<06:19,  1.15it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  13%|█▎        | 63/500 [00:54<06:16,  1.16it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  13%|█▎        | 64/500 [00:54<06:14,  1.16it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  13%|█▎        | 65/500 [00:55<06:11,  1.17it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  13%|█▎        | 66/500 [00:56<06:09,  1.17it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  13%|█▎        | 67/500 [00:56<06:07,  1.18it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  14%|█▎        | 68/500 [00:57<06:04,  1.18it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  14%|█▍        | 69/500 [00:58<06:02,  1.19it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  14%|█▍        | 70/500 [00:58<06:00,  1.19it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  14%|█▍        | 71/500 [00:59<05:58,  1.20it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  14%|█▍        | 72/500 [00:59<05:55,  1.20it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  15%|█▍        | 73/500 [01:00<05:53,  1.21it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  15%|█▍        | 74/500 [01:01<05:51,  1.21it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  15%|█▌        | 75/500 [01:01<05:49,  1.22it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  15%|█▌        | 76/500 [01:02<05:47,  1.22it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  15%|█▌        | 77/500 [01:02<05:45,  1.22it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  16%|█▌        | 78/500 [01:03<05:43,  1.23it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  16%|█▌        | 79/500 [01:04<05:42,  1.23it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  16%|█▌        | 80/500 [01:04<05:40,  1.23it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  16%|█▌        | 81/500 [01:05<05:38,  1.24it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  16%|█▋        | 82/500 [01:06<05:36,  1.24it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  17%|█▋        | 83/500 [01:06<05:35,  1.24it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  17%|█▋        | 84/500 [01:07<05:33,  1.25it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  17%|█▋        | 85/500 [01:07<05:31,  1.25it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  17%|█▋        | 86/500 [01:08<05:29,  1.25it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  17%|█▋        | 87/500 [01:09<05:28,  1.26it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  18%|█▊        | 88/500 [01:09<05:26,  1.26it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  18%|█▊        | 89/500 [01:10<05:25,  1.26it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  18%|█▊        | 90/500 [01:11<05:23,  1.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  18%|█▊        | 91/500 [01:11<05:21,  1.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  18%|█▊        | 92/500 [01:12<05:20,  1.27it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  19%|█▊        | 93/500 [01:12<05:18,  1.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  19%|█▉        | 94/500 [01:13<05:17,  1.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  19%|█▉        | 95/500 [01:14<05:15,  1.28it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  19%|█▉        | 96/500 [01:14<05:14,  1.29it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  19%|█▉        | 97/500 [01:15<05:12,  1.29it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  20%|█▉        | 98/500 [01:15<05:11,  1.29it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  20%|█▉        | 99/500 [01:16<05:10,  1.29it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  20%|██        | 100/500 [01:17<05:08,  1.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  20%|██        | 101/500 [01:17<05:07,  1.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  20%|██        | 102/500 [01:18<05:05,  1.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  21%|██        | 103/500 [01:19<05:04,  1.30it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  21%|██        | 104/500 [01:19<05:03,  1.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  21%|██        | 105/500 [01:20<05:01,  1.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  21%|██        | 106/500 [01:20<05:00,  1.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  21%|██▏       | 107/500 [01:21<04:59,  1.31it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  22%|██▏       | 108/500 [01:22<04:58,  1.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  22%|██▏       | 109/500 [01:22<04:56,  1.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  22%|██▏       | 110/500 [01:23<04:56,  1.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  22%|██▏       | 111/500 [01:24<04:55,  1.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  22%|██▏       | 112/500 [01:24<04:53,  1.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  23%|██▎       | 113/500 [01:25<04:52,  1.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  23%|██▎       | 114/500 [01:26<04:51,  1.32it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  23%|██▎       | 115/500 [01:26<04:50,  1.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  23%|██▎       | 116/500 [01:27<04:48,  1.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  23%|██▎       | 117/500 [01:27<04:47,  1.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  24%|██▎       | 118/500 [01:28<04:46,  1.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  24%|██▍       | 119/500 [01:29<04:45,  1.33it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  24%|██▍       | 120/500 [01:29<04:44,  1.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  24%|██▍       | 121/500 [01:30<04:43,  1.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  24%|██▍       | 122/500 [01:30<04:41,  1.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  25%|██▍       | 123/500 [01:31<04:40,  1.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  25%|██▍       | 124/500 [01:32<04:39,  1.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  25%|██▌       | 125/500 [01:32<04:38,  1.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  25%|██▌       | 126/500 [01:33<04:38,  1.34it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  25%|██▌       | 127/500 [01:34<04:36,  1.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  26%|██▌       | 128/500 [01:34<04:35,  1.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  26%|██▌       | 129/500 [01:35<04:34,  1.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  26%|██▌       | 130/500 [01:36<04:33,  1.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  26%|██▌       | 131/500 [01:36<04:32,  1.35it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  26%|██▋       | 132/500 [01:37<04:31,  1.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  27%|██▋       | 133/500 [01:38<04:30,  1.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  27%|██▋       | 134/500 [01:38<04:29,  1.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  27%|██▋       | 135/500 [01:39<04:28,  1.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  27%|██▋       | 136/500 [01:39<04:27,  1.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  27%|██▋       | 137/500 [01:40<04:26,  1.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  28%|██▊       | 138/500 [01:41<04:25,  1.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  28%|██▊       | 139/500 [01:41<04:24,  1.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  28%|██▊       | 140/500 [01:42<04:23,  1.36it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  28%|██▊       | 141/500 [01:43<04:22,  1.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  28%|██▊       | 142/500 [01:43<04:21,  1.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  29%|██▊       | 143/500 [01:44<04:20,  1.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  29%|██▉       | 144/500 [01:45<04:19,  1.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  29%|██▉       | 145/500 [01:45<04:18,  1.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  29%|██▉       | 146/500 [01:46<04:17,  1.37it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  29%|██▉       | 147/500 [01:46<04:16,  1.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  30%|██▉       | 148/500 [01:47<04:15,  1.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  30%|██▉       | 149/500 [01:48<04:14,  1.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  30%|███       | 150/500 [01:48<04:13,  1.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  30%|███       | 151/500 [01:49<04:12,  1.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  30%|███       | 152/500 [01:49<04:11,  1.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  31%|███       | 153/500 [01:50<04:10,  1.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  31%|███       | 154/500 [01:51<04:09,  1.38it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  31%|███       | 155/500 [01:51<04:08,  1.39it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  31%|███       | 156/500 [01:52<04:07,  1.39it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  31%|███▏      | 157/500 [01:53<04:07,  1.39it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  32%|███▏      | 158/500 [01:53<04:06,  1.39it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  32%|███▏      | 159/500 [01:54<04:05,  1.39it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  32%|███▏      | 160/500 [01:54<04:04,  1.39it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  32%|███▏      | 161/500 [01:55<04:03,  1.39it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  32%|███▏      | 162/500 [01:56<04:02,  1.39it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  33%|███▎      | 163/500 [01:56<04:01,  1.40it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  33%|███▎      | 164/500 [01:57<04:00,  1.40it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  33%|███▎      | 165/500 [01:58<03:59,  1.40it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  33%|███▎      | 166/500 [01:58<03:58,  1.40it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  33%|███▎      | 167/500 [01:59<03:57,  1.40it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  34%|███▎      | 168/500 [01:59<03:56,  1.40it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  34%|███▍      | 169/500 [02:00<03:56,  1.40it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  34%|███▍      | 170/500 [02:01<03:55,  1.40it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  34%|███▍      | 171/500 [02:01<03:54,  1.40it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  34%|███▍      | 172/500 [02:02<03:53,  1.41it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  35%|███▍      | 173/500 [02:02<03:52,  1.41it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  35%|███▍      | 174/500 [02:03<03:51,  1.41it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  35%|███▌      | 175/500 [02:04<03:50,  1.41it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  35%|███▌      | 176/500 [02:04<03:49,  1.41it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  35%|███▌      | 177/500 [02:05<03:48,  1.41it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  36%|███▌      | 178/500 [02:06<03:48,  1.41it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  36%|███▌      | 179/500 [02:06<03:47,  1.41it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  36%|███▌      | 180/500 [02:07<03:46,  1.41it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  36%|███▌      | 181/500 [02:07<03:45,  1.42it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  36%|███▋      | 182/500 [02:08<03:44,  1.42it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  37%|███▋      | 183/500 [02:09<03:43,  1.42it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  37%|███▋      | 184/500 [02:09<03:42,  1.42it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  37%|███▋      | 185/500 [02:10<03:41,  1.42it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  37%|███▋      | 186/500 [02:10<03:41,  1.42it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  37%|███▋      | 187/500 [02:11<03:40,  1.42it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  38%|███▊      | 188/500 [02:12<03:39,  1.42it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  38%|███▊      | 189/500 [02:12<03:38,  1.42it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  38%|███▊      | 190/500 [02:13<03:37,  1.42it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  38%|███▊      | 191/500 [02:14<03:36,  1.42it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  38%|███▊      | 192/500 [02:14<03:36,  1.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  39%|███▊      | 193/500 [02:15<03:35,  1.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  39%|███▉      | 194/500 [02:15<03:34,  1.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  39%|███▉      | 195/500 [02:16<03:33,  1.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  39%|███▉      | 196/500 [02:17<03:32,  1.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  39%|███▉      | 197/500 [02:18<03:32,  1.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  40%|███▉      | 198/500 [02:18<03:31,  1.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  40%|███▉      | 199/500 [02:19<03:30,  1.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  40%|████      | 200/500 [02:19<03:29,  1.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  40%|████      | 201/500 [02:20<03:29,  1.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  40%|████      | 202/500 [02:21<03:28,  1.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  41%|████      | 203/500 [02:21<03:27,  1.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  41%|████      | 204/500 [02:22<03:26,  1.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  41%|████      | 205/500 [02:22<03:25,  1.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  41%|████      | 206/500 [02:23<03:24,  1.43it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  41%|████▏     | 207/500 [02:24<03:24,  1.44it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  42%|████▏     | 208/500 [02:24<03:23,  1.44it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  42%|████▏     | 209/500 [02:25<03:22,  1.44it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  42%|████▏     | 210/500 [02:26<03:21,  1.44it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  42%|████▏     | 211/500 [02:26<03:20,  1.44it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  42%|████▏     | 212/500 [02:27<03:20,  1.44it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  43%|████▎     | 213/500 [02:27<03:19,  1.44it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  43%|████▎     | 214/500 [02:28<03:18,  1.44it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  43%|████▎     | 215/500 [02:29<03:17,  1.44it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  43%|████▎     | 216/500 [02:29<03:16,  1.44it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  43%|████▎     | 217/500 [02:30<03:16,  1.44it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  44%|████▎     | 218/500 [02:31<03:15,  1.44it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  44%|████▍     | 219/500 [02:31<03:14,  1.44it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  44%|████▍     | 220/500 [02:32<03:13,  1.44it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  44%|████▍     | 221/500 [02:32<03:13,  1.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  44%|████▍     | 222/500 [02:33<03:12,  1.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  45%|████▍     | 223/500 [02:34<03:11,  1.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  45%|████▍     | 224/500 [02:34<03:10,  1.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  45%|████▌     | 225/500 [02:35<03:09,  1.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  45%|████▌     | 226/500 [02:36<03:09,  1.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  45%|████▌     | 227/500 [02:36<03:08,  1.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  46%|████▌     | 228/500 [02:37<03:07,  1.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  46%|████▌     | 229/500 [02:37<03:06,  1.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  46%|████▌     | 230/500 [02:38<03:06,  1.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  46%|████▌     | 231/500 [02:39<03:05,  1.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  46%|████▋     | 232/500 [02:39<03:04,  1.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  47%|████▋     | 233/500 [02:40<03:03,  1.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  47%|████▋     | 234/500 [02:40<03:02,  1.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  47%|████▋     | 235/500 [02:41<03:02,  1.45it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  47%|████▋     | 236/500 [02:42<03:01,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  47%|████▋     | 237/500 [02:42<03:00,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  48%|████▊     | 238/500 [02:43<02:59,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  48%|████▊     | 239/500 [02:44<02:59,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  48%|████▊     | 240/500 [02:44<02:58,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  48%|████▊     | 241/500 [02:45<02:57,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  48%|████▊     | 242/500 [02:45<02:56,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  49%|████▊     | 243/500 [02:46<02:56,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  49%|████▉     | 244/500 [02:47<02:55,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  49%|████▉     | 245/500 [02:47<02:54,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  49%|████▉     | 246/500 [02:48<02:53,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  49%|████▉     | 247/500 [02:48<02:53,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  50%|████▉     | 248/500 [02:49<02:52,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  50%|████▉     | 249/500 [02:50<02:51,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  50%|█████     | 250/500 [02:50<02:50,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  50%|█████     | 251/500 [02:51<02:50,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  50%|█████     | 252/500 [02:52<02:49,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  51%|█████     | 253/500 [02:52<02:48,  1.46it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  51%|█████     | 254/500 [02:53<02:47,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  51%|█████     | 255/500 [02:53<02:47,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  51%|█████     | 256/500 [02:54<02:46,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  51%|█████▏    | 257/500 [02:55<02:45,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  52%|█████▏    | 258/500 [02:55<02:44,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  52%|█████▏    | 259/500 [02:56<02:44,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  52%|█████▏    | 260/500 [02:57<02:43,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  52%|█████▏    | 261/500 [02:57<02:42,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  52%|█████▏    | 262/500 [02:58<02:41,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  53%|█████▎    | 263/500 [02:58<02:41,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  53%|█████▎    | 264/500 [02:59<02:40,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  53%|█████▎    | 265/500 [03:00<02:39,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  53%|█████▎    | 266/500 [03:00<02:39,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  53%|█████▎    | 267/500 [03:01<02:38,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  54%|█████▎    | 268/500 [03:01<02:37,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  54%|█████▍    | 269/500 [03:02<02:36,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  54%|█████▍    | 270/500 [03:03<02:36,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  54%|█████▍    | 271/500 [03:03<02:35,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  54%|█████▍    | 272/500 [03:04<02:34,  1.47it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  55%|█████▍    | 273/500 [03:05<02:33,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  55%|█████▍    | 274/500 [03:05<02:33,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  55%|█████▌    | 275/500 [03:06<02:32,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  55%|█████▌    | 276/500 [03:06<02:31,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  55%|█████▌    | 277/500 [03:07<02:30,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  56%|█████▌    | 278/500 [03:08<02:30,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  56%|█████▌    | 279/500 [03:08<02:29,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  56%|█████▌    | 280/500 [03:09<02:28,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  56%|█████▌    | 281/500 [03:10<02:28,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  56%|█████▋    | 282/500 [03:10<02:27,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  57%|█████▋    | 283/500 [03:11<02:26,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  57%|█████▋    | 284/500 [03:11<02:25,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  57%|█████▋    | 285/500 [03:12<02:25,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  57%|█████▋    | 286/500 [03:13<02:24,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  57%|█████▋    | 287/500 [03:13<02:23,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  58%|█████▊    | 288/500 [03:14<02:23,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  58%|█████▊    | 289/500 [03:14<02:22,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  58%|█████▊    | 290/500 [03:15<02:21,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  58%|█████▊    | 291/500 [03:16<02:20,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  58%|█████▊    | 292/500 [03:16<02:20,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  59%|█████▊    | 293/500 [03:17<02:19,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  59%|█████▉    | 294/500 [03:18<02:18,  1.48it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  59%|█████▉    | 295/500 [03:18<02:18,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  59%|█████▉    | 296/500 [03:19<02:17,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  59%|█████▉    | 297/500 [03:19<02:16,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  60%|█████▉    | 298/500 [03:20<02:15,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  60%|█████▉    | 299/500 [03:21<02:15,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  60%|██████    | 300/500 [03:21<02:14,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  60%|██████    | 301/500 [03:22<02:13,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  60%|██████    | 302/500 [03:22<02:13,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  61%|██████    | 303/500 [03:23<02:12,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  61%|██████    | 304/500 [03:24<02:11,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  61%|██████    | 305/500 [03:24<02:10,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  61%|██████    | 306/500 [03:25<02:10,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  61%|██████▏   | 307/500 [03:26<02:09,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  62%|██████▏   | 308/500 [03:26<02:08,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  62%|██████▏   | 309/500 [03:27<02:08,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  62%|██████▏   | 310/500 [03:27<02:07,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  62%|██████▏   | 311/500 [03:28<02:06,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  62%|██████▏   | 312/500 [03:29<02:06,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  63%|██████▎   | 313/500 [03:29<02:05,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  63%|██████▎   | 314/500 [03:30<02:04,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  63%|██████▎   | 315/500 [03:31<02:03,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  63%|██████▎   | 316/500 [03:31<02:03,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  63%|██████▎   | 317/500 [03:32<02:02,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  64%|██████▎   | 318/500 [03:33<02:01,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  64%|██████▍   | 319/500 [03:33<02:01,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  64%|██████▍   | 320/500 [03:34<02:00,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  64%|██████▍   | 321/500 [03:34<01:59,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  64%|██████▍   | 322/500 [03:35<01:59,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  65%|██████▍   | 323/500 [03:36<01:58,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  65%|██████▍   | 324/500 [03:36<01:57,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  65%|██████▌   | 325/500 [03:37<01:57,  1.49it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  65%|██████▌   | 326/500 [03:38<01:56,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  65%|██████▌   | 327/500 [03:38<01:55,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  66%|██████▌   | 328/500 [03:39<01:54,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  66%|██████▌   | 329/500 [03:39<01:54,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  66%|██████▌   | 330/500 [03:40<01:53,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  66%|██████▌   | 331/500 [03:41<01:52,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  66%|██████▋   | 332/500 [03:41<01:52,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  67%|██████▋   | 333/500 [03:42<01:51,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  67%|██████▋   | 334/500 [03:42<01:50,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  67%|██████▋   | 335/500 [03:43<01:50,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  67%|██████▋   | 336/500 [03:44<01:49,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  67%|██████▋   | 337/500 [03:44<01:48,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  68%|██████▊   | 338/500 [03:45<01:48,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  68%|██████▊   | 339/500 [03:46<01:47,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  68%|██████▊   | 340/500 [03:46<01:46,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  68%|██████▊   | 341/500 [03:47<01:45,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  68%|██████▊   | 342/500 [03:47<01:45,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  69%|██████▊   | 343/500 [03:48<01:44,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  69%|██████▉   | 344/500 [03:49<01:43,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  69%|██████▉   | 345/500 [03:49<01:43,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  69%|██████▉   | 346/500 [03:50<01:42,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  69%|██████▉   | 347/500 [03:51<01:41,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  70%|██████▉   | 348/500 [03:51<01:41,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  70%|██████▉   | 349/500 [03:52<01:40,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  70%|███████   | 350/500 [03:52<01:39,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  70%|███████   | 351/500 [03:53<01:39,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  70%|███████   | 352/500 [03:54<01:38,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  71%|███████   | 353/500 [03:54<01:37,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  71%|███████   | 354/500 [03:55<01:37,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  71%|███████   | 355/500 [03:55<01:36,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  71%|███████   | 356/500 [03:56<01:35,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  71%|███████▏  | 357/500 [03:57<01:35,  1.50it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  72%|███████▏  | 358/500 [03:57<01:34,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  72%|███████▏  | 359/500 [03:58<01:33,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  72%|███████▏  | 360/500 [03:59<01:32,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  72%|███████▏  | 361/500 [03:59<01:32,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  72%|███████▏  | 362/500 [04:00<01:31,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  73%|███████▎  | 363/500 [04:00<01:30,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  73%|███████▎  | 364/500 [04:01<01:30,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  73%|███████▎  | 365/500 [04:02<01:29,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  73%|███████▎  | 366/500 [04:02<01:28,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  73%|███████▎  | 367/500 [04:03<01:28,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  74%|███████▎  | 368/500 [04:03<01:27,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  74%|███████▍  | 369/500 [04:04<01:26,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  74%|███████▍  | 370/500 [04:05<01:26,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  74%|███████▍  | 371/500 [04:05<01:25,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  74%|███████▍  | 372/500 [04:06<01:24,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  75%|███████▍  | 373/500 [04:07<01:24,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  75%|███████▍  | 374/500 [04:07<01:23,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  75%|███████▌  | 375/500 [04:08<01:22,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  75%|███████▌  | 376/500 [04:08<01:22,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  75%|███████▌  | 377/500 [04:09<01:21,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  76%|███████▌  | 378/500 [04:10<01:20,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  76%|███████▌  | 379/500 [04:10<01:20,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  76%|███████▌  | 380/500 [04:11<01:19,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  76%|███████▌  | 381/500 [04:12<01:18,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  76%|███████▋  | 382/500 [04:12<01:18,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  77%|███████▋  | 383/500 [04:13<01:17,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  77%|███████▋  | 384/500 [04:13<01:16,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  77%|███████▋  | 385/500 [04:14<01:16,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  77%|███████▋  | 386/500 [04:15<01:15,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  77%|███████▋  | 387/500 [04:15<01:14,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  78%|███████▊  | 388/500 [04:16<01:14,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  78%|███████▊  | 389/500 [04:16<01:13,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  78%|███████▊  | 390/500 [04:17<01:12,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  78%|███████▊  | 391/500 [04:18<01:11,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  78%|███████▊  | 392/500 [04:18<01:11,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  79%|███████▊  | 393/500 [04:19<01:10,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  79%|███████▉  | 394/500 [04:20<01:09,  1.51it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  79%|███████▉  | 395/500 [04:20<01:09,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  79%|███████▉  | 396/500 [04:21<01:08,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  79%|███████▉  | 397/500 [04:21<01:07,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  80%|███████▉  | 398/500 [04:22<01:07,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  80%|███████▉  | 399/500 [04:23<01:06,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  80%|████████  | 400/500 [04:23<01:05,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  80%|████████  | 401/500 [04:24<01:05,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  80%|████████  | 402/500 [04:25<01:04,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  81%|████████  | 403/500 [04:25<01:03,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  81%|████████  | 404/500 [04:26<01:03,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  81%|████████  | 405/500 [04:27<01:02,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  81%|████████  | 406/500 [04:27<01:01,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  81%|████████▏ | 407/500 [04:28<01:01,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  82%|████████▏ | 408/500 [04:28<01:00,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  82%|████████▏ | 409/500 [04:29<00:59,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  82%|████████▏ | 410/500 [04:30<00:59,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  82%|████████▏ | 411/500 [04:30<00:58,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  82%|████████▏ | 412/500 [04:31<00:57,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  83%|████████▎ | 413/500 [04:32<00:57,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  83%|████████▎ | 414/500 [04:32<00:56,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  83%|████████▎ | 415/500 [04:33<00:55,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  83%|████████▎ | 416/500 [04:33<00:55,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  83%|████████▎ | 417/500 [04:34<00:54,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  84%|████████▎ | 418/500 [04:35<00:53,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  84%|████████▍ | 419/500 [04:35<00:53,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  84%|████████▍ | 420/500 [04:36<00:52,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  84%|████████▍ | 421/500 [04:37<00:51,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  84%|████████▍ | 422/500 [04:37<00:51,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  85%|████████▍ | 423/500 [04:38<00:50,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  85%|████████▍ | 424/500 [04:38<00:49,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  85%|████████▌ | 425/500 [04:39<00:49,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  85%|████████▌ | 426/500 [04:40<00:48,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  85%|████████▌ | 427/500 [04:40<00:48,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  86%|████████▌ | 428/500 [04:41<00:47,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  86%|████████▌ | 429/500 [04:42<00:46,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  86%|████████▌ | 430/500 [04:42<00:46,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  86%|████████▌ | 431/500 [04:43<00:45,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  86%|████████▋ | 432/500 [04:43<00:44,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  87%|████████▋ | 433/500 [04:44<00:44,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  87%|████████▋ | 434/500 [04:45<00:43,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  87%|████████▋ | 435/500 [04:45<00:42,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  87%|████████▋ | 436/500 [04:46<00:42,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  87%|████████▋ | 437/500 [04:46<00:41,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  88%|████████▊ | 438/500 [04:47<00:40,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  88%|████████▊ | 439/500 [04:48<00:40,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  88%|████████▊ | 440/500 [04:48<00:39,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  88%|████████▊ | 441/500 [04:49<00:38,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  88%|████████▊ | 442/500 [04:50<00:38,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  89%|████████▊ | 443/500 [04:50<00:37,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  89%|████████▉ | 444/500 [04:51<00:36,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  89%|████████▉ | 445/500 [04:51<00:36,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  89%|████████▉ | 446/500 [04:52<00:35,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  89%|████████▉ | 447/500 [04:53<00:34,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  90%|████████▉ | 448/500 [04:53<00:34,  1.52it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  90%|████████▉ | 449/500 [04:54<00:33,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  90%|█████████ | 450/500 [04:55<00:32,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  90%|█████████ | 451/500 [04:55<00:32,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  90%|█████████ | 452/500 [04:56<00:31,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  91%|█████████ | 453/500 [04:56<00:30,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  91%|█████████ | 454/500 [04:57<00:30,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  91%|█████████ | 455/500 [04:58<00:29,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  91%|█████████ | 456/500 [04:58<00:28,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  91%|█████████▏| 457/500 [04:59<00:28,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  92%|█████████▏| 458/500 [04:59<00:27,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  92%|█████████▏| 459/500 [05:00<00:26,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  92%|█████████▏| 460/500 [05:01<00:26,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  92%|█████████▏| 461/500 [05:01<00:25,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  92%|█████████▏| 462/500 [05:02<00:24,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  93%|█████████▎| 463/500 [05:03<00:24,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  93%|█████████▎| 464/500 [05:03<00:23,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  93%|█████████▎| 465/500 [05:04<00:22,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  93%|█████████▎| 466/500 [05:04<00:22,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  93%|█████████▎| 467/500 [05:05<00:21,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  94%|█████████▎| 468/500 [05:06<00:20,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  94%|█████████▍| 469/500 [05:06<00:20,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  94%|█████████▍| 470/500 [05:07<00:19,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  94%|█████████▍| 471/500 [05:08<00:18,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  94%|█████████▍| 472/500 [05:08<00:18,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  95%|█████████▍| 473/500 [05:09<00:17,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  95%|█████████▍| 474/500 [05:09<00:16,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  95%|█████████▌| 475/500 [05:10<00:16,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  95%|█████████▌| 476/500 [05:11<00:15,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  95%|█████████▌| 477/500 [05:11<00:15,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  96%|█████████▌| 478/500 [05:12<00:14,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  96%|█████████▌| 479/500 [05:13<00:13,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  96%|█████████▌| 480/500 [05:13<00:13,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  96%|█████████▌| 481/500 [05:14<00:12,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  96%|█████████▋| 482/500 [05:15<00:11,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  97%|█████████▋| 483/500 [05:15<00:11,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  97%|█████████▋| 484/500 [05:16<00:10,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  97%|█████████▋| 485/500 [05:16<00:09,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  97%|█████████▋| 486/500 [05:17<00:09,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  97%|█████████▋| 487/500 [05:18<00:08,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  98%|█████████▊| 488/500 [05:18<00:07,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  98%|█████████▊| 489/500 [05:19<00:07,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  98%|█████████▊| 490/500 [05:20<00:06,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  98%|█████████▊| 491/500 [05:20<00:05,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  98%|█████████▊| 492/500 [05:21<00:05,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  99%|█████████▊| 493/500 [05:21<00:04,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  99%|█████████▉| 494/500 [05:22<00:03,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  99%|█████████▉| 495/500 [05:23<00:03,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  99%|█████████▉| 496/500 [05:23<00:02,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0:  99%|█████████▉| 497/500 [05:24<00:01,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0: 100%|█████████▉| 498/500 [05:24<00:01,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0: 100%|█████████▉| 499/500 [05:25<00:00,  1.53it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 1\n",
      "Testing DataLoader 0: 100%|██████████| 500/500 [05:26<00:00,  1.53it/s][NeMo I 2024-12-10 14:12:57 megatron_gpt_sft_model:551] Total deduplicated inference data size: 500 to 500\n",
      "[NeMo I 2024-12-10 14:12:57 megatron_gpt_sft_model:702] Predictions saved to pubmedQA_result__test_pubmedqa_inputs_preds_labels.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 14:12:57 megatron_gpt_sft_model:642] No training data found, reconfiguring microbatches based on validation batch sizes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of micro-batches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 14:12:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-10 14:12:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('test_loss_pubmedqa', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-10 14:12:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 500/500 [05:26<00:00,  1.53it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.15388180315494537   \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m   test_loss_pubmedqa    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.15388180315494537   \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.15388180315494537   \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "PRECISION=bf16-mixed\n",
    "MODEL=\"models/llama-3.1-nemotron/70B/nemo/Llama-3_1-Nemotron-70B-Instruct.nemo\"\n",
    "OUTPUT_DIR=\"results/llama-3.1-nemotron/70B/$PRECISION\"\n",
    "TEST_DS=\"[pubmedqa/data/pubmedqa_test.jsonl]\"\n",
    "TEST_NAMES=\"[pubmedqa]\"\n",
    "SCHEME=\"lora\"\n",
    "GPUS=4\n",
    "TP_SIZE=4\n",
    "PP_SIZE=1\n",
    "\n",
    "# This is where your LoRA checkpoint was saved\n",
    "PATH_TO_TRAINED_MODEL=\"$OUTPUT_DIR/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "\n",
    "# The generation run will save the generated outputs over the test dataset in a file prefixed like so\n",
    "OUTPUT_PREFIX=\"pubmedQA_result_\"\n",
    "\n",
    "export TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "torchrun --nproc_per_node=${GPUS} \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.peft.restore_from_path=${PATH_TO_TRAINED_MODEL} \\\n",
    "    trainer.devices=${GPUS} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    model.data.test_ds.file_names=${TEST_DS} \\\n",
    "    model.data.test_ds.names=${TEST_NAMES} \\\n",
    "    model.data.test_ds.global_batch_size=1 \\\n",
    "    model.data.test_ds.micro_batch_size=1 \\\n",
    "    model.data.test_ds.tokens_to_generate=3 \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.output_file_path_prefix=${OUTPUT_PREFIX} \\\n",
    "    model.data.test_ds.write_predictions_to_file=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe048f9",
   "metadata": {},
   "source": [
    "### Step 4: Check the model accuracy\n",
    "\n",
    "Now that the results are in, let's read the results and calculate the accuracy on the pubmedQA task. You can compare your accuracy results with the public leaderboard at https://pubmedqa.github.io/.\n",
    "\n",
    "Let's take a look at one of the predictions in the generated output file. The `pred` key indicates what was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa5c0fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"OBJECTIVES: Outcome feedback is the process of learning patient outcomes after their care within the emergency department. We conducted a national survey of Canadian Royal College emergency medicine (EM) residents and program directors to determine the extent to which active outcome feedback and follow-up occurred. We also compared the perceived educational value of outcome feedback between residents and program directors.\\nMETHODS: We distributed surveys to all Royal College-accredited adult and pediatric EM training programs using a modified Dillman method. We analyzed the data using student's t-test for continuous variables and Fisher's exact test for categorical variables.\\nRESULTS: We received 210 completed surveys from 260 eligible residents (80.8%) and 21 of 24 program directors (87.5%) (overall 81.3%). Mandatory active outcome feedback was not present in any EM training program for admitted or discharged patients (0/21). Follow-up was performed electively by 89.4% of residents for patients admitted to the hospital, and by 44.2% of residents for patients discharged home. A majority of residents (76.9%) believed that patient follow-up should be mandatory compared to 42.9% of program directors (p=0.002). The perceived educational value of outcome feedback was 5.8/7 for residents and 5.1/7 for program directors (difference 0.7; p=0.002) based on a seven-point Likert scale (1=not important; 7=very important).\\nQUESTION: Outcome Feedback within Emergency Medicine Training Programs: An Opportunity to Apply the Theory of Deliberate Practice?\\n ### ANSWER (yes|no|maybe):\", \"pred\": \" <<< yes >>>\", \"label\": \" <<< maybe >>>\"}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "tail -n 1 pubmedQA_result__test_pubmedqa_inputs_preds_labels.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c91df7",
   "metadata": {},
   "source": [
    "Note that the model produces output in the specified format, such as `<<< no >>>`.\n",
    "\n",
    "The following snippet loads the generated output and calculates accuracy in comparison to the test set using the `evaluation.py` script included in the PubMedQA repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "900f81c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "answers = []\n",
    "with open(\"pubmedQA_result__test_pubmedqa_inputs_preds_labels.jsonl\",'rt') as f:\n",
    "    st = f.readline()\n",
    "    while st:\n",
    "        answers.append(json.loads(st))\n",
    "        st = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74e1bbce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_test = json.load(open(\"./pubmedqa/data/test_set.json\",'rt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a85926e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "sample_id = list(data_test.keys())\n",
    "\n",
    "for i, key in enumerate(sample_id):\n",
    "    answer = answers[i]['pred']\n",
    "    if 'yes' in answer:\n",
    "        results[key] = 'yes'\n",
    "    elif 'no' in answer:\n",
    "        results[key] = 'no'\n",
    "    elif 'maybe' in answer:\n",
    "        results[key] = 'maybe'\n",
    "    else:\n",
    "        print(\"Malformed answer: \", answer)\n",
    "        results[key] = 'maybe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "768074cf-d189-4b19-bf28-dc7149029ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.796000\n",
      "Macro-F1 0.570244\n"
     ]
    }
   ],
   "source": [
    "# Dump results in a format that can be ingested by PubMedQA evaluation file\n",
    "FILENAME=\"pubmedqa-llama-3-70b-lora.json\"\n",
    "with(open(FILENAME, \"w\")) as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Evaluation\n",
    "!cp $FILENAME ./pubmedqa/\n",
    "!cd ./pubmedqa/ && python evaluation.py $FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f34d17-55d7-4001-8530-673457321114",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Export Model to TensorRT-LLM Format for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b1f4b23-90d7-42be-832b-b46f3c6ec198",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-10 14:39:10 nemo_logging:349] /tmp/ipykernel_730/1971355109.py:11: DeprecationWarning: Parameter n_gpus is deprecated and will be removed in the next release. Please use tensor_parallelism_size and pipeline_parallelism_size parameters instead.\n",
      "      trt_llm_exporter.export(\n",
      "    \n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "saving weights: 100%|██████████| 481/481 [03:09<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:45:24] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:45:24.515959 139999580046464 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:45:24] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:45:24.517820 139999580046464 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:45:24] [TRT-LLM] [I] Set multi_block_mode to False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:45:24.518685 139999580046464 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:45:24] [TRT-LLM] [I] Set paged_kv_cache to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:45:24.519243 139999580046464 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:45:24] [TRT-LLM] [I] Set tokens_per_block to 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:45:24.519765 139999580046464 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:45:24] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
      "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 14:45:24.520328 139999580046464 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
      "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:45:24] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 14:45:24.520862 139999580046464 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:39] [TRT] [I] [MemUsageChange] Init CUDA: CPU +16, GPU +0, now: CPU 423065, GPU 528 (MiB)\n",
      "[12/10/2024-14:46:45] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4312, GPU +1150, now: CPU 427513, GPU 1678 (MiB)\n",
      "[12/10/2024-14:46:45] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n",
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.100662 139999580046464 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.101771 139999580046464 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.723005 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.724067 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.724715 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.725315 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.725916 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.726485 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.727061 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.727629 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.728212 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.728749 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.729308 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.729916 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.730469 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.731508 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.732935 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.733507 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.16.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.734072 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.16.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.17.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.734632 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.17.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.18.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.735191 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.18.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.19.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.735736 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.19.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.20.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.736295 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.20.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.21.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.736810 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.21.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.22.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.738280 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.22.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.23.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.739035 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.23.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.24.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.739575 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.24.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.25.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.740158 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.25.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.26.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.740705 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.26.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.27.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.741308 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.27.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.28.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.741841 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.28.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.29.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.742399 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.29.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.30.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.742977 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.30.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.31.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.743518 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.31.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.32.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.744076 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.32.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.33.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.744623 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.33.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.34.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.745176 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.34.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.35.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.745734 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.35.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.36.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.746273 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.36.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.37.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.746827 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.37.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.38.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.747391 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.38.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.39.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.747920 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.39.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.40.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.748481 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.40.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.41.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.749029 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.41.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.42.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.749550 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.42.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.43.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.750087 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.43.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.44.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.750609 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.44.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.45.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.751127 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.45.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.46.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.751658 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.46.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.47.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.752209 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.47.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.48.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.752729 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.48.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.49.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.753320 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.49.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.50.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.753901 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.50.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.51.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.754459 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.51.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.52.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.755016 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.52.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.53.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.755560 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.53.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.54.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.756096 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.54.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.55.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.756646 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.55.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.56.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.757173 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.56.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.57.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.757739 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.57.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.58.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.758271 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.58.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.59.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.758808 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.59.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.60.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.759369 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.60.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.61.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.759902 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.61.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.62.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.760447 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.62.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.63.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.760957 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.63.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.64.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.761473 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.64.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.65.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.762000 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.65.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.66.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.762529 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.66.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.67.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.763062 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.67.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.68.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.763626 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.68.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.69.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.764171 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.69.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.70.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.764695 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.70.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.71.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.765227 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.71.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.72.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.765876 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.72.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.73.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.766418 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.73.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.74.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.766977 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.74.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.75.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.767523 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.75.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.76.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.768057 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.76.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.77.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.768594 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.77.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.78.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.774603 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.78.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Parameter transformer.layers.79.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.775199 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.79.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:46:45.775758 139999580046464 logger.py:92] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:46:45] [TRT] [W] Unused Input: position_ids\n",
      "[12/10/2024-14:46:45] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[12/10/2024-14:46:45] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[12/10/2024-14:46:57] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[12/10/2024-14:46:57] [TRT] [I] Detected 15 inputs and 5 output network tensors.\n",
      "[12/10/2024-14:47:16] [TRT] [I] Total Host Persistent Memory: 305280\n",
      "[12/10/2024-14:47:16] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[12/10/2024-14:47:16] [TRT] [I] Total Scratch Memory: 67117056\n",
      "[12/10/2024-14:47:16] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 1617 steps to complete.\n",
      "[12/10/2024-14:47:16] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 82.6494ms to assign 17 blocks to 1617 nodes requiring 251672576 bytes.\n",
      "[12/10/2024-14:47:16] [TRT] [I] Total Activation Memory: 251671552\n",
      "[12/10/2024-14:47:16] [TRT] [I] Total Weights Memory: 36937678848\n",
      "[12/10/2024-14:47:16] [TRT] [I] Engine generation completed in 30.6111 seconds.\n",
      "[12/10/2024-14:47:16] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 4032 MiB, GPU 35226 MiB\n",
      "[12/10/2024-14:47:32] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 412548 MiB\n",
      "[12/10/2024-14:47:32] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:47:32.935170 139999580046464 logger.py:92] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:47:32] [TRT-LLM] [I] Serializing engine to models/llama-3.1-nemotron/70B/trt_llm/rank0.engine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:47:32.942047 139999580046464 logger.py:92] [TRT-LLM] [I] Serializing engine to models/llama-3.1-nemotron/70B/trt_llm/rank0.engine...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:47:52] [TRT-LLM] [I] Engine serialized. Total time: 00:00:19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:47:52.927093 139999580046464 logger.py:92] [TRT-LLM] [I] Engine serialized. Total time: 00:00:19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:47:55] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:47:55.815099 139999580046464 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:47:55] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:47:55.816414 139999580046464 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:47:55] [TRT-LLM] [I] Set multi_block_mode to False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:47:55.816962 139999580046464 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:47:55] [TRT-LLM] [I] Set paged_kv_cache to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:47:55.817445 139999580046464 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:47:55] [TRT-LLM] [I] Set tokens_per_block to 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:47:55.817957 139999580046464 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:47:55] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
      "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 14:47:55.818501 139999580046464 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
      "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:47:55] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 14:47:55.818986 139999580046464 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:11] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 433067, GPU 1764 (MiB)\n",
      "[12/10/2024-14:49:11] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:11.673665 139999580046464 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:11] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n",
      "[12/10/2024-14:49:11] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:11.675464 139999580046464 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:11] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.214994 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.216323 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.216948 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.217521 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.218111 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.219141 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.219693 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.220276 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.220879 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.221455 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.222030 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.222585 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.223142 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.223692 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.224219 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.224749 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.16.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.225296 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.16.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.17.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.225846 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.17.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.18.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.226379 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.18.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.19.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.226922 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.19.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.20.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.227445 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.20.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.21.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.227985 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.21.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.22.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.228502 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.22.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.23.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.229031 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.23.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.24.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.229557 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.24.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.25.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.230095 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.25.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.26.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.230611 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.26.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.27.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.231179 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.27.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.28.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.231689 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.28.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.29.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.232213 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.29.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.30.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.232759 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.30.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.31.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.233258 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.31.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.32.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.233812 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.32.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.33.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.234361 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.33.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.34.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.234897 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.34.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.35.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.238439 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.35.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.36.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.239149 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.36.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.37.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.239726 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.37.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.38.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.240578 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.38.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.39.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.241140 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.39.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.40.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.241673 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.40.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.41.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.242236 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.41.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.42.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.242744 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.42.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.43.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.243291 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.43.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.44.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.243832 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.44.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.45.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.245110 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.45.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.46.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.245652 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.46.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.47.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.246177 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.47.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.48.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.246689 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.48.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.49.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.247235 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.49.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.50.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.247756 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.50.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.51.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.248285 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.51.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.52.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.248822 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.52.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.53.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.249367 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.53.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.54.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.249886 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.54.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.55.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.250402 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.55.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.56.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.250918 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.56.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.57.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.251436 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.57.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.58.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.251971 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.58.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.59.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.252484 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.59.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.60.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.253012 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.60.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.61.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.253529 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.61.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.62.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.254039 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.62.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.63.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.254561 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.63.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.64.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.255112 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.64.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.65.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.255633 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.65.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.66.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.256168 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.66.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.67.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.256691 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.67.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.68.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.257224 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.68.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.69.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.257733 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.69.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.70.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.258264 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.70.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.71.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.258784 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.71.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.72.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.259323 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.72.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.73.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.259815 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.73.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.74.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.260344 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.74.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.75.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.263870 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.75.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.76.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.264416 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.76.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.77.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.264948 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.77.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.78.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.265493 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.78.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Parameter transformer.layers.79.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.266012 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.79.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:49:12.266564 139999580046464 logger.py:92] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:49:12] [TRT] [W] Unused Input: position_ids\n",
      "[12/10/2024-14:49:12] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[12/10/2024-14:49:12] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[12/10/2024-14:49:25] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[12/10/2024-14:49:25] [TRT] [I] Detected 15 inputs and 5 output network tensors.\n",
      "[12/10/2024-14:49:42] [TRT] [I] Total Host Persistent Memory: 305280\n",
      "[12/10/2024-14:49:42] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[12/10/2024-14:49:42] [TRT] [I] Total Scratch Memory: 67117056\n",
      "[12/10/2024-14:49:42] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 1617 steps to complete.\n",
      "[12/10/2024-14:49:42] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 82.1465ms to assign 17 blocks to 1617 nodes requiring 251672576 bytes.\n",
      "[12/10/2024-14:49:42] [TRT] [I] Total Activation Memory: 251671552\n",
      "[12/10/2024-14:49:42] [TRT] [I] Total Weights Memory: 36937678848\n",
      "[12/10/2024-14:49:42] [TRT] [I] Engine generation completed in 30.3929 seconds.\n",
      "[12/10/2024-14:49:42] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 4032 MiB, GPU 35226 MiB\n",
      "[12/10/2024-14:50:03] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 417690 MiB\n",
      "[12/10/2024-14:50:03] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:50:03.740431 139999580046464 logger.py:92] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:50:03] [TRT-LLM] [I] Serializing engine to models/llama-3.1-nemotron/70B/trt_llm/rank1.engine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:50:03.745454 139999580046464 logger.py:92] [TRT-LLM] [I] Serializing engine to models/llama-3.1-nemotron/70B/trt_llm/rank1.engine...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:50:28] [TRT-LLM] [I] Engine serialized. Total time: 00:00:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:50:28.277422 139999580046464 logger.py:92] [TRT-LLM] [I] Engine serialized. Total time: 00:00:24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:50:31] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:50:31.216939 139999580046464 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:50:31] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:50:31.219235 139999580046464 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:50:31] [TRT-LLM] [I] Set multi_block_mode to False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:50:31.220340 139999580046464 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:50:31] [TRT-LLM] [I] Set paged_kv_cache to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:50:31.220829 139999580046464 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:50:31] [TRT-LLM] [I] Set tokens_per_block to 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:50:31.221545 139999580046464 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:50:31] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
      "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 14:50:31.222020 139999580046464 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
      "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:50:31] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 14:50:31.222528 139999580046464 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:07] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 433068, GPU 1764 (MiB)\n",
      "[12/10/2024-14:51:07] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:07.784151 139999580046464 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:07] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n",
      "[12/10/2024-14:51:07] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:07.786616 139999580046464 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.321146 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.322551 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.323274 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.323924 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.324608 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.325243 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.325892 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.327007 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.327620 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.328361 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.329011 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.329670 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.330280 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.330902 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.331507 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.332121 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.16.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.332759 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.16.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.17.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.333380 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.17.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.18.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.333992 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.18.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.19.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.334624 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.19.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.20.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.336239 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.20.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.21.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.336885 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.21.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.22.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.337494 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.22.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.23.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.338116 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.23.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.24.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.338730 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.24.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.25.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.339352 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.25.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.26.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.339977 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.26.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.27.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.340584 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.27.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.28.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.341235 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.28.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.29.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.341819 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.29.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.30.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.342453 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.30.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.31.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.343135 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.31.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.32.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.343737 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.32.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.33.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.344348 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.33.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.34.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.344969 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.34.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.35.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.345816 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.35.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.36.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.346443 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.36.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.37.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.347060 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.37.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.38.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.347657 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.38.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.39.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.348290 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.39.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.40.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.348900 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.40.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.41.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.351215 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.41.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.42.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.351829 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.42.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.43.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.352481 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.43.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.44.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.353208 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.44.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.45.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.353834 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.45.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.46.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.354438 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.46.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.47.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.355059 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.47.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.48.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.355674 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.48.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.49.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.356340 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.49.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.50.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.356945 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.50.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.51.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.357561 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.51.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.52.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.359235 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.52.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.53.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.359869 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.53.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.54.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.360531 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.54.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.55.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.361173 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.55.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.56.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.363218 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.56.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.57.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.363832 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.57.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.58.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.364482 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.58.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.59.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.365218 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.59.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.60.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.365838 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.60.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.61.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.366447 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.61.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.62.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.367039 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.62.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.63.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.367661 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.63.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.64.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.368282 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.64.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.65.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.368860 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.65.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.66.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.369497 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.66.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.67.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.370081 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.67.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.68.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.370690 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.68.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.69.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.371311 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.69.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.70.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.371918 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.70.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.71.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.372529 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.71.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.72.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.373127 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.72.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.73.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.373737 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.73.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.74.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.374333 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.74.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.75.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.374936 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.75.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.76.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.375533 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.76.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.77.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.376116 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.77.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.78.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.376737 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.78.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Parameter transformer.layers.79.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.377318 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.79.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:08.377989 139999580046464 logger.py:92] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:08] [TRT] [W] Unused Input: position_ids\n",
      "[12/10/2024-14:51:08] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[12/10/2024-14:51:08] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[12/10/2024-14:51:18] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[12/10/2024-14:51:18] [TRT] [I] Detected 15 inputs and 5 output network tensors.\n",
      "[12/10/2024-14:51:34] [TRT] [I] Total Host Persistent Memory: 305344\n",
      "[12/10/2024-14:51:34] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[12/10/2024-14:51:34] [TRT] [I] Total Scratch Memory: 67117056\n",
      "[12/10/2024-14:51:34] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 1619 steps to complete.\n",
      "[12/10/2024-14:51:34] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 83.1084ms to assign 17 blocks to 1619 nodes requiring 251672576 bytes.\n",
      "[12/10/2024-14:51:34] [TRT] [I] Total Activation Memory: 251671552\n",
      "[12/10/2024-14:51:34] [TRT] [I] Total Weights Memory: 36937678848\n",
      "[12/10/2024-14:51:34] [TRT] [I] Engine generation completed in 25.7937 seconds.\n",
      "[12/10/2024-14:51:34] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 4032 MiB, GPU 35226 MiB\n",
      "[12/10/2024-14:51:39] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 417697 MiB\n",
      "[12/10/2024-14:51:39] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:39.475409 139999580046464 logger.py:92] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:51:39] [TRT-LLM] [I] Serializing engine to models/llama-3.1-nemotron/70B/trt_llm/rank2.engine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:51:39.479081 139999580046464 logger.py:92] [TRT-LLM] [I] Serializing engine to models/llama-3.1-nemotron/70B/trt_llm/rank2.engine...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:00] [TRT-LLM] [I] Engine serialized. Total time: 00:00:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:00.103983 139999580046464 logger.py:92] [TRT-LLM] [I] Engine serialized. Total time: 00:00:20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:00] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:00.822478 139999580046464 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:00] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:00.825022 139999580046464 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:00] [TRT-LLM] [I] Set multi_block_mode to False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:00.825633 139999580046464 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:00] [TRT-LLM] [I] Set paged_kv_cache to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:00.826249 139999580046464 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:00] [TRT-LLM] [I] Set tokens_per_block to 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:00.826817 139999580046464 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:00] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
      "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 14:52:00.828420 139999580046464 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
      "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:00] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 14:52:00.829135 139999580046464 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "[12/10/2024-14:52:07] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 433070, GPU 1764 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.347326 139999580046464 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n",
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.348982 139999580046464 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.990309 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.991483 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.992201 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.993193 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.993812 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.994467 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.995064 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.995667 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.996292 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.996880 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.997497 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.998110 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.998737 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:07] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:07.999467 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.000076 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.000676 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.16.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.001293 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.16.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.17.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.001872 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.17.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.18.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.004095 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.18.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.19.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.004711 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.19.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.20.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.005301 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.20.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.21.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.005894 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.21.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.22.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.006503 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.22.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.23.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.007137 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.23.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.24.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.007736 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.24.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.25.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.008334 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.25.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.26.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.008957 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.26.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.27.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.009530 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.27.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.28.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.010120 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.28.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.29.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.010698 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.29.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.30.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.011320 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.30.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.31.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.011945 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.31.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.32.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.012536 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.32.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.33.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.013139 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.33.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.34.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.013739 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.34.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.35.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.014312 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.35.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.36.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.015142 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.36.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.37.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.015749 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.37.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.38.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.016365 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.38.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.39.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.016984 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.39.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.40.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.017555 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.40.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.41.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.018166 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.41.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.42.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.018755 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.42.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.43.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.019361 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.43.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.44.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.020554 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.44.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.45.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.021195 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.45.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.46.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.021785 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.46.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.47.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.022382 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.47.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.48.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.022972 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.48.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.49.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.023591 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.49.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.50.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.024177 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.50.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.51.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.024784 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.51.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.52.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.025355 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.52.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.53.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.025957 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.53.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.54.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.026561 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.54.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.55.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.030355 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.55.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.56.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.031123 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.56.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.57.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.031727 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.57.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.58.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.032334 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.58.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.59.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.032935 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.59.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.60.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.033537 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.60.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.61.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.034141 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.61.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.62.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.034725 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.62.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.63.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.035343 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.63.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.64.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.035930 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.64.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.65.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.036548 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.65.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.66.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.037152 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.66.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.67.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.038928 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.67.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.68.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.039566 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.68.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.69.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.040164 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.69.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.70.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.040759 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.70.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.71.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.041342 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.71.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.72.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.041944 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.72.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.73.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.042558 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.73.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.74.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.043127 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.74.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.75.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.043737 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.75.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.76.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.044358 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.76.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.77.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.045000 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.77.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.78.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.045611 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.78.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Parameter transformer.layers.79.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.046214 139999580046464 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.79.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:08.046862 139999580046464 logger.py:92] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:08] [TRT] [W] Unused Input: position_ids\n",
      "[12/10/2024-14:52:08] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[12/10/2024-14:52:08] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[12/10/2024-14:52:18] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[12/10/2024-14:52:18] [TRT] [I] Detected 15 inputs and 5 output network tensors.\n",
      "[12/10/2024-14:52:33] [TRT] [I] Total Host Persistent Memory: 305280\n",
      "[12/10/2024-14:52:33] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[12/10/2024-14:52:33] [TRT] [I] Total Scratch Memory: 67117056\n",
      "[12/10/2024-14:52:33] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 1617 steps to complete.\n",
      "[12/10/2024-14:52:33] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 82.283ms to assign 17 blocks to 1617 nodes requiring 251672576 bytes.\n",
      "[12/10/2024-14:52:33] [TRT] [I] Total Activation Memory: 251671552\n",
      "[12/10/2024-14:52:33] [TRT] [I] Total Weights Memory: 36937678848\n",
      "[12/10/2024-14:52:33] [TRT] [I] Engine generation completed in 25.607 seconds.\n",
      "[12/10/2024-14:52:33] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 4032 MiB, GPU 35226 MiB\n",
      "[12/10/2024-14:52:49] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 417704 MiB\n",
      "[12/10/2024-14:52:49] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:49.608895 139999580046464 logger.py:92] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:52:49] [TRT-LLM] [I] Serializing engine to models/llama-3.1-nemotron/70B/trt_llm/rank3.engine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:52:49.613179 139999580046464 logger.py:92] [TRT-LLM] [I] Serializing engine to models/llama-3.1-nemotron/70B/trt_llm/rank3.engine...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:53:13] [TRT-LLM] [I] Engine serialized. Total time: 00:00:23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 14:53:13.333480 139999580046464 logger.py:92] [TRT-LLM] [I] Engine serialized. Total time: 00:00:23\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-14:53:22] [TRT-LLM] [W] Found pynvml==11.4.1. Please use pynvml>=11.5.0 to get accurate memory usage\n",
      "[12/10/2024-14:53:22] [TRT-LLM] [W] Found pynvml==11.4.1. Please use pynvml>=11.5.0 to get accurate memory usage\n",
      "[12/10/2024-14:53:22] [TRT-LLM] [W] Found pynvml==11.4.1. Please use pynvml>=11.5.0 to get accurate memory usage\n",
      "[12/10/2024-14:53:22] [TRT-LLM] [W] Found pynvml==11.4.1. Please use pynvml>=11.5.0 to get accurate memory usage\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.10.0\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.10.0\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.10.0\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.10.0\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set smooth_quant_gemm_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set identity_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set layernorm_quantization_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set rmsnorm_quantization_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set lookup_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set lora_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set weight_only_groupwise_quant_matmul_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set quantize_per_token_plugin to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set quantize_tensor_plugin to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set moe_plugin to float16.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set context_fmha to True.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set remove_input_padding to True.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set multi_block_mode to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set enable_xqa to True.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set multiple_profiles to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set paged_state to True.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set streamingllm to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set smooth_quant_gemm_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set identity_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set layernorm_quantization_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set rmsnorm_quantization_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set lookup_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set lora_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set weight_only_groupwise_quant_matmul_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to None.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set quantize_per_token_plugin to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set quantize_tensor_plugin to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set moe_plugin to float16.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set context_fmha to True.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set remove_input_padding to True.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set multi_block_mode to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set enable_xqa to True.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set multiple_profiles to False.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set paged_state to True.\n",
      "[12/10/2024-14:53:34] [TRT-LLM] [I] Set streamingllm to False.\n",
      "[12/10/2024-14:53:34] [TRT] [I] Loaded engine size: 35234 MiB\n",
      "[12/10/2024-14:53:35] [TRT] [I] Loaded engine size: 35234 MiB\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set smooth_quant_gemm_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set identity_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set layernorm_quantization_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set rmsnorm_quantization_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set lookup_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set lora_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set weight_only_groupwise_quant_matmul_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set quantize_per_token_plugin to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set quantize_tensor_plugin to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set moe_plugin to float16.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set context_fmha to True.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set remove_input_padding to True.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set multi_block_mode to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set enable_xqa to True.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set multiple_profiles to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set paged_state to True.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set streamingllm to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set smooth_quant_gemm_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set identity_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set layernorm_quantization_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set rmsnorm_quantization_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set lookup_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set lora_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set weight_only_groupwise_quant_matmul_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to None.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set quantize_per_token_plugin to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set quantize_tensor_plugin to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set moe_plugin to float16.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set context_fmha to True.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set remove_input_padding to True.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set multi_block_mode to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set enable_xqa to True.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set multiple_profiles to False.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set paged_state to True.\n",
      "[12/10/2024-14:53:40] [TRT-LLM] [I] Set streamingllm to False.\n",
      "[12/10/2024-14:53:41] [TRT] [I] Loaded engine size: 35234 MiB\n",
      "[12/10/2024-14:53:41] [TRT] [I] Loaded engine size: 35234 MiB\n",
      "[TensorRT-LLM][INFO] Detecting local TP group for rank 2\n",
      "[TensorRT-LLM][INFO] Detecting local TP group for rank 0\n",
      "[TensorRT-LLM][INFO] Detecting local TP group for rank 1\n",
      "[TensorRT-LLM][INFO] Detecting local TP group for rank 3\n",
      "[TensorRT-LLM][INFO] TP group is intra-node for rank 2\n",
      "[TensorRT-LLM][INFO] TP group is intra-node for rank 0\n",
      "[TensorRT-LLM][INFO] TP group is intra-node for rank 1\n",
      "[TensorRT-LLM][INFO] TP group is intra-node for rank 3\n",
      "[12/10/2024-14:53:50] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n",
      "[12/10/2024-14:53:50] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n",
      "[12/10/2024-14:53:50] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n",
      "[12/10/2024-14:53:50] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n",
      "[12/10/2024-14:53:51] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n",
      "[12/10/2024-14:53:51] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n",
      "[12/10/2024-14:53:51] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n",
      "[12/10/2024-14:53:51] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n",
      "[12/10/2024-14:53:51] [TRT-LLM] [W] The paged KV cache in Python runtime is experimental. For performance and correctness, please, use C++ runtime.\n",
      "[12/10/2024-14:53:51] [TRT-LLM] [I] Load engine takes: 28.244981050491333 sec\n",
      "[12/10/2024-14:53:51] [TRT-LLM] [W] The paged KV cache in Python runtime is experimental. For performance and correctness, please, use C++ runtime.\n",
      "[12/10/2024-14:53:51] [TRT-LLM] [I] Load engine takes: 28.251206636428833 sec\n",
      "[12/10/2024-14:53:51] [TRT-LLM] [W] The paged KV cache in Python runtime is experimental. For performance and correctness, please, use C++ runtime.\n",
      "[12/10/2024-14:53:51] [TRT-LLM] [W] The paged KV cache in Python runtime is experimental. For performance and correctness, please, use C++ runtime.\n",
      "[12/10/2024-14:53:51] [TRT-LLM] [I] Load engine takes: 28.20422649383545 sec\n",
      "[12/10/2024-14:53:51] [TRT-LLM] [I] Load engine takes: 28.200499296188354 sec\n"
     ]
    }
   ],
   "source": [
    "from nemo.export.tensorrt_llm import TensorRTLLM\n",
    "\n",
    "PRECISION=bf16-mixed\n",
    "MODEL_DIR=\"models/llama-3.1-nemotron/70B/trt_llm\"\n",
    "MODEL_CKPT=\"models/llama-3.1-nemotron/70B/nemo/Llama-3_1-Nemotron-70B-Instruct.nemo\"\n",
    "LORA_CKPT=\"results/llama-3.1-nemotron/70B/$PRECISION/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "\n",
    "trt_llm_exporter = TensorRTLLM(\n",
    "    model_dir=MODEL_DIR,\n",
    "    lora_ckpt_list=[LORA_CKPT],\n",
    ")\n",
    "\n",
    "trt_llm_exporter.export(\n",
    "    nemo_checkpoint_path=MODEL_CKPT,\n",
    "    model_type=\"llama\",\n",
    "    n_gpus=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c6dad-1f74-4342-84d9-0b385c34e8dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trt_llm_exporter.forward(\n",
    "    [\"Comment about extended aortic replacement in acute type A dissection.\"],\n",
    "    max_output_token=150,\n",
    "    top_k=1,\n",
    "    top_p=0.0,\n",
    "    temperature=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07432d4-3160-42c5-8124-3c356f5216bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ucloud/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1211 09:40:31.525615 140666415416448 logger.py:92] [TRT-LLM] [W] Found pynvml==11.4.1. Please use pynvml>=11.5.0 to get accurate memory usage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 09:40:32.337549 140666415416448 deploy_triton.py:344] Logging level set to 20\n",
      "I1211 09:40:32.337709 140666415416448 deploy_triton.py:345] Namespace(nemo_checkpoint='models/llama-3.1-nemotron/70B/nemo/Llama-3_1-Nemotron-70B-Instruct.nemo', ptuning_nemo_checkpoint=None, task_ids=None, model_type='llama', triton_model_name='llama3-finetuned', triton_model_version=1, triton_port=8000, triton_http_address='0.0.0.0', triton_request_timeout=60, triton_model_repository='models/llama-3.1-nemotron/70B/trt_llm', num_gpus=4, tensor_parallelism_size=4, pipeline_parallelism_size=1, dtype='bfloat16', max_input_len=256, max_output_len=256, max_batch_size=8, max_num_tokens=None, opt_num_tokens=None, max_prompt_embedding_table_size=None, no_paged_kv_cache=False, disable_remove_input_padding=False, use_parallel_embedding=False, multi_block_mode=False, enable_streaming=False, use_lora_plugin=None, lora_target_modules=None, max_lora_rank=64, lora_ckpt=['results/llama-3.1-nemotron/70B/bf16-mixed/checkpoints/megatron_gpt_peft_lora_tuning.nemo'], use_cpp_runtime=False, backend='TensorRT-LLM', start_rest_service=False, service_http_address='0.0.0.0', service_port=8080, openai_format_response=False, debug_mode=False)\n",
      "I1211 09:40:32.337767 140666415416448 deploy_triton.py:280] Export operation will be started to export the nemo checkpoint to TensorRT-LLM.\n",
      "[NeMo W 2024-12-11 09:40:32 nemo_logging:349] /opt/NeMo/scripts/deploy/nlp/deploy_triton.py:281: DeprecationWarning: Parameter n_gpus is deprecated and will be removed in the next release. Please use tensor_parallelism_size and pipeline_parallelism_size parameters instead.\n",
      "      trt_llm_exporter.export(\n",
      "    \n",
      "[NeMo W 2024-12-11 09:43:20 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded mpi lib /usr/local/lib/python3.10/dist-packages/mpi4py/MPI.cpython-310-x86_64-linux-gnu.so successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "saving weights: 100%|██████████| 481/481 [06:35<00:00,  1.22it/s]\n",
      "I1211 09:50:17.085886 140666415416448 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
      "I1211 09:50:17.086187 140666415416448 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
      "I1211 09:50:17.086217 140666415416448 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n",
      "I1211 09:50:17.086241 140666415416448 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "I1211 09:50:17.086260 140666415416448 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
      "W1211 09:50:17.086288 140666415416448 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
      "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n",
      "W1211 09:50:17.086309 140666415416448 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-09:53:35] [TRT] [I] [MemUsageChange] Init CUDA: CPU +16, GPU +0, now: CPU 279691, GPU 528 (MiB)\n",
      "[12/11/2024-09:53:41] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4312, GPU +1150, now: CPU 284139, GPU 1678 (MiB)\n",
      "[12/11/2024-09:53:41] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 09:53:41.346775 140666415416448 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "I1211 09:53:41.347060 140666415416448 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:53:41] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 09:53:41.964361 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.964623 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.964730 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.964814 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.964894 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.964977 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.965055 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.965134 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.965207 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.965286 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.965360 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.965430 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.965504 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.965576 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.965646 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.965715 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.965788 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.16.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.965858 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.17.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.965934 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.18.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.966005 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.19.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.966076 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.20.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.966146 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.21.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.966215 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.22.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.966286 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.23.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.966355 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.24.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.966425 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.25.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.966496 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.26.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.966588 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.27.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.966659 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.28.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.966728 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.29.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.966815 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.30.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.966888 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.31.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.966965 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.32.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.967037 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.33.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.967130 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.34.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.967203 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.35.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.967273 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.36.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.967343 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.37.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.967410 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.38.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.967481 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.39.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.967546 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.40.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.967614 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.41.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.967678 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.42.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.967744 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.43.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.967811 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.44.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.967875 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.45.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.967950 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.46.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.968019 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.47.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.968088 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.48.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.968156 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.49.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.968221 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.50.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.968286 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.51.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.968352 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.52.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.968417 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.53.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.968480 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.54.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.968544 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.55.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.968611 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.56.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.968678 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.57.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.968744 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.58.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.968808 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.59.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.968873 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.60.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.968946 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.61.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969007 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.62.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969069 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.63.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969132 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.64.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969192 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.65.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969254 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.66.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969316 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.67.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969403 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.68.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969467 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.69.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969527 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.70.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969588 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.71.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969652 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.72.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969713 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.73.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969779 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.74.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969839 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.75.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969902 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.76.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.969970 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.77.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.970033 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.78.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.970096 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.79.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:53:41.970177 140666415416448 logger.py:92] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-09:53:41] [TRT] [W] Unused Input: position_ids\n",
      "[12/11/2024-09:53:42] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[12/11/2024-09:53:42] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[12/11/2024-09:53:56] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[12/11/2024-09:53:56] [TRT] [I] Detected 15 inputs and 5 output network tensors.\n",
      "[12/11/2024-09:54:21] [TRT] [I] Total Host Persistent Memory: 305280\n",
      "[12/11/2024-09:54:21] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[12/11/2024-09:54:21] [TRT] [I] Total Scratch Memory: 67117056\n",
      "[12/11/2024-09:54:21] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 1617 steps to complete.\n",
      "[12/11/2024-09:54:21] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 83.9577ms to assign 17 blocks to 1617 nodes requiring 251672576 bytes.\n",
      "[12/11/2024-09:54:21] [TRT] [I] Total Activation Memory: 251671552\n",
      "[12/11/2024-09:54:21] [TRT] [I] Total Weights Memory: 36937678848\n",
      "[12/11/2024-09:54:21] [TRT] [I] Engine generation completed in 39.7687 seconds.\n",
      "[12/11/2024-09:54:21] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 4032 MiB, GPU 35226 MiB\n",
      "[12/11/2024-09:54:39] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 320836 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 09:54:39.642799 140666415416448 logger.py:92] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:57\n",
      "I1211 09:54:39.649972 140666415416448 logger.py:92] [TRT-LLM] [I] Serializing engine to models/llama-3.1-nemotron/70B/trt_llm/rank0.engine...\n",
      "I1211 09:55:05.325882 140666415416448 logger.py:92] [TRT-LLM] [I] Engine serialized. Total time: 00:00:25\n",
      "I1211 09:55:08.996442 140666415416448 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
      "I1211 09:55:08.996592 140666415416448 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
      "I1211 09:55:08.996623 140666415416448 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n",
      "I1211 09:55:08.996644 140666415416448 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "I1211 09:55:08.996661 140666415416448 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
      "W1211 09:55:08.996694 140666415416448 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
      "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n",
      "W1211 09:55:08.996714 140666415416448 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-09:55:36] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 289693, GPU 1764 (MiB)\n",
      "[12/11/2024-09:55:36] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 09:55:36.487796 140666415416448 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "I1211 09:55:36.488140 140666415416448 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:55:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 09:55:36.970633 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.970934 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.971036 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.971118 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.971198 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.971277 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.971354 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.971431 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.971505 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.971580 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.971655 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.971728 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.971800 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.971871 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.971950 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.972022 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.972096 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.16.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.972170 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.17.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.972239 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.18.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.972311 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.19.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.972380 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.20.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.972451 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.21.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.972522 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.22.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.972594 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.23.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.972665 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.24.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.972736 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.25.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.972805 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.26.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.972876 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.27.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.972951 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.28.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.973021 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.29.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.973097 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.30.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.973169 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.31.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.973241 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.32.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.973309 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.33.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.973382 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.34.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.973457 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.35.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.973529 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.36.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.973595 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.37.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.973662 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.38.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.973729 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.39.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.973795 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.40.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.973860 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.41.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.973930 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.42.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.973999 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.43.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.974066 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.44.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.974135 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.45.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.974201 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.46.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.974269 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.47.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.974333 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.48.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.974399 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.49.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.974467 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.50.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.974533 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.51.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.974599 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.52.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.974663 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.53.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.974729 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.54.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.974792 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.55.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.974856 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.56.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.974927 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.57.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.974992 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.58.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.975057 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.59.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.975121 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.60.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.975184 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.61.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.975244 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.62.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.975311 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.63.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.975374 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.64.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.975437 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.65.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.975500 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.66.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.975562 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.67.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.975630 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.68.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.975694 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.69.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.975757 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.70.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.975821 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.71.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.975885 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.72.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.975953 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.73.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.976016 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.74.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.976081 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.75.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.976141 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.76.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.976202 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.77.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.976264 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.78.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.976326 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.79.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:55:36.976420 140666415416448 logger.py:92] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-09:55:36] [TRT] [W] Unused Input: position_ids\n",
      "[12/11/2024-09:55:37] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[12/11/2024-09:55:37] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[12/11/2024-09:55:48] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[12/11/2024-09:55:48] [TRT] [I] Detected 15 inputs and 5 output network tensors.\n",
      "[12/11/2024-09:56:06] [TRT] [I] Total Host Persistent Memory: 305280\n",
      "[12/11/2024-09:56:06] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[12/11/2024-09:56:06] [TRT] [I] Total Scratch Memory: 67117056\n",
      "[12/11/2024-09:56:06] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 1617 steps to complete.\n",
      "[12/11/2024-09:56:06] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 81.0358ms to assign 17 blocks to 1617 nodes requiring 251672576 bytes.\n",
      "[12/11/2024-09:56:06] [TRT] [I] Total Activation Memory: 251671552\n",
      "[12/11/2024-09:56:07] [TRT] [I] Total Weights Memory: 36937678848\n",
      "[12/11/2024-09:56:07] [TRT] [I] Engine generation completed in 29.9298 seconds.\n",
      "[12/11/2024-09:56:07] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 4032 MiB, GPU 35226 MiB\n",
      "[12/11/2024-09:56:23] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 325842 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 09:56:23.044344 140666415416448 logger.py:92] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:46\n",
      "I1211 09:56:23.046135 140666415416448 logger.py:92] [TRT-LLM] [I] Serializing engine to models/llama-3.1-nemotron/70B/trt_llm/rank1.engine...\n",
      "I1211 09:56:45.491527 140666415416448 logger.py:92] [TRT-LLM] [I] Engine serialized. Total time: 00:00:22\n",
      "I1211 09:56:48.858254 140666415416448 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
      "I1211 09:56:48.858414 140666415416448 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
      "I1211 09:56:48.858443 140666415416448 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n",
      "I1211 09:56:48.858466 140666415416448 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "I1211 09:56:48.858484 140666415416448 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
      "W1211 09:56:48.858505 140666415416448 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
      "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n",
      "W1211 09:56:48.858525 140666415416448 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-09:57:08] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 289695, GPU 1764 (MiB)\n",
      "[12/11/2024-09:57:08] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 09:57:08.733625 140666415416448 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "I1211 09:57:08.735268 140666415416448 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:08] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:57:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 09:57:09.442889 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.443202 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.443302 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.443384 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.443461 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.443536 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.443607 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.443679 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.443750 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.443823 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.443895 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.443975 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.444044 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.444108 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.444179 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.444246 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.444314 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.16.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.444381 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.17.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.444450 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.18.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.444515 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.19.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.444583 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.20.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.444648 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.21.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.444712 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.22.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.444778 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.23.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.444844 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.24.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.444915 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.25.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.444981 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.26.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.445048 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.27.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.445113 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.28.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.445174 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.29.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.445244 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.30.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.445315 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.31.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.445389 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.32.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.445453 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.33.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.445521 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.34.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.445588 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.35.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.445655 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.36.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.445721 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.37.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.445785 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.38.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.445847 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.39.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.445915 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.40.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.445978 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.41.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446042 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.42.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446105 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.43.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446167 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.44.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446233 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.45.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446298 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.46.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446359 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.47.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446420 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.48.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446483 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.49.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446544 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.50.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446608 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.51.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446673 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.52.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446734 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.53.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446798 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.54.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446861 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.55.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446933 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.56.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.446997 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.57.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.447059 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.58.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.447122 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.59.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.447182 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.60.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.447244 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.61.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.447307 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.62.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.447370 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.63.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.447443 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.64.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.447506 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.65.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.447570 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.66.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.447633 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.67.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.447704 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.68.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.447769 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.69.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.447833 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.70.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.447896 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.71.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.447965 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.72.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.448028 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.73.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.448089 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.74.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.448153 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.75.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.448215 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.76.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.448279 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.77.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.448343 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.78.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.448405 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.79.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:57:09.448503 140666415416448 logger.py:92] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-09:57:09] [TRT] [W] Unused Input: position_ids\n",
      "[12/11/2024-09:57:09] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[12/11/2024-09:57:09] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[12/11/2024-09:57:22] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[12/11/2024-09:57:22] [TRT] [I] Detected 15 inputs and 5 output network tensors.\n",
      "[12/11/2024-09:57:41] [TRT] [I] Total Host Persistent Memory: 305280\n",
      "[12/11/2024-09:57:41] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[12/11/2024-09:57:41] [TRT] [I] Total Scratch Memory: 67117056\n",
      "[12/11/2024-09:57:41] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 1617 steps to complete.\n",
      "[12/11/2024-09:57:41] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 79.2674ms to assign 17 blocks to 1617 nodes requiring 251672576 bytes.\n",
      "[12/11/2024-09:57:41] [TRT] [I] Total Activation Memory: 251671552\n",
      "[12/11/2024-09:57:41] [TRT] [I] Total Weights Memory: 36937678848\n",
      "[12/11/2024-09:57:41] [TRT] [I] Engine generation completed in 31.7747 seconds.\n",
      "[12/11/2024-09:57:41] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 4032 MiB, GPU 35226 MiB\n",
      "[12/11/2024-09:57:58] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 325842 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 09:57:58.303178 140666415416448 logger.py:92] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:48\n",
      "I1211 09:57:58.305532 140666415416448 logger.py:92] [TRT-LLM] [I] Serializing engine to models/llama-3.1-nemotron/70B/trt_llm/rank2.engine...\n",
      "I1211 09:58:20.362878 140666415416448 logger.py:92] [TRT-LLM] [I] Engine serialized. Total time: 00:00:22\n",
      "I1211 09:58:23.286481 140666415416448 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
      "I1211 09:58:23.286608 140666415416448 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
      "I1211 09:58:23.286639 140666415416448 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n",
      "I1211 09:58:23.286660 140666415416448 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "I1211 09:58:23.286679 140666415416448 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
      "W1211 09:58:23.286702 140666415416448 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
      "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n",
      "W1211 09:58:23.286724 140666415416448 logger.py:92] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-09:58:30] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 289696, GPU 1764 (MiB)\n",
      "[12/11/2024-09:58:30] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 09:58:30.041434 140666415416448 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "I1211 09:58:30.041722 140666415416448 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/32/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/32/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/33/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/33/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/34/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/34/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/35/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/35/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/36/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/36/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/37/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/37/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/38/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/38/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/39/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/39/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/40/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/40/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/41/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/41/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/42/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/42/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/43/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/43/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/44/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/44/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/45/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/45/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/46/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/46/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/47/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/47/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/48/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/48/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/49/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/49/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/50/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/50/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/51/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/51/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/52/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/52/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/53/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/53/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/54/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/54/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/55/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/55/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/56/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/56/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/57/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/57/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/58/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/58/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/59/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/59/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/60/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/60/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/61/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/61/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/62/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/62/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/63/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/63/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/64/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/64/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/65/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/65/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/66/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/66/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/67/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/67/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/68/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/68/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/69/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/69/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/70/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/70/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/71/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/71/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/72/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/72/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/73/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/73/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/74/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/74/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/75/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/75/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/76/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/76/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/77/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/77/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/78/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/78/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/79/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/79/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[12/11/2024-09:58:30] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 09:58:30.526968 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.527212 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.527307 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.527388 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.527463 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.527534 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.527605 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.527682 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.527753 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.527825 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.527894 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.527968 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.528036 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.528103 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.528170 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.528238 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.528307 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.16.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.528374 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.17.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.528440 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.18.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.528505 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.19.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.528570 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.20.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.528635 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.21.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.528702 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.22.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.528769 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.23.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.528833 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.24.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.528895 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.25.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.528965 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.26.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.529028 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.27.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.529090 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.28.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.529155 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.29.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.529224 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.30.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.529294 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.31.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.529366 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.32.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.529432 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.33.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.529500 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.34.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.529564 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.35.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.529628 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.36.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.529692 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.37.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.529760 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.38.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.529825 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.39.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.529890 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.40.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.529960 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.41.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530025 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.42.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530088 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.43.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530149 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.44.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530213 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.45.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530277 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.46.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530339 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.47.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530401 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.48.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530464 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.49.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530528 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.50.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530590 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.51.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530656 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.52.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530720 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.53.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530782 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.54.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530843 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.55.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530910 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.56.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.530976 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.57.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531040 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.58.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531102 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.59.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531164 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.60.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531226 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.61.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531288 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.62.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531348 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.63.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531412 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.64.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531475 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.65.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531536 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.66.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531597 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.67.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531671 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.68.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531734 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.69.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531795 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.70.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531856 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.71.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531924 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.72.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.531987 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.73.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.532050 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.74.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.532111 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.75.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.532173 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.76.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.532233 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.77.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.532293 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.78.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.532352 140666415416448 logger.py:92] [TRT-LLM] [I] Parameter transformer.layers.79.attention.embed_positions (1, 131072, 128) float32 was not materialized to TRT network\n",
      "I1211 09:58:30.532443 140666415416448 logger.py:92] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-09:58:30] [TRT] [W] Unused Input: position_ids\n",
      "[12/11/2024-09:58:30] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[12/11/2024-09:58:30] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[12/11/2024-09:58:41] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[12/11/2024-09:58:41] [TRT] [I] Detected 15 inputs and 5 output network tensors.\n",
      "[12/11/2024-09:59:02] [TRT] [I] Total Host Persistent Memory: 305280\n",
      "[12/11/2024-09:59:02] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[12/11/2024-09:59:02] [TRT] [I] Total Scratch Memory: 67117056\n",
      "[12/11/2024-09:59:02] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 1617 steps to complete.\n",
      "[12/11/2024-09:59:02] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 80.8076ms to assign 17 blocks to 1617 nodes requiring 251672576 bytes.\n",
      "[12/11/2024-09:59:02] [TRT] [I] Total Activation Memory: 251671552\n",
      "[12/11/2024-09:59:03] [TRT] [I] Total Weights Memory: 36937678848\n",
      "[12/11/2024-09:59:03] [TRT] [I] Engine generation completed in 32.6564 seconds.\n",
      "[12/11/2024-09:59:03] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 4032 MiB, GPU 35226 MiB\n",
      "[12/11/2024-09:59:20] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 325842 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 09:59:20.141936 140666415416448 logger.py:92] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:49\n",
      "I1211 09:59:20.145720 140666415416448 logger.py:92] [TRT-LLM] [I] Serializing engine to models/llama-3.1-nemotron/70B/trt_llm/rank3.engine...\n",
      "I1211 09:59:42.510133 140666415416448 logger.py:92] [TRT-LLM] [I] Engine serialized. Total time: 00:00:22\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1211 10:00:02.130824 139674090443904 logger.py:92] [TRT-LLM] [W] Found pynvml==11.4.1. Please use pynvml>=11.5.0 to get accurate memory usage\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1211 10:00:02.130826 140483455378560 logger.py:92] [TRT-LLM] [W] Found pynvml==11.4.1. Please use pynvml>=11.5.0 to get accurate memory usage\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1211 10:00:02.130819 140543358837888 logger.py:92] [TRT-LLM] [W] Found pynvml==11.4.1. Please use pynvml>=11.5.0 to get accurate memory usage\n",
      "W1211 10:00:02.130830 140691274339456 logger.py:92] [TRT-LLM] [W] Found pynvml==11.4.1. Please use pynvml>=11.5.0 to get accurate memory usage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.10.0\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.10.0\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.10.0\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.10.0\n",
      "Loaded mpi lib /usr/local/lib/python3.10/dist-packages/mpi4py/MPI.cpython-310-x86_64-linux-gnu.so successfully\n",
      "Loaded mpi lib /usr/local/lib/python3.10/dist-packages/mpi4py/MPI.cpython-310-x86_64-linux-gnu.so successfully\n",
      "Loaded mpi lib /usr/local/lib/python3.10/dist-packages/mpi4py/MPI.cpython-310-x86_64-linux-gnu.so successfully\n",
      "Loaded mpi lib /usr/local/lib/python3.10/dist-packages/mpi4py/MPI.cpython-310-x86_64-linux-gnu.so successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 10:00:21.108588 140691274339456 logger.py:92] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n",
      "I1211 10:00:21.108868 140691274339456 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
      "I1211 10:00:21.108896 140691274339456 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
      "I1211 10:00:21.108930 140691274339456 logger.py:92] [TRT-LLM] [I] Set smooth_quant_gemm_plugin to None.\n",
      "I1211 10:00:21.108949 140691274339456 logger.py:92] [TRT-LLM] [I] Set identity_plugin to None.\n",
      "I1211 10:00:21.108966 140691274339456 logger.py:92] [TRT-LLM] [I] Set layernorm_quantization_plugin to None.\n",
      "I1211 10:00:21.108981 140691274339456 logger.py:92] [TRT-LLM] [I] Set rmsnorm_quantization_plugin to None.\n",
      "I1211 10:00:21.108997 140691274339456 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "I1211 10:00:21.109013 140691274339456 logger.py:92] [TRT-LLM] [I] Set lookup_plugin to None.\n",
      "I1211 10:00:21.109030 140691274339456 logger.py:92] [TRT-LLM] [I] Set lora_plugin to None.\n",
      "I1211 10:00:21.109050 140691274339456 logger.py:92] [TRT-LLM] [I] Set weight_only_groupwise_quant_matmul_plugin to None.\n",
      "I1211 10:00:21.109066 140691274339456 logger.py:92] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to None.\n",
      "I1211 10:00:21.109086 140691274339456 logger.py:92] [TRT-LLM] [I] Set quantize_per_token_plugin to False.\n",
      "I1211 10:00:21.109102 140691274339456 logger.py:92] [TRT-LLM] [I] Set quantize_tensor_plugin to False.\n",
      "I1211 10:00:21.109120 140691274339456 logger.py:92] [TRT-LLM] [I] Set moe_plugin to float16.\n",
      "I1211 10:00:21.109136 140691274339456 logger.py:92] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.\n",
      "I1211 10:00:21.109155 140691274339456 logger.py:92] [TRT-LLM] [I] Set context_fmha to True.\n",
      "I1211 10:00:21.109173 140691274339456 logger.py:92] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.\n",
      "I1211 10:00:21.109188 140691274339456 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "I1211 10:00:21.109204 140691274339456 logger.py:92] [TRT-LLM] [I] Set remove_input_padding to True.\n",
      "I1211 10:00:21.109218 140691274339456 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n",
      "I1211 10:00:21.109235 140691274339456 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n",
      "I1211 10:00:21.109251 140691274339456 logger.py:92] [TRT-LLM] [I] Set enable_xqa to True.\n",
      "I1211 10:00:21.109266 140691274339456 logger.py:92] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.\n",
      "I1211 10:00:21.109282 140691274339456 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
      "I1211 10:00:21.109299 140691274339456 logger.py:92] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n",
      "I1211 10:00:21.109314 140691274339456 logger.py:92] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
      "I1211 10:00:21.109333 140691274339456 logger.py:92] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.\n",
      "I1211 10:00:21.109349 140691274339456 logger.py:92] [TRT-LLM] [I] Set multiple_profiles to False.\n",
      "I1211 10:00:21.109365 140691274339456 logger.py:92] [TRT-LLM] [I] Set paged_state to True.\n",
      "I1211 10:00:21.109385 140691274339456 logger.py:92] [TRT-LLM] [I] Set streamingllm to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-10:00:21] [TRT] [I] Loaded engine size: 35234 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 10:00:24.998260 140483455378560 logger.py:92] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n",
      "I1211 10:00:24.998497 140483455378560 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
      "I1211 10:00:24.998527 140483455378560 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
      "I1211 10:00:24.998553 140483455378560 logger.py:92] [TRT-LLM] [I] Set smooth_quant_gemm_plugin to None.\n",
      "I1211 10:00:24.998573 140483455378560 logger.py:92] [TRT-LLM] [I] Set identity_plugin to None.\n",
      "I1211 10:00:24.998593 140483455378560 logger.py:92] [TRT-LLM] [I] Set layernorm_quantization_plugin to None.\n",
      "I1211 10:00:24.998612 140483455378560 logger.py:92] [TRT-LLM] [I] Set rmsnorm_quantization_plugin to None.\n",
      "I1211 10:00:24.998631 140483455378560 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "I1211 10:00:24.998651 140483455378560 logger.py:92] [TRT-LLM] [I] Set lookup_plugin to None.\n",
      "I1211 10:00:24.998671 140483455378560 logger.py:92] [TRT-LLM] [I] Set lora_plugin to None.\n",
      "I1211 10:00:24.998692 140483455378560 logger.py:92] [TRT-LLM] [I] Set weight_only_groupwise_quant_matmul_plugin to None.\n",
      "I1211 10:00:24.998714 140483455378560 logger.py:92] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to None.\n",
      "I1211 10:00:24.998736 140483455378560 logger.py:92] [TRT-LLM] [I] Set quantize_per_token_plugin to False.\n",
      "I1211 10:00:24.998755 140483455378560 logger.py:92] [TRT-LLM] [I] Set quantize_tensor_plugin to False.\n",
      "I1211 10:00:24.998774 140483455378560 logger.py:92] [TRT-LLM] [I] Set moe_plugin to float16.\n",
      "I1211 10:00:24.998794 140483455378560 logger.py:92] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.\n",
      "I1211 10:00:24.998811 140483455378560 logger.py:92] [TRT-LLM] [I] Set context_fmha to True.\n",
      "I1211 10:00:24.998831 140483455378560 logger.py:92] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.\n",
      "I1211 10:00:24.998849 140483455378560 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "I1211 10:00:24.998867 140483455378560 logger.py:92] [TRT-LLM] [I] Set remove_input_padding to True.\n",
      "I1211 10:00:24.998885 140483455378560 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n",
      "I1211 10:00:24.998903 140483455378560 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n",
      "I1211 10:00:24.998927 140483455378560 logger.py:92] [TRT-LLM] [I] Set enable_xqa to True.\n",
      "I1211 10:00:24.998952 140483455378560 logger.py:92] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.\n",
      "I1211 10:00:24.998971 140483455378560 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
      "I1211 10:00:24.998988 140483455378560 logger.py:92] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n",
      "I1211 10:00:24.999007 140483455378560 logger.py:92] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
      "I1211 10:00:24.999027 140483455378560 logger.py:92] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.\n",
      "I1211 10:00:24.999044 140483455378560 logger.py:92] [TRT-LLM] [I] Set multiple_profiles to False.\n",
      "I1211 10:00:24.999062 140483455378560 logger.py:92] [TRT-LLM] [I] Set paged_state to True.\n",
      "I1211 10:00:24.999080 140483455378560 logger.py:92] [TRT-LLM] [I] Set streamingllm to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-10:00:25] [TRT] [I] Loaded engine size: 35234 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 10:00:28.351375 139674090443904 logger.py:92] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n",
      "I1211 10:00:28.351610 139674090443904 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
      "I1211 10:00:28.351641 139674090443904 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
      "I1211 10:00:28.351668 139674090443904 logger.py:92] [TRT-LLM] [I] Set smooth_quant_gemm_plugin to None.\n",
      "I1211 10:00:28.351686 139674090443904 logger.py:92] [TRT-LLM] [I] Set identity_plugin to None.\n",
      "I1211 10:00:28.351706 139674090443904 logger.py:92] [TRT-LLM] [I] Set layernorm_quantization_plugin to None.\n",
      "I1211 10:00:28.351723 139674090443904 logger.py:92] [TRT-LLM] [I] Set rmsnorm_quantization_plugin to None.\n",
      "I1211 10:00:28.351742 139674090443904 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "I1211 10:00:28.351760 139674090443904 logger.py:92] [TRT-LLM] [I] Set lookup_plugin to None.\n",
      "I1211 10:00:28.351781 139674090443904 logger.py:92] [TRT-LLM] [I] Set lora_plugin to None.\n",
      "I1211 10:00:28.351800 139674090443904 logger.py:92] [TRT-LLM] [I] Set weight_only_groupwise_quant_matmul_plugin to None.\n",
      "I1211 10:00:28.351818 139674090443904 logger.py:92] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to None.\n",
      "I1211 10:00:28.351840 139674090443904 logger.py:92] [TRT-LLM] [I] Set quantize_per_token_plugin to False.\n",
      "I1211 10:00:28.351858 139674090443904 logger.py:92] [TRT-LLM] [I] Set quantize_tensor_plugin to False.\n",
      "I1211 10:00:28.351875 139674090443904 logger.py:92] [TRT-LLM] [I] Set moe_plugin to float16.\n",
      "I1211 10:00:28.351892 139674090443904 logger.py:92] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.\n",
      "I1211 10:00:28.351915 139674090443904 logger.py:92] [TRT-LLM] [I] Set context_fmha to True.\n",
      "I1211 10:00:28.351933 139674090443904 logger.py:92] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.\n",
      "I1211 10:00:28.351955 139674090443904 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "I1211 10:00:28.351972 139674090443904 logger.py:92] [TRT-LLM] [I] Set remove_input_padding to True.\n",
      "I1211 10:00:28.351992 139674090443904 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n",
      "I1211 10:00:28.352009 139674090443904 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n",
      "I1211 10:00:28.352027 139674090443904 logger.py:92] [TRT-LLM] [I] Set enable_xqa to True.\n",
      "I1211 10:00:28.352047 139674090443904 logger.py:92] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.\n",
      "I1211 10:00:28.352067 139674090443904 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
      "I1211 10:00:28.352085 139674090443904 logger.py:92] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n",
      "I1211 10:00:28.352102 139674090443904 logger.py:92] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
      "I1211 10:00:28.352120 139674090443904 logger.py:92] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.\n",
      "I1211 10:00:28.352139 139674090443904 logger.py:92] [TRT-LLM] [I] Set multiple_profiles to False.\n",
      "I1211 10:00:28.352155 139674090443904 logger.py:92] [TRT-LLM] [I] Set paged_state to True.\n",
      "I1211 10:00:28.352173 139674090443904 logger.py:92] [TRT-LLM] [I] Set streamingllm to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-10:00:28] [TRT] [I] Loaded engine size: 35234 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 10:00:29.169286 140543358837888 logger.py:92] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n",
      "I1211 10:00:29.169537 140543358837888 logger.py:92] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
      "I1211 10:00:29.169566 140543358837888 logger.py:92] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
      "I1211 10:00:29.169593 140543358837888 logger.py:92] [TRT-LLM] [I] Set smooth_quant_gemm_plugin to None.\n",
      "I1211 10:00:29.169615 140543358837888 logger.py:92] [TRT-LLM] [I] Set identity_plugin to None.\n",
      "I1211 10:00:29.169633 140543358837888 logger.py:92] [TRT-LLM] [I] Set layernorm_quantization_plugin to None.\n",
      "I1211 10:00:29.169651 140543358837888 logger.py:92] [TRT-LLM] [I] Set rmsnorm_quantization_plugin to None.\n",
      "I1211 10:00:29.169668 140543358837888 logger.py:92] [TRT-LLM] [I] Set nccl_plugin to bfloat16.\n",
      "I1211 10:00:29.169687 140543358837888 logger.py:92] [TRT-LLM] [I] Set lookup_plugin to None.\n",
      "I1211 10:00:29.169706 140543358837888 logger.py:92] [TRT-LLM] [I] Set lora_plugin to None.\n",
      "I1211 10:00:29.169725 140543358837888 logger.py:92] [TRT-LLM] [I] Set weight_only_groupwise_quant_matmul_plugin to None.\n",
      "I1211 10:00:29.169743 140543358837888 logger.py:92] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to None.\n",
      "I1211 10:00:29.169765 140543358837888 logger.py:92] [TRT-LLM] [I] Set quantize_per_token_plugin to False.\n",
      "I1211 10:00:29.169788 140543358837888 logger.py:92] [TRT-LLM] [I] Set quantize_tensor_plugin to False.\n",
      "I1211 10:00:29.169807 140543358837888 logger.py:92] [TRT-LLM] [I] Set moe_plugin to float16.\n",
      "I1211 10:00:29.169825 140543358837888 logger.py:92] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.\n",
      "I1211 10:00:29.169845 140543358837888 logger.py:92] [TRT-LLM] [I] Set context_fmha to True.\n",
      "I1211 10:00:29.169863 140543358837888 logger.py:92] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.\n",
      "I1211 10:00:29.169881 140543358837888 logger.py:92] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "I1211 10:00:29.169898 140543358837888 logger.py:92] [TRT-LLM] [I] Set remove_input_padding to True.\n",
      "I1211 10:00:29.169924 140543358837888 logger.py:92] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n",
      "I1211 10:00:29.169941 140543358837888 logger.py:92] [TRT-LLM] [I] Set multi_block_mode to False.\n",
      "I1211 10:00:29.169957 140543358837888 logger.py:92] [TRT-LLM] [I] Set enable_xqa to True.\n",
      "I1211 10:00:29.169975 140543358837888 logger.py:92] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.\n",
      "I1211 10:00:29.169994 140543358837888 logger.py:92] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
      "I1211 10:00:29.170011 140543358837888 logger.py:92] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n",
      "I1211 10:00:29.170028 140543358837888 logger.py:92] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
      "I1211 10:00:29.170044 140543358837888 logger.py:92] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.\n",
      "I1211 10:00:29.170062 140543358837888 logger.py:92] [TRT-LLM] [I] Set multiple_profiles to False.\n",
      "I1211 10:00:29.170079 140543358837888 logger.py:92] [TRT-LLM] [I] Set paged_state to True.\n",
      "I1211 10:00:29.170097 140543358837888 logger.py:92] [TRT-LLM] [I] Set streamingllm to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-10:00:29] [TRT] [I] Loaded engine size: 35234 MiB\n",
      "[TensorRT-LLM][INFO] Detecting local TP group for rank 1\n",
      "[TensorRT-LLM][INFO] Detecting local TP group for rank 3\n",
      "[TensorRT-LLM][INFO] Detecting local TP group for rank 2\n",
      "[TensorRT-LLM][INFO] Detecting local TP group for rank 0\n",
      "[TensorRT-LLM][INFO] TP group is intra-node for rank 2\n",
      "[TensorRT-LLM][INFO] TP group is intra-node for rank 0\n",
      "[TensorRT-LLM][INFO] TP group is intra-node for rank 1\n",
      "[TensorRT-LLM][INFO] TP group is intra-node for rank 3\n",
      "[12/11/2024-10:00:38] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n",
      "[12/11/2024-10:00:38] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n",
      "[12/11/2024-10:00:38] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n",
      "[12/11/2024-10:00:38] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n",
      "[12/11/2024-10:00:39] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1211 10:00:39.117069 139674090443904 logger.py:92] [TRT-LLM] [W] The paged KV cache in Python runtime is experimental. For performance and correctness, please, use C++ runtime.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-10:00:39] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1211 10:00:39.123890 140483455378560 logger.py:92] [TRT-LLM] [W] The paged KV cache in Python runtime is experimental. For performance and correctness, please, use C++ runtime.\n",
      "I1211 10:00:39.129375 140483455378560 logger.py:92] [TRT-LLM] [I] Load engine takes: 36.43295121192932 sec\n",
      "I1211 10:00:39.129376 139674090443904 logger.py:92] [TRT-LLM] [I] Load engine takes: 36.56637644767761 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/11/2024-10:00:39] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n",
      "[12/11/2024-10:00:39] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 35226 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1211 10:00:39.154350 140543358837888 logger.py:92] [TRT-LLM] [W] The paged KV cache in Python runtime is experimental. For performance and correctness, please, use C++ runtime.\n",
      "W1211 10:00:39.154768 140691274339456 logger.py:92] [TRT-LLM] [W] The paged KV cache in Python runtime is experimental. For performance and correctness, please, use C++ runtime.\n",
      "I1211 10:00:39.155468 140543358837888 logger.py:92] [TRT-LLM] [I] Load engine takes: 36.52978515625 sec\n",
      "I1211 10:00:39.156029 140691274339456 logger.py:92] [TRT-LLM] [I] Load engine takes: 36.460482120513916 sec\n",
      "I1211 10:00:43.900752 140666415416448 deploy_triton.py:377] Triton deploy function will be called.\n",
      "I1211 10:00:43.905046 140666415416448 deploy_triton.py:384] Model serving on Triton is will be started.\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "PRECISION=bf16-mixed\n",
    "MODEL_DIR=\"models/llama-3.1-nemotron/70B/trt_llm\"\n",
    "mkdir -p \"$MODEL_DIR\"\n",
    "MODEL_CKPT=\"models/llama-3.1-nemotron/70B/nemo/Llama-3_1-Nemotron-70B-Instruct.nemo\"\n",
    "LORA_CKPT=\"results/llama-3.1-nemotron/70B/$PRECISION/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "\n",
    "python /opt/NeMo/scripts/deploy/nlp/deploy_triton.py \\\n",
    "    --nemo_checkpoint \"$MODEL_CKPT\" \\\n",
    "    --lora_ckpt \"$LORA_CKPT\" \\\n",
    "    --model_type llama \\\n",
    "    --triton_model_name llama3-finetuned \\\n",
    "    --triton_model_repository \"$MODEL_DIR\" \\\n",
    "    --num_gpus 4 \\\n",
    "    --tensor_parallelism_size 4 \\\n",
    "    --pipeline_parallelism_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d5215a-0766-4561-bc8f-bb632474f0d4",
   "metadata": {},
   "source": [
    "Open a terminal to query the model:\n",
    "\n",
    "```bash\n",
    "QUERY=\"Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?\"\n",
    "\n",
    "python /opt/NeMo/scripts/deploy/nlp/query.py \\\n",
    "    -mn llama3-finetuned \\\n",
    "    -p \"$QUERY\" \\\n",
    "    -mol 350\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f89de3-fe8c-45e9-970c-673833c8b5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
